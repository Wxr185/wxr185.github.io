<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Normalization</title>
    <url>/2022/09/25/AI/Normalization/</url>
    <content><![CDATA[<p>机器学习领域有个很重要的假设：IID (Independent Identically Distribution) 独立同分布假设，即假设训练数据和测试数据是满足同分布的。</p>
<blockquote>
<p>神经网络的学习过程本质上是为了学习数据的分布。<br>在mini-batch梯度下降训练的时候，如果每批训练数据的分布不同，那么网络在每次迭代的时候都要学习适应不同的分布，大大降低了网络的训练速度。</p>
</blockquote>
<h2 id="BatchNorm"><a href="#BatchNorm" class="headerlink" title="BatchNorm"></a>BatchNorm</h2><p>BatchNorm就是在深度神经网络训练过程中，使得每一层神经网络的输入保持相同分布。</p>
<p>BN层在激活函数之前。BN层的作用机制：通过平滑隐藏层输入的分布，帮助随机梯度下降的进行，缓解随机梯度下降随遇后续层的负面影响。</p>
<blockquote>
<ol>
<li>sigmoid, tanh激活函数。函数图像两端，梯度较小，容易出现 <strong>梯度衰减</strong> 问题。因此，把BN层放在非线性激活函数之前，将数据分布调整到均值为0附近，加速训练。</li>
<li>relu激活函数。relu函数负半区的输出值被抑制，正半区的值被保留。因此，BN层放在前面，可以防止某一层的激活值全部被抑制，导致梯度全部为0，梯度消失。同理，防止梯度爆炸。</li>
</ol>
</blockquote>
<h3 id="Internal-Covariate-Shift-问题"><a href="#Internal-Covariate-Shift-问题" class="headerlink" title="Internal Covariate Shift 问题"></a>Internal Covariate Shift 问题</h3><p>在训练过程中，隐层的 <em><strong>输入分布</strong></em> 总是变来变去。导致下一层网络很难进行学习（神经网络本来就是要学习数据分布的）。</p>
<blockquote>
<p>Internal Covariate Shift: 发生在神经网络内部；<br>Covariate Shift: 发生在输入数据上。主要描述由于训练数据和测试数据存在分布差异，影响模型的泛化性和训练速度。</p>
</blockquote>
<h3 id="BatchNorm基本思想"><a href="#BatchNorm基本思想" class="headerlink" title="BatchNorm基本思想"></a>BatchNorm基本思想</h3><p>深度神经网络在做 <strong>非线性变化前</strong> 的激活输入值随着网络深度加深，在训练过程中，数据分布逐渐向着 <strong>非线性函数取值区间的上下限两端靠近</strong>，导致反向传播时，低层神经网络梯度消失，最终造成收敛变慢。</p>
<p>BN就是规范化隐层数据分布，将数据分布强制规范到非线形激活函数比较敏感的区域，避免梯度消失问题产生。<br>就是说经过BN后，大部分输出值落在非线形函数的非饱和区，加速收敛过程。</p>
<blockquote>
<p>如果都通过BN，那么不就跟把非线性函数替换成线性函数效果相同了？这意味着什么？我们知道，如果是多层的线性函数变换其实这个深层是没有意义的，因为多层线性网络跟一层线性网络是等价的。这意味着网络的表达能力下降了，这也意味着深度的意义就没有了。</p>
</blockquote>
<p>BN为了保证模型的非线形，对变换后的数据分布，进行了scale加上shift操作，<strong>这两个参数通过训练学习得到</strong>。等价于非线性函数的值，从正中心周围的线性区域往非线性区域偏移。增强模型的表达能力。</p>
<blockquote>
<p><strong>BN的核心思想：</strong>在非线性和线性之间找到较好的平衡点。既能享受非线性较强的表达能力，又能避免非线性激活函数饱和区梯度消失问题。</p>
</blockquote>
<h3 id="BatchNorm训练阶段"><a href="#BatchNorm训练阶段" class="headerlink" title="BatchNorm训练阶段"></a>BatchNorm训练阶段</h3><p>对于mini-batch SGD来说，一次训练过程中包含m个训练实例，其具体BN操作就是对于隐层中 <strong>每个神经元</strong> 的激活值，进行如下变换：<br><img src="/pictures/AI/Normal/img1.png" alt="每个神经元数据的标准化操作"></p>
<p>经过上述变化后，某个神经元的激活值变成了N(0, 1)正态分布。<br>为了防止网络表达能力下降，每个神经元增加两个调节参数，这两个参数通过训练学习得到，用来还原网络非线性表达能力。<br><img src="/pictures/AI/Normal/img2.png" alt="数据的放缩与偏移"></p>
<p>BN的具体操作流程如下，</p>
<blockquote>
<ol>
<li>先求出此次批量数据x的均值</li>
<li>求出此次batch的方差</li>
<li>接下来就是对x做归一化</li>
<li>最重要的一步，引入缩放和平移变量γ和β ,计算归一化后的值</li>
</ol>
</blockquote>
<p><img src="/pictures/AI/Normal/img3.png" alt="BN具体流程"></p>
<p>一个简单的代码实现，</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def BatchNorm(x, gamma, beta, bn_param):</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">param:x		: 输入数据，shape(B, L)</span><br><span class="line">param:gamma	: 缩放因子</span><br><span class="line">param:beta	: 平移因子</span><br><span class="line">param:bn_param	: batchnorm所需要的一些参数</span><br><span class="line">	eps		: 接近0的数，防止分母出现0</span><br><span class="line">	momentum	: 动量参数，一般为0.9，0.99，0.999</span><br><span class="line">	running_mean	: 滑动平均的方式计算新的均值</span><br><span class="line">	running_var	: 滑动平均的方式计算新的方差</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    x_mean = x.mean(axis=0)</span><br><span class="line">    x_var = x.var(axis=0)</span><br><span class="line">    x_norm = (x - x_mean) / np.sqrt(x_var + eps)</span><br><span class="line">    x_norm = gamma * x_norm + beta</span><br><span class="line"></span><br><span class="line">    # 滑动平均计算得到均值方差信息，用于推断阶段</span><br><span class="line">    running_mean = bn_param[&#x27;running_mean&#x27;]</span><br><span class="line">    running_var = bn_parma[&#x27;running_var&#x27;]</span><br><span class="line">    momentum = bn_param[&#x27;momentum&#x27;]</span><br><span class="line"></span><br><span class="line">    running_mean = momentum * running_mean + (1-momentum) * x_mean</span><br><span class="line">    running_var = momentum * running_var + (1-momentum) * x_var</span><br><span class="line"></span><br><span class="line">    bn_param[&#x27;running_mean&#x27;] = running_mean</span><br><span class="line">    bn_param[&#x27;running_var&#x27;] = running_var</span><br><span class="line"></span><br><span class="line">    return x_norm, bn_param</span><br></pre></td></tr></table></figure>

<p>在训练中完成的任务，每次训练给一个批量，然后计算批量的均值方差，但是在测试的时候可不是这样，测试的时候 <strong>每次只输入一张图片</strong>，这怎么计算批量的均值和方差，于是，就有了代码中下面两行，在训练的时候实现计算好mean和var，测试的时候直接拿来用就可以了，不用计算均值和方差。</p>
<h3 id="BatchNorm优势"><a href="#BatchNorm优势" class="headerlink" title="BatchNorm优势"></a>BatchNorm优势</h3><blockquote>
<ol>
<li>不仅仅极大提升了训练速度，收敛过程大大加快；</li>
<li>还能增加分类效果，一种解释是这是类似于Dropout的一种防止过拟合的正则化表达方式，所以不用Dropout也能达到相当的效果；</li>
<li>另外调参过程也简单多了，对于初始化要求没那么高，而且可以使用大的学习率等；</li>
<li>batchnorm降低了数据之间的绝对差异，有一个去相关的性质，更多的考虑相对差异性，因此在分类任务上具有更好的效果。</li>
</ol>
</blockquote>
<h2 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h2><h3 id="BatchNorm缺点"><a href="#BatchNorm缺点" class="headerlink" title="BatchNorm缺点"></a>BatchNorm缺点</h3><p>Batch size太小会影响模型性能。对batchsize的大小比较敏感，由于每次计算均值和方差是在一个batch上，所以 <strong>如果batchsize太小，则计算的均值、方差不足以代表整个数据分布</strong>；</p>
<p>BN实际使用时需要计算并且保存某一层神经网络batch的均值和方差等统计信息，对于对一个固定深度的前向神经网络（DNN，CNN）使用BN，很方便；但对于RNN来说，sequence的长度是不一致的，换句话说RNN的深度不是固定的，不同的time-step需要保存不同的statics特征，可能存在一个特殊sequence比其他sequence长很多，这样training时，计算很麻烦。</p>
<p>BN不适用于RNN等动态网络，适用于CNN；LN适用于RNN。</p>
<blockquote>
<p>很直观的一个例子：BN计算每个句子同一个位置字的均值和方差，但因为每个句子的长度不一样，最后是padding成一样的长度；那假如在该位置时，最后一句在该位置是没有字的，也就是用0表示了，那就会影响整个结果。</p>
</blockquote>
<h3 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h3><p>BN的主要思想：在每一层的每一批数据（一个batch里的同一个通道）上进行归一化；<br>LN的主要思想：在每一个样本（一个样本的不同通道）上计算均值和方差，并不是BN那种在批方向计算均值和方差。<br><img src="/pictures/AI/Normal/img4.png" alt="BN和LN的区别"></p>
<h3 id="源码实现"><a href="#源码实现" class="headerlink" title="源码实现"></a>源码实现</h3><p>Layer Normalization在NLP的直观图中，就是对一个batch中的同一句话中的 <strong>每个字</strong> 分别进行归一化。</p>
<p>如果只看 NLP 问题，假设我们的 batch 是（2,3,4）的，也就是 batch_size &#x3D; 2, seq_length &#x3D; 3, dim &#x3D; 4 的，假设第一个句子是 w1 w2 w3，第二个句子是 w4 w5 w6，那么这个 tensor 可以写为</p>
<blockquote>
<p>[ [[w11,w12,w13,w14], …]<br>[[w41,w42,w43,w44], …] ]</p>
</blockquote>
<p>如果是 BN 的话，会对同一个 batch 里对应位置上的 token 求平均值，也就是说 (w11+w12+w13+w14+w41+w42+w43+w44)&#x2F;8是其中一个 mean，一共会求出 3 个 mean，也就是上图里 C 个（seq_length）个 mean。</p>
<p>如果是 LN 的话，<strong>看起来（其实并不是）</strong> 是对每个 sample 里的所有 feature 求 mean，也就是(w11+w12+w13+w14+w21+w22+w23+w24+w31+w32+w33+w34)&#x2F;12，可以求出一共 2 个 mean，也就是图里 N（batch_size）个 mean。<br><img src="/pictures/AI/Normal/img5.png" alt="Layer Norm的不同"></p>
<p>左图和我们认为的 LN 一致，也是我一直认为的 LN，但是右图却是在一个 token 上求平均，带回我们原来的问题，对于一个(2,3,4)的 tensor，(w11+w12+w13+w14)&#x2F;4 是一个 mean，一共会有 2*3&#x3D;6 个 mean。</p>
]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>TransX</title>
    <url>/2022/09/11/KG/TransX/</url>
    <content><![CDATA[<p>参考信息：<a href="https://zhuanlan.zhihu.com/p/354867179">https://zhuanlan.zhihu.com/p/354867179</a></p>
<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>知识图谱&#x2F;知识库通常以网络的形式组织知识，网络中每个节点代表实体，边代表实体间关系，因此大部分知识往往可以用三元组（实体1，关系，实体2）来表示。</p>
<p>知识表示学习(Knowledge Representation Learning)，又称知识图谱嵌入(Knowledge Graph Embedding)，是指将由组成知识的实体和关系在低维连续向量空间中表征的过程。</p>
<p>我们以h,r,t分别表示头实体、关系、尾实体，对于一个三元组&lt;h[i],r[i],t[i]&gt;，如果其符合事实，我们称其为置信度(plausibility)为1，如果其不成立，则其置信度为0。</p>
<p>知识表示学习的一般流程为：</p>
<blockquote>
<ol>
<li>随机初始化实体和关系向量；</li>
<li>定义打分函数(Scoring Function)来计算一个三元组的 <strong>置信度</strong>；</li>
<li>最大化置信度来训练实体、关系向量。</li>
</ol>
</blockquote>
<p>从工作流程上而言，可以将KRL分解为四部分：</p>
<blockquote>
<ol>
<li>表征空间，关系和实体表征在一个什么样的空间；</li>
<li>打分函数，如何计算给定三元组的置信度；</li>
<li>补充信息，采用了哪些补充信息（实体类别、实体描述、关系路径等）来参与表示学习；</li>
<li>训练方式，如何生成正负样本，使用何种loss函数等。</li>
</ol>
</blockquote>
<h3 id="表征空间"><a href="#表征空间" class="headerlink" title="表征空间"></a>表征空间</h3><p>表征空间需要满足三个条件：<strong>可微分，可计算概率，可定义打分函数</strong>。</p>
<h4 id="实内积空间模型"><a href="#实内积空间模型" class="headerlink" title="实内积空间模型"></a>实内积空间模型</h4><p>将实体和关系表征在实内积空间中。</p>
<h4 id="复空间模型"><a href="#复空间模型" class="headerlink" title="复空间模型"></a>复空间模型</h4><p>将实体和关系表征在复空间中。复空间主要是能表征平移信息之外的旋转信息。</p>
<h4 id="高斯分布模型"><a href="#高斯分布模型" class="headerlink" title="高斯分布模型"></a>高斯分布模型</h4><p>使用高斯分布去表征实体和关系中的不确定性信息。</p>
<h4 id="流行和群"><a href="#流行和群" class="headerlink" title="流行和群"></a>流行和群</h4><p>这一类模型将知识表征在流形空间(manifold space)，李群(Lie group)或二面体群(dihedral group)。典型代表是ManifoldE，TorusE和DihEdra。</p>
<h3 id="打分函数"><a href="#打分函数" class="headerlink" title="打分函数"></a>打分函数</h3><p>打分函数用于衡量一个三元组的置信度。</p>
<h4 id="基于距离的打分函数"><a href="#基于距离的打分函数" class="headerlink" title="基于距离的打分函数"></a>基于距离的打分函数</h4><p>通过计算实体间的距离来衡量三元组的置信度。其中，基于加性平移的关系模型应用最广。</p>
<blockquote>
<p>h + r &#x3D; t</p>
</blockquote>
<p><img src="/pictures/KG/TransX/img1.jpg" alt="传统基于距离变换模型"></p>
<p>基于平移表征的关系模型，即将 <strong>关系表示为头实体向尾实体的平移向量</strong>。</p>
<ul>
<li>TransE：基于平移表征；</li>
<li>TransH: 将实体和关系映射到超平面；</li>
<li>TransR：将实体和关系映射到不同的空间；</li>
<li>TransD：构建动态映射矩阵完成实体空间的映射；</li>
<li>TransA：将欧式距离替换成马氏距离；</li>
<li>TransF：松弛了严格平移条件，使用内积作为度量函数</li>
</ul>
<p><img src="/pictures/KG/TransX/img2.jpg" alt="距离变换模型总结"></p>
<h4 id="基于语义匹配度的打分函数"><a href="#基于语义匹配度的打分函数" class="headerlink" title="基于语义匹配度的打分函数"></a>基于语义匹配度的打分函数</h4><p>基于语义匹配度衡量三元组置信度，通常使用关系矩阵将头实体映射至尾实体。</p>
<blockquote>
<p>h * M &#x3D; t</p>
</blockquote>
<h5 id="线性-x2F-双线性模型"><a href="#线性-x2F-双线性模型" class="headerlink" title="线性 &#x2F; 双线性模型"></a>线性 &#x2F; 双线性模型</h5><p>RESCAL将语义相似度定义为实体关系对的匹配程度，使用双线性函数对其进行表征。但是由于双线性函数满足交换律，所以RESCAL不能表达非对称关系，即(h,r,t)成立而(t,r,h)不成立的情况。同时其计算复杂度较高。</p>
<p>DistMult将双线性映射加以简化为对角阵。但由于DistMult仍然满足交换律，也不能表达非对称关系。</p>
<p>HolE提出使用头尾实体的循环相关操作来表示实体对，定义循环相关运算符，使用循环相关操作表示语义匹配程度。HolE的循环相关操作不满足交换律，所以可以表达非对称关系。</p>
<p><img src="/pictures/KG/TransX/img3.jpg" alt="语义匹配模型"></p>
<h5 id="张量分解模型"><a href="#张量分解模型" class="headerlink" title="张量分解模型"></a>张量分解模型</h5><p>TuckerER使用Tucker张量分解(Tucker Decomposition)方法对原始矩阵进行分解，并使用分解的核心矩阵来参与打分函数计算。</p>
<p>LowFER提出多模态张量分解双线性池化机制来更好地表达实体和关系之间的语义联系，并通过低秩估计相较于TuckerER降低了计算复杂度。</p>
<h5 id="神经网络模型"><a href="#神经网络模型" class="headerlink" title="神经网络模型"></a>神经网络模型</h5><p><img src="/pictures/KG/TransX/img7.jpg" alt="神经网络模型"></p>
<h6 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h6><p>SME（Semantic Matching Model）、NTN（Neural Tensor Network）、NAM（Neural Association Model）等都使用MLP对实体和关系进行编码。<br><img src="/pictures/KG/TransX/img4.png" alt="MLP模型公式"></p>
<h6 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h6><p>ConvE使用二维卷积来表征实体和关系：<br><img src="/pictures/KG/TransX/img5.png" alt="ConvE模型公式"><br>其中的ω是卷积层的卷积核，vec(·)是对张量的flatten操作，在卷积层抽取空域特征后，使用多个非线性函数得到语义信息。</p>
<p>ConvKB则直接将实体和关系concat起来加以卷积：<br><img src="/pictures/KG/TransX/img6.png" alt="ConvKB模型公式"></p>
<h6 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h6><p>RSN在RNN基础上加入skip connection来捕捉路径上的长程依赖。先使用Random Walk的方法生成随机路径(x[1], x[2], …, x[T])，使用RNN计算隐状态h[t] &#x3D; tanh(W_h<em>h[t-1] + W_x</em>x[t] + b)，skip connection对于实体和关系的计算不同。</p>
<h6 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h6><p>CoKE使用Transformer的结构对边和路径进行编码，KG-BERT则利用预训练语言模型BERT作为编码器对实体和关系进行编码。</p>
<h6 id="GNN"><a href="#GNN" class="headerlink" title="GNN"></a>GNN</h6><p>图神经网络对于图结构信息的挖掘具有一定优势。SACN使用Encoder-Decoder结构，将带权GCN作为Encoder，将Conv-TransE作为Decoder。</p>
<p><img src="/pictures/KG/TransX/img8.jpg" alt="语义匹配模型计算公式"></p>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><h4 id="基于开放世界假设"><a href="#基于开放世界假设" class="headerlink" title="基于开放世界假设"></a>基于开放世界假设</h4><p>KGS只包含真实的事实，D+只存储正例。</p>
<p>可以定义logistic loss:<br><img src="/pictures/KG/TransX/img9.png" alt="logistic loss定义"></p>
<p>可以定义pairwise ranking loss:<br><img src="/pictures/KG/TransX/img10.png" alt="pairwise ranking loss定义"></p>
<h4 id="基于闭合世界假设"><a href="#基于闭合世界假设" class="headerlink" title="基于闭合世界假设"></a>基于闭合世界假设</h4><p>没有包含在D+中的样例都是错误的。不存在负样本。</p>
<p>定义squared loss:<br><img src="/pictures/KG/TransX/img11.png" alt="squared loss定义"></p>
]]></content>
      <categories>
        <category>知识表示学习</category>
      </categories>
      <tags>
        <tag>KGE</tag>
      </tags>
  </entry>
  <entry>
    <title>RankNet</title>
    <url>/2022/10/06/LTR/RankNet/</url>
    <content><![CDATA[<p>RankNet是2005年微软提出的一种pairwise的Learning to Rank算法，它从 <strong>概率</strong> 的角度来解决排序问题。RankNet的核心是提出了一种 <strong>概率损失函数来学习Ranking Function</strong>，并应用Ranking Function对文档进行排序。这里的Ranking Function可以是 <strong>任意对参数可微的模型</strong>，也就是说，该概率损失函数并不依赖于特定的机器学习模型，在论文中，RankNet是基于神经网络实现的。除此之外，GDBT等模型也可以应用于该框架。</p>
<p>算法的核心思想：<em><strong>最小化文档对的排序误差</strong></em>。</p>
<h3 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h3><p>区别于标准的机器学习的数据结构，<br><img src="/pictures/LTR/RankNet/img1.jpeg" alt="标准的机器学习数据结构"></p>
<p>RankNet做了如下定义，训练样本由 <strong>item对</strong> 构成，最终的训练样本集为：<br><img src="/pictures/LTR/RankNet/img2.png" alt="RankNet数据结构"></p>
<h4 id="样本标签"><a href="#样本标签" class="headerlink" title="样本标签"></a>样本标签</h4><h5 id="预测相关性概率"><a href="#预测相关性概率" class="headerlink" title="预测相关性概率"></a>预测相关性概率</h5><p>对于任意一个doc对(U_i, U_j)，模型输出的score分别为s_i和s_j。那么根据模型的预测，U_i比U_j与Query更相关的概率为：<br><img src="/pictures/LTR/RankNet/img3.png" alt="预测相关性概率"></p>
<p>由于RankNet使用的模型一般为神经网络，根据以往经验，sigmoid函数能够提供一个比较好的概率评估。</p>
<p>RankNet证明了如果知道一个待排序文档的排列中相邻两个文档之间的排序概率，则通过推导可以算出每两个文档之间的排序概率。因此对于一个待排序文档序列，只需计算相邻文档之间的排序概率，不需要计算所有pair，减少计算量。</p>
<h5 id="真实相关性概率"><a href="#真实相关性概率" class="headerlink" title="真实相关性概率"></a>真实相关性概率</h5><p>对于训练数据中的U_i和U_j，它们都包含有一个与Query相关性的真实label，比如U_i与Query的相关性label为good，U_j与Query的相关性label为bad，那么显然U_i比U_j更相关。我们定义U_i比U_j更相关的真实概率为：<br><img src="/pictures/LTR/RankNet/img4.png" alt="真实相关性概率"></p>
<p>在RankNet中类别标签记为S_ij ∈ {+1, -1, 0}。由于接下来要使用交叉熵作为损失函数，因此将标签S_ij与真实概率P_ij（真实相关性）进行上图所示的一一映射。<br>如果U_i比U_j更相关，那么Sij&#x3D;1；如果U_i不如U_j相关，那么S_ij&#x3D;−1；如果U_i、U_j与Query的相关程度相同，那么S_ij&#x3D;0。</p>
<h3 id="假设函数"><a href="#假设函数" class="headerlink" title="假设函数"></a>假设函数</h3><p>f 没有固定的形式，RankNet 设计为两层浅层网络，这也是 RankNet 得名的原因。f 没有固定形式预留了灵活的空间，为后面 LambdaMART 埋下伏笔。<br><img src="/pictures/LTR/RankNet/img8.jpeg" alt="单个样本的交叉熵损失"></p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>RankNet使用交叉熵作为损失函数，单个样本的交叉熵损失函数为<br><img src="/pictures/LTR/RankNet/img5.png" alt="单个样本的交叉熵损失"></p>
<p>对于一个排序，RankNet从各个doc的相对关系来评价排序结果的好坏，排序的效果越好，那么有错误相对关系的pair就越少。RankNet本质上就是以 <strong>错误的pair最少</strong> 为优化目标。</p>
<p>在抽象成cost function时，RankNet实际上是引入了概率的思想：不是直接判断Ui排在Uj前面，而是说Ui以一定的概率P排在Uj前面，即是以预测概率与真实概率的差距最小作为优化目标。最后，RankNet使用Cross Entropy作为cost function，来衡量P_ij和~P_ij的拟合程度。化简后，有<br><img src="/pictures/LTR/RankNet/img6.png" alt="化简后的交叉熵损失"></p>
<p>下面展示了当S_ij分别取1，0，-1的时候cost function以s_i-s_j为变量的示意图：<br><img src="/pictures/LTR/RankNet/img7.png" alt="损失函数曲线变化"></p>
<p>可以看到当S_ij&#x3D;1时，模型预测的s_i比s_j越大，其代价越小；S_ij&#x3D;−1时，s_i比s_j越小，代价越小；S_ij&#x3D;0时，代价的最小值在s_i与s_j相等处取得。</p>
<p>该损失函数有以下几个特点：</p>
<blockquote>
<ol>
<li>当两个相关性不同的文档算出来的模型分数相同时，损失函数的值大于0，仍会对这对pair做惩罚，使他们的排序位置区分开；</li>
<li>损失函数是一个类线性函数，可以有效减少异常样本数据对模型的影响，因此具有鲁棒性。</li>
</ol>
</blockquote>
<h3 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h3><p><img src="/pictures/LTR/RankNet/img9.jpeg" alt="损失函数求偏导数"></p>
<p>损失函数L_ij（上文中表示为C_ij）分别对s_i和s_j求偏微分，可以发现他们之间互为异号，将其定义为lambda：<br><img src="/pictures/LTR/RankNet/img10.jpeg" alt="偏微分互为异号"></p>
<p>根据上述性质，损失函数的梯度可以改写为：<br><img src="/pictures/LTR/RankNet/img11.webp" alt="损失函数的梯度计算"></p>
<p>其中，倒数第三行就是 RankNet 的梯度向量，倒数第二行是 <strong>LambdaRank</strong> 的梯度向量，它在 RankNet 的基础上加上了 <strong>nDCG</strong> 的变化量，在优化的过程中融入了评估指标的信息。 nDCG 本身不可微，这里 <strong>只是计算出这个标量，没有涉及到梯度</strong> 的意思。</p>
<blockquote>
<p>关于这个变化量的绝对值的物理意义，我个人的理解打个不恰当的比喻就是“火上浇油、推波助澜”，如果一个样本对中的两个样本排序悬殊较大，那么互换位置后的变化量也相对较大，就会产生一个较大的梯度信息，告诉算法这俩不是一路人，你要努力地优化让他们进一步拉开差距；而如果两个样本排序悬殊较小，无论是排名靠前还是靠后，都不会产生较大的梯度信息，或者说是给 RankNet 的原始梯度打了一个大大的折扣，告诉算法这俩是“绝代双骄”或者“难兄难弟”，你不用过分地拆散他们。</p>
</blockquote>
<p>最后一行，就是 LambdaMART 的梯度向量，将 Net 换成了 GBDT。</p>
<p>工程上，XGBoost 提供了得天独厚的 LambdaMART 框架，通过指定 objective&#x3D;”rank:map”, eval_metric&#x3D;”map@n” 等参数实现。需要特别注意的是，排序任务需要对 DMatrix 数据结构设置分组信息，使用 set_group() 方法。</p>
]]></content>
      <categories>
        <category>LTR</category>
        <category>Pairwise</category>
      </categories>
      <tags>
        <tag>LTR</tag>
        <tag>Pairwise</tag>
      </tags>
  </entry>
  <entry>
    <title>Learning to Ranking</title>
    <url>/2022/10/06/LTR/Learning-to-Ranking/</url>
    <content><![CDATA[<p>参考博客：<a href="http://it.taocms.org/07/76317.htm">http://it.taocms.org/07/76317.htm</a></p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Learning to Rank (LTR) 是指一系列基于机器学习的排序算法，最初主要应用于信息检索（Information Retrieval, IR）领域，最典型的是解决搜索引擎对于搜索结果的排序问题。除了信息检索外，Learning to Rank也被应用到许多其他排序问题上，如商品推荐，计算广告，生物信息学等。</p>
<blockquote>
<p>排序学习的定义：基于机器学习中用于解决分类与回归问题的思想，提出利用机器学习方法解决排序的问题。<br>排序学习的目标：自动地从训练数据中学习得到一个排序函数，使其在文本检索中能够针对文本的相关性，重要性等衡量标准对文本进行排序。<br>机器学习的优势：整合大量复杂特征并自动进行参数调整，自动学习最优参数，降低了单一考虑排序因素的风险。同时，能够通过众多有效手段规避过过拟合问题。</p>
</blockquote>
<h3 id="LTR定义"><a href="#LTR定义" class="headerlink" title="LTR定义"></a>LTR定义</h3><p>广义定义：使用机器学习技术来解决ranking问题都统称为LTR方法。<br>狭义定义：满足以下两点的ranking方法：1. Feature Based，样本以特征向量的形式体现；2. Discriminative Training是一个基于训练机的自动学习过程。</p>
<p>一个learning-to-rank方案包括：</p>
<blockquote>
<ol>
<li>输入空间 input space，以特征向量来表示的样本，特诊时以某种方式提取出来的；</li>
<li>输出空间 output space，对input处理后的展现形式，包括以下2种：1. task的最后形式；2. Machine Learning中间便于处理的形式；</li>
<li>假设空间 hypothesis space，定义了一系列从input space到output space的映射函数；</li>
<li>损失函数 loss function，ML的目标就是在training set上定义一个loss function来学习一个optimal hypothesis。</li>
</ol>
</blockquote>
<h3 id="LTR框架"><a href="#LTR框架" class="headerlink" title="LTR框架"></a>LTR框架</h3><p>LTR是 <strong>有监督学习</strong>，因此是需要有标注的training set。经典Learning to Rank框架如下所示。<br><img src="/pictures/LTR/Learning-to-Ranking/img1.png" alt="Learning to rank框架"></p>
<p>排序学习在现代推荐架构中处于非常关键的环节，它可以完成不同召回策略的统一排序，也可将离线、近线、在线的推荐结果根据用户所处的场景进行整合和实时调整，完成打分重排并推荐给用户。美团推荐框架：<br><img src="/pictures/LTR/Learning-to-Ranking/img2.jpeg" alt="美团推荐框架"></p>
<h3 id="LTR的优缺点"><a href="#LTR的优缺点" class="headerlink" title="LTR的优缺点"></a>LTR的优缺点</h3><p>LTR 则是基于特征，通过机器学习算法训练来学习到最佳的拟合公式，相比传统的排序方法，优势有很多：</p>
<blockquote>
<ul>
<li>可以根据反馈自动学习并调整参数</li>
<li>可以融合多方面的排序影响因素</li>
<li>避免过拟合（通过正则项）</li>
<li>实现个性化需求（推荐）</li>
<li>多种召回策略的融合排序推荐（推荐）</li>
<li>多目标学习（推荐）</li>
</ul>
</blockquote>
<p>LTR的局限性：</p>
<blockquote>
<ul>
<li>可解释性差。在一个机器学习的系统当中，人们很难理解为什么一个结果比另一个结果要好。机器学习系统像一个黑盒子，大部分时候告诉我们结果1比结果2更加相关的概率，但不会告诉我们是因为什么原因结果1比结果2要好。</li>
<li>通用性差。很多information retrieval当中发现的特征很难在机器学习模型中产生效果。因为这些特征常常是针对某一类检索问题，然而对于那一类检索问题，常见的机器学习算法可能会为了模型的概括性以及防止overfitting，忽略特定的特征。这也是为什么当有了足够多的ranking engineer，大部分人都会专注改进rule-based scoring方法，去直接针对特定问题进行改进。</li>
</ul>
</blockquote>
<h2 id="Learning-to-Rank算法分类"><a href="#Learning-to-Rank算法分类" class="headerlink" title="Learning to Rank算法分类"></a>Learning to Rank算法分类</h2><p>排序学习模型通常分为三大类：<strong>单点法 Pointwise，配对法 Pairwise，和列表法 Listwise</strong>。<br>三大类的划分并不是特定的算法，而是排序学习模型的设计思路。主要区别体现在 <strong>损失函数</strong> 以及 <strong>相应的标签标注方式和优化方法</strong> 的不同。</p>
<h3 id="单点法-Pointwise"><a href="#单点法-Pointwise" class="headerlink" title="单点法 Pointwise"></a>单点法 Pointwise</h3><p><strong>单点法排序学习模型的每一个训练样本都仅仅是某一个查询关键字和某个文档的配对</strong>。他们之间是否相关，与其他文档和其他查询关键字都没有关系。<strong>单点排序学习是对现实的极大简化</strong>。</p>
<p>单点法将文档转换为特征向量后，机器学习系统根据从训练数据中学习到的分类或者回归函数对文档打分，打分结果即是搜索结果。<br>单点排序学习可以按照标注和损失函数设计的不同，将排序问题转化成回归、分类、和有序分类问题（有些文献也称有序回归）问题。<br><img src="/pictures/LTR/Learning-to-Ranking/img3.png" alt="Pointwise方法"></p>
<p>损失函数的设计思想：</p>
<blockquote>
<ul>
<li>分类（Classification）：输出空间包含的是无序类别，对每个查询-文档对的样本判断是否相关，可以是二分类的，如相关认为是正例，不相关认为是负例；也可以是类似 NDCG 那样的五级标注的多分类问题。分类模型通常会输出一个概率值，可根据概率值的排序作为排序最终结果。</li>
<li>回归（Regression）：输出空间包含的是真实值相关度得分，可通过回归来直接拟合相关度打分。</li>
<li>有序分类（Ordinal Classification）：有序分类也称有序回归（Ordinal Regression），输出空间一般包含的是有序类别，通常的做法是找到一个打分函数，然后用一系列阈值对得分进行分割，得到有序类别。</li>
</ul>
</blockquote>
<h4 id="单点法的应用"><a href="#单点法的应用" class="headerlink" title="单点法的应用"></a>单点法的应用</h4><p>推荐系统领域，最常用的就是二元分类的pointwise，比如常见的点击率CTR预估问题。之所以用的多，是因为二元分类的Pointwise <strong>模型的复杂度</strong> 通常比Pairwise和Listwise低，而且可以 <strong>借助用户的点击反馈自然地完成正负样例的标注</strong>，而其余两者的模型标注比较困难。</p>
<p>Pointwise成功地将排序问题转化为分类问题，也就意味着机器学习中常见的 <strong>分类</strong> 方法都可以直接用来解决排序问题，比如，LR, GBDT, SVM, FM以及结合DNN的各种排序算法。</p>
<h4 id="单点法的缺点"><a href="#单点法的缺点" class="headerlink" title="单点法的缺点"></a>单点法的缺点</h4><p>Pointwise 方法通过优化损失函数求解最优的参数，可以看到 Pointwise 方法非常简单，工程上也易实现，但是 Pointwise 也存在很多问题。</p>
<blockquote>
<ul>
<li>Pointwise只考虑了单个文档同query的相关性，没有考虑文档之间的关系。然而排序最求的排序的结果，只要有相对打分即可；</li>
<li>通过分类知识把不同的文档做了简单的分类，同一类别里的文档无法深入区分；</li>
<li>Pointwise方法并没有考虑同一个query对应的文档间的内部依赖性；</li>
<li>排序结果的 Top N 条的顺序重要性远比剩下全部顺序重要性要高，因为损失函数没有相对排序位置信息，这样会使损失函数可能无意的过多强调那些不重要的 docs。</li>
</ul>
</blockquote>
<h3 id="配对法-Pairwise"><a href="#配对法-Pairwise" class="headerlink" title="配对法 Pairwise"></a>配对法 Pairwise</h3><p>配对法的基本思路是对样本进行两两比较，构建偏序文档对，从比较中学习排序，因为对于一个查询关键字来说，最重要的其实不是针对某一个文档的相关性是否估计得准确，而是要能够正确估计一组文档之间的 “相对关系”。</p>
<p>每一个数据样本其实是一个比较关系，当前一个文档比后一个文档相关排序更靠前的话，就是正例，否则便是负例。</p>
<p>这里面有3个非常关键的假设，</p>
<blockquote>
<ul>
<li>针对某一个关键字得到一个完美的排序关系。在实际操作中，这歌关系可以通过相关标签得到，也可以通过其他信息获得，比如点击率等信息。然而，完美排序关系并不是永远存在的。</li>
<li>通过学习文档之间两两配对关系，从而“重构”这种完美排序。</li>
<li>构建样本来描述这样的两两相对的比较关系。一个相对比较简单的情况，认为文档之间的两两关系来自于文档特征（Feature）之间的差异。也就是说，可以利用样本之间特征的差值当做新的特征，从而学习到差值到相关性差异这样的一组对应关系。</li>
</ul>
</blockquote>
<p>Pairwise 最终的算分，分类和回归都可以实现，不过最常用的还是二元分类。<br><img src="/pictures/LTR/Learning-to-Ranking/img4.png" alt="Pairwise方法"></p>
<h4 id="配对法的应用"><a href="#配对法的应用" class="headerlink" title="配对法的应用"></a>配对法的应用</h4><p>代表算法：</p>
<blockquote>
<ul>
<li>基于 SVM 的 Ranking SVM 算法</li>
<li>基于神经网络的 RankNet 算法（2007）</li>
<li>基于 Boosting 的 RankBoost 算法（2003）</li>
</ul>
</blockquote>
<p>推荐系统中使用较多的 Pairwise 方法是贝叶斯个性化排序（Bayesian personalized ranking，BPR）。</p>
<h4 id="配对法的缺点"><a href="#配对法的缺点" class="headerlink" title="配对法的缺点"></a>配对法的缺点</h4><p>Pairwise 方法通过考虑两两文档之间的相关对顺序来进行排序，相比 Pointwise 方法有明显改善。</p>
<blockquote>
<ul>
<li>使用的是两文档之间相关度的损失函数，而它和真正衡量排序效果的指标之间存在很大不同，甚至可能是负相关的，如可能出现 Pairwise Loss 越来越低，但 NDCG 分数也越来越低的现象。</li>
<li>只考虑了两个文档的先后顺序，且没有考虑文档在搜索列表中出现的位置，导致最终排序效果并不理想。</li>
<li>不同的查询，其相关文档数量差异很大，转换为文档对之后，有的查询可能有几百对文档，有的可能只有几十个，这样不加均一化地在一起学习，模型会优先考虑文档对数量多的查询，减少这些查询的 loss，最终对机器学习的效果评价造成困难。</li>
<li>Pairwise 方法的训练样例是偏序文档对，它将对文档的排序转化为对不同文档与查询相关性大小关系的预测；因此，如果因某个文档相关性被预测错误，或文档对的两个文档相关性均被预测错误，则会影响与之关联的其它文档，进而引起连锁反应并影响最终排序结果。</li>
</ul>
</blockquote>
<h3 id="列表法-Listwise"><a href="#列表法-Listwise" class="headerlink" title="列表法 Listwise"></a>列表法 Listwise</h3><p>Listwise方法是直接优化排序列表，输入为单条样本为一个文档排列。相对于尝试学习每一个样本是否相关或者两个文档的相对比较关系，列表法排序学习的基本思路是尝试直接优化像 NDCG（Normalized Discounted Cumulative Gain）这样的指标，从而能够学习到最佳排序结果。</p>
<p>列表法排序学习有两种基本思路：</p>
<blockquote>
<ul>
<li>第一种称为 Measure-specific，就是直接针对 NDCG 这样的指标进行优化。目的简单明了，用什么做衡量标准，就优化什么目标。</li>
<li>第二种称为 Non-measure specific，则是根据一个已经知道的最优排序，尝试重建这个顺序，然后来衡量这中间的差异。</li>
</ul>
</blockquote>
<p><img src="/pictures/LTR/Learning-to-Ranking/img5.png" alt="Listwise方法"></p>
<h4 id="Measure-specific，直接针对-NDCG-类的排序指标进行优化"><a href="#Measure-specific，直接针对-NDCG-类的排序指标进行优化" class="headerlink" title="Measure-specific，直接针对 NDCG 类的排序指标进行优化"></a>Measure-specific，直接针对 NDCG 类的排序指标进行优化</h4><p>直接优化排序指标的难点在于，希望能够优化 NDCG 指标这样的 “理想” 很美好，但是现实却很残酷。**NDCG、MAP 以及 AUC ** 这类排序标准，都是在数学的形式上的 <strong>“非连续”（Non-Continuous）和 “非可微分”（Non-Differentiable）</strong>。而绝大多数的优化算法都是基于 “连续”（Continuous）和 “可微分”（Differentiable）函数的。因此，直接优化难度比较大。</p>
<p>针对这种情况，主要有这么几种解决方法，</p>
<blockquote>
<ul>
<li>找一个近似 NDCG 的另外一种指标。而这种替代的指标是 “连续” 和 “可微分” 的 。只要我们建立这个替代指标和 NDCG 之间的近似关系，那么就能够通过优化这个替代指标达到逼近优化 NDCG 的目的。这类的代表性算法的有 SoftRank 和 AppRank。</li>
<li>尝试从数学的形式上写出一个 NDCG 等指标的 “边界”（Bound），然后优化这个边界。比如，如果推导出一个上界，那就可以通过最小化这个上界来优化 NDCG。这类的代表性算法有 SVM-MAP 和 SVM-NDCG。</li>
<li>希望从优化算法上下手，看是否能够设计出复杂的优化算法来达到优化 NDCG 等指标的目的。对于这类算法来说，算法要求的目标函数可以是 “非连续” 和 “非可微分” 的。这类的代表性算法有 AdaRank 和 RankGP。</li>
</ul>
</blockquote>
<h4 id="Non-measure-specific，尝试重建最优顺序，衡量其中差异"><a href="#Non-measure-specific，尝试重建最优顺序，衡量其中差异" class="headerlink" title="Non-measure specific，尝试重建最优顺序，衡量其中差异"></a>Non-measure specific，尝试重建最优顺序，衡量其中差异</h4><p>这种思路的主要假设是，已经知道了针对某个搜索关键字的完美排序，那么怎么通过学习算法来逼近这个完美排序。我们希望缩小预测排序和完美排序之间的差距。值得注意的是，在这种思路的讨论中，优化 NDCG 等排序的指标并不是主要目的。这里面的代表有 ListNet 和 ListMLE。</p>
<h4 id="列表法和配对法的中间解法"><a href="#列表法和配对法的中间解法" class="headerlink" title="列表法和配对法的中间解法"></a>列表法和配对法的中间解法</h4><p>这类思路的核心思想，是从 NDCG 等指标中受到启发，设计出一种替代的目标函数，把直接优化列表的想法退化成优化某种配对。这个方向的代表方法就是微软发明的 LambdaRank 以及后来的 LambdaMART。</p>
<h4 id="列表法的应用"><a href="#列表法的应用" class="headerlink" title="列表法的应用"></a>列表法的应用</h4><p>代表算法：</p>
<blockquote>
<ul>
<li>基于 Measure-specific 的 SoftRank、SVM-MAP、SoftRank、LambdaRank、LambdaMART</li>
<li>基于 Non-measure specific 的 ListNet、ListMLE、BoltzRank。</li>
</ul>
</blockquote>
<p>推荐中使用较多的 Listwise 方法是 LambdaMART。</p>
<h4 id="列表法的缺点"><a href="#列表法的缺点" class="headerlink" title="列表法的缺点"></a>列表法的缺点</h4><p>列表法相较单点法和配对法针对排序问题的模型设计更加自然，解决了排序应该基于 query 和 position 问题。</p>
<blockquote>
<ul>
<li>一些算法需要基于排列来计算 loss，从而使得训练复杂度较高，如 ListNet 和 BoltzRank。</li>
<li>位置信息并没有在 loss 中得到充分利用，可以考虑在 ListNet 和 ListMLE 的 loss 中引入位置折扣因子。</li>
</ul>
</blockquote>
<h2 id="Learning-to-Ranking评估指标"><a href="#Learning-to-Ranking评估指标" class="headerlink" title="Learning to Ranking评估指标"></a>Learning to Ranking评估指标</h2><h3 id="P-K-Precision-at-K"><a href="#P-K-Precision-at-K" class="headerlink" title="P@K (Precision at K)"></a>P@K (Precision at K)</h3><p>对于现在的大规模 IR 任务，每个 query 都有大量相关的 doc，因此很难再用查全率进行衡量召回质量，但是可以用 Precision at K 对召回质量进行评价。</p>
<p>Precision at K 通常表示为 P@K， 表示 top-k 的结果中有相关结果所占比例，其中 K 表示前 K 位.</p>
<blockquote>
<p>比如，一个模型输出了一组排序，其输出的好坏依次为：好、坏、好、坏、好。那么，</p>
<ul>
<li>Prec@3 &#x3D; 2&#x2F;3</li>
<li>Prec@4 &#x3D; 2&#x2F;4</li>
<li>Prec@5 &#x3D; 3&#x2F;5</li>
</ul>
</blockquote>
<h3 id="MAP-Mean-Average-Precision"><a href="#MAP-Mean-Average-Precision" class="headerlink" title="MAP (Mean Average Precision)"></a>MAP (Mean Average Precision)</h3><p>在二分类中，常常使用Precision, Recall, ROC 曲线，AUC来评价一个模型的性能，然而这些指标很难对多分类模型进行准确的评价。</p>
<p>AP 是指的在所有Recall的可能取值情况下，得到的所有的Precision的平均值。AP衡量的是我们训练得到的模型在每个类别上的好坏，MAP衡量的是该模型在所有类别上的好坏，得到AP后，MAP的计算就变得很简单了，就是取所有AP的平均值。</p>
<p>MAP（Mean Average Precision）是信息检中的一个评价指标。MAP假定相关度有两个级别 —— 相关与不相关。 首先了解下AP（Average Precision）计算方法：<br><img src="/pictures/LTR/Learning-to-Ranking/img6.png" alt="AP的计算公式"></p>
<p>上式中，k为文档在排序列表中的位置，p(k) 为前k个结果的准确率，rel(k)表示位置k的文档是否相关，相关为1，不相关为0。MAP为一组查询AP的平均值。</p>
<blockquote>
<p>假设有两个主题，主题1有4个相关网页，主题2有5个相关网页。某系统对于主题1检索出4个相关网页，其rank分别为1, 2, 4, 7；对于主题2检索出3个相关网页，其rank分别为1,3,5。对于主题1，平均准确率为(1&#x2F;1+2&#x2F;2+3&#x2F;4+4&#x2F;7)&#x2F;4&#x3D;0.83。对于主题2，平均准确率为(1&#x2F;1+2&#x2F;3+3&#x2F;5+0+0)&#x2F;5&#x3D;0.45。则MAP&#x3D; (0.83+0.45)&#x2F;2&#x3D;0.64。</p>
</blockquote>
<h4 id="MAP优点"><a href="#MAP优点" class="headerlink" title="MAP优点"></a>MAP优点</h4><blockquote>
<ul>
<li>给出了一个代表精确度—召回率曲线下复杂区域的单一度量。这提供了每个列表的平均精度。</li>
<li>处理列表推荐物品的自然排序。这与将检索项视为集合的度量标准形成了对比。</li>
<li>这一指标能够给予发生在排序高的推荐名单中的错误更多的权重。相反，它对发生在推荐列表中较深位置的错误的权重较小。这符合在推荐列表的最前面显示尽可能多的相关条目的需要。</li>
</ul>
</blockquote>
<h4 id="MAP缺点"><a href="#MAP缺点" class="headerlink" title="MAP缺点"></a>MAP缺点</h4><blockquote>
<ul>
<li>这个度量标准适用于二进制(相关&#x2F;非相关)评级。然而，它不适合细粒度的数字评级。此度量无法从此信息中提取误差度量。</li>
<li>对于细粒度的评分，例如从 1 星到 5 星的评分，评估首先需要对评分进行阈值，以产生二元相关性。一种选择是只考虑大于 4 的评级。由于人工阈值的存在，这在评估度量中引入了偏差。</li>
</ul>
</blockquote>
<h3 id="NDGG-Normalized-Discounted-Cumulative-Gain"><a href="#NDGG-Normalized-Discounted-Cumulative-Gain" class="headerlink" title="NDGG (Normalized Discounted Cumulative Gain)"></a>NDGG (Normalized Discounted Cumulative Gain)</h3><p>在MAP中相关度只有相关、不相关两个级别。NDCG则可以定义多级相关度，相关度级别更高的文档排序更靠前。</p>
<h4 id="DCG-Discounted-Cumulative-Gain，-折扣累计增益"><a href="#DCG-Discounted-Cumulative-Gain，-折扣累计增益" class="headerlink" title="DCG (Discounted Cumulative Gain， 折扣累计增益)"></a>DCG (Discounted Cumulative Gain， 折扣累计增益)</h4><p>DCG， Discounted 的CG，就是在每一个CG的结果上处以一个折损值，为什么要这么做呢？目的就是为了让排名越靠前的结果越能影响最后的结果。</p>
<p>DCG认为应对出现在排序列表中靠后的文档进行惩罚，因此 <strong>文档相关度与其所在位置的对数成反比</strong>。 只考虑前P个文档，DCG定义为：<br><img src="/pictures/LTR/Learning-to-Ranking/img7.png" alt="DCG计算公式"></p>
<p>其中，rel_i 为位置i上文档的相关度得分，1 &#x2F; log_2(i+1)为折算因子。</p>
<p>DCG还有另外一种定义，也被经常使用。该定义 <strong>更加强调检索相关度高的文档</strong>，被广泛应用于网络搜索公司和Kaggle等机器学习竞赛中。<br><img src="/pictures/LTR/Learning-to-Ranking/img8.png" alt="DCG另一种计算公式"></p>
<h4 id="NDCG计算公式"><a href="#NDCG计算公式" class="headerlink" title="NDCG计算公式"></a>NDCG计算公式</h4><p>因为 <strong>不同的搜索结果列表长度可能有所不同</strong>，因此不能用DCG对不同搜索结果进行对比，需要 <strong>对DCG值进行归一化</strong>，即需要用到下面介绍的NDCG。首先计算位置最大可能的DCG，即理想情况的DCG（IDCG）【正确的排序结果】。</p>
<h4 id="NDCG计算例子"><a href="#NDCG计算例子" class="headerlink" title="NDCG计算例子"></a>NDCG计算例子</h4><p>假设查询q的结果列表包含5个文档，分别为D_1,..,D_5，相关度取值为1、2、3；这五个文档的相关度rel_i分别为：3、1、3、2、2；对每个文档计算 log_2(i+1) 和 rel_i &#x2F; log_2(i+1)。<br><img src="/pictures/LTR/Learning-to-Ranking/img9.png" alt="预测后前5个结果计算DCG"></p>
<p>假设处理查询结果的5个文档，另外还返回了两个文档D_6, D_7其相关度为3、1。则对这7个文档按照相关度进行排序有：3、3、3、2、2、1, 1。计算得到 <strong>按照相关性从大到小排序</strong> 后结果 IDCG_5&#x3D;8.028</p>
<p>最终计算结果为：NDCG_5 &#x3D; DCG_5 &#x2F; IDCG_5 &#x3D; 0.843</p>
<h4 id="NDCG优缺点"><a href="#NDCG优缺点" class="headerlink" title="NDCG优缺点"></a>NDCG优缺点</h4><blockquote>
<ul>
<li>NDCG 的主要优势是它考虑到了分等级的相关性值。当它们在数据集中可用时，NDCG 是一个很好的选择。</li>
<li>与 MAP 度量相比，它在评估排名项目的位置方面做得很好。它适用于二元的相关&#x2F;非相关场景。</li>
<li>平滑的对数折现因子有一个很好的理论基础，该工作的作者表明，对于每一对显著不同的排名推荐系统，NDCG 度量始终能够确定更好的一个。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>NDCG 在部分反馈方面有一些问题。当我们有不完整的评级时，就会发生这种情况。这是大多数推荐系统的情况。如果我们有完整的评级，就没有真正的任务去实现！在这种情况下，recsys 系统所有者需要决定如何归罪于缺失的评级。将缺少的值设置为 0 将把它们标记为不相关的项。其他计算值(如用户的平均&#x2F;中值)也可以帮助解决这个缺点。</li>
<li>接下来，用户需要手动处理 IDCG 等于 0 的情况。当用户没有相关文档时，就会发生这种情况。这里的一个策略是也将 NDCG 设置为 0。</li>
<li>另一个问题是处理 NDCG@K。recsys 系统返回的排序列表的大小可以小于 k。为了处理这个问题，我们可以考虑固定大小的结果集，并用最小分数填充较小的集合。</li>
</ul>
</blockquote>
<h3 id="MRR-Mean-Reciprocal-Rank"><a href="#MRR-Mean-Reciprocal-Rank" class="headerlink" title="MRR (Mean Reciprocal Rank)"></a>MRR (Mean Reciprocal Rank)</h3><p>平均倒数排名（Mean Reciprocal Rank, MRR）是一个国际上通用的对搜索算法进行评价的机制，其评估假设是 <strong>基于唯一的一个相关结果</strong>，即第一个结果匹配，分数为 1 ，第二个匹配分数为 0.5，第 n 个匹配分数为 1&#x2F;n，如果没有匹配的句子分数为0。最终的分数为所有得分之和。</p>
<p>指标反应的是我们找到的这些item是否摆在用户更明显的位置，强调位置关系，顺序性。公式如下，N表示推荐次数， <strong>1&#x2F;p表示用户真实访问的item在结果列表中的排名位置</strong>，如果没在结果序列中，则p为无穷大，1&#x2F;p为0。<br><img src="/pictures/LTR/Learning-to-Ranking/img10.png" alt="MRR计算公式"></p>
<p>如对于第一个 Query，查询结果将正确结果排名 rank 为 3，则其 Reciprocal Rank 为 1&#x2F;3，对于第二个 Query，查询结果将正确结果排名 rank 为 2，则其 Reciprocal Rank 为 1&#x2F;2，对于第三个 Query，查询结果将正确结果排名 rank 为 1，则其 Reciprocal Rank 为 1，则 MRR &#x3D; (1&#x2F;3 + 1&#x2F;2 + 1)&#x2F;3 &#x3D; 11&#x2F;18 &#x3D; 0.61。</p>
<h4 id="MRR优缺点"><a href="#MRR优缺点" class="headerlink" title="MRR优缺点"></a>MRR优缺点</h4><blockquote>
<ul>
<li>该方法计算简单，解释简单。</li>
<li>这种方法高度关注列表的第一个相关元素。它最适合有针对性的搜索，比如用户询问“对我来说最好的东西”。</li>
<li>适用于已知项目搜索，如导航查询或寻找事实。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>MRR 指标不评估推荐项目列表的其余部分。它只关注列表中的第一个项目。</li>
<li>它给出一个只有一个相关物品的列表。如果这是评估的目标，那找个度量指标是可以的。</li>
<li>对于想要浏览相关物品列表的用户来说，这可能不是一个好的评估指标。用户的目标可能是比较多个相关物品。</li>
</ul>
</blockquote>
<h3 id="ERR-Expected-Reciprocal-Rank"><a href="#ERR-Expected-Reciprocal-Rank" class="headerlink" title="ERR (Expected Reciprocal Rank)"></a>ERR (Expected Reciprocal Rank)</h3><p>倒序排名期望ERR是受到cascade model的启发。点击模型中的cascade model，考虑到在同一个检索结果列表中各文档之间的位置依赖关系，假设用户从上至下查看，如果遇到某一检索结果项满意并进行点击，则操作结束；否则跳过该项继续往后查看。</p>
<p>ERR (倒数排名的期望)，表示用户的需求被满足时停止的位置的倒数的期望，与 MRR 计算第一个相关文档的位置倒数不同。</p>
<h4 id="Cascade-Models"><a href="#Cascade-Models" class="headerlink" title="Cascade Models"></a>Cascade Models</h4><p>之前的评分模型虽然考虑了 <strong>位置自身的价值信息</strong> 和 <strong>位置上文档的相关度信息</strong>，但是没有考虑 <strong>文档之间</strong> 的相关性信息。</p>
<blockquote>
<p>一种考虑是，一个文档是否被用户点击和排在它前面的文档有很大的关系，比如排在前面的文档都是不相关文档，那么它被点击的概率就高，如果排它前面的文档都是非常相关的文档，那么它被点击的概率就很低。</p>
</blockquote>
<p>Cascade Models假设用户从排名由高到底依次查看文档，一旦文档满足了用户的需求，则停止查看后续的文档。用R_i表示用户只看在位置i上的文档后就不在需要查看其它文档的概率，显然文档的相关度越高，R_i越大。那么用户在位置 r 停止的概率公式如下：<br><img src="/pictures/LTR/Learning-to-Ranking/img11.png" alt="用户在位置r处停止的概率公式"></p>
<h4 id="ERR计算公式"><a href="#ERR计算公式" class="headerlink" title="ERR计算公式"></a>ERR计算公式</h4><p>区别于上述计算公式，R_i是关于文档相关度等级的函数，可以取如下的函数，其中g表示原始的相关度等级：<br><img src="/pictures/LTR/Learning-to-Ranking/img12.png" alt="文档相关度等级函数"></p>
<p>那么ERR计算公式如下：<br><img src="/pictures/LTR/Learning-to-Ranking/img13.png" alt="ERR计算公式"></p>
<h4 id="ERR优缺点"><a href="#ERR优缺点" class="headerlink" title="ERR优缺点"></a>ERR优缺点</h4><blockquote>
<p>NDCG和ERR指标的优势在于，它们对doc的相关性划分多个（&gt;2）等级，而MRR和MAP只会对doc的相关性划分2个等级（相关和不相关）。并且，这些指标都包含了doc位置信息（给予靠前位置的doc以较高的权重），这很适合于web search。</p>
</blockquote>
<blockquote>
<p>然而，这些指标的缺点是不平滑、不连续，无法求梯度，如果将这些指标直接作为模型评分的函数的话，是无法直接用梯度下降法进行求解的。</p>
</blockquote>
]]></content>
      <categories>
        <category>排序模型</category>
      </categories>
      <tags>
        <tag>LTR</tag>
      </tags>
  </entry>
  <entry>
    <title>BERT</title>
    <url>/2022/09/04/NLP/BERT/</url>
    <content><![CDATA[<p>BERT全称为Bidirectional Encoder Representation from Transformers，是一个预训练的语言表征模型。<br>它强调了不再像以往一样采用传统的单向语言模型或者把两个单向语言模型进行浅层拼接的方法进行预训练，而是采用新的 <strong>masked language model（MLM）</strong> ，以致能生成深度的双向语言表征。</p>
<p>该模型的主要优点：</p>
<ul>
<li>采用MLM对双向的Transformers进行预训练，以生成深层的双向语言表征；</li>
<li>预训练后，只需要添加一个额外的输出层进行fine-tune，就可以在各种各样的下游任务中取得优异的表现。在此过程中不需要对BERT结构进行修改。</li>
</ul>
<h3 id="BERT提出动机"><a href="#BERT提出动机" class="headerlink" title="BERT提出动机"></a>BERT提出动机</h3><p>预训练语言模型对于下游很多自然语言处理任务都有着显著改善。现有的训练模型的网络结构限制了模型本身的表达能力，最主要的限制就是没有采用<strong>双向编码</strong>的方法来对输入进行编码。这就导致模型只能看见当前时刻之前的信息，而不能同时捕捉当前时刻之后的信息。</p>
<p>在论文中，作者提出了采用BERT(Bidirectional Encoder Representations from Transformers)这一网络结构来实现模型的双向编码学习能力。同时，为了使得模型能够有效的学习到双向编码的能力，BERT在训练过程中使用了基于掩盖的语言模型(Masked Language Model, MLM)，即随机对输入序列中的某些位置进行遮蔽，然后通过模型来对其进行预测。</p>
<p>由于MLM 预测任务能够使得模型编码得到的结果同时包含上下文的语境信息，因此有利于训练得到更深的BERT网络模型。除此之外，在训练BERT的过程中作者还加入了下句预测任务(Next Sentence Prediction, NSP)， 即同时输入两句话到模型中，然后预测第 2 句话是不是第 1 句话的下一句话。</p>
<h3 id="BERT网络结构"><a href="#BERT网络结构" class="headerlink" title="BERT网络结构"></a>BERT网络结构</h3><p>BERT网络结构整体上就是由多层的Transformer Encoder堆叠所形成。其上半部分的结构与之前介绍的Transformer Encoder差不多，只不过在Input部分多了一个<strong>Segment Embedding</strong>。</p>
<h4 id="Input-Embedding"><a href="#Input-Embedding" class="headerlink" title="Input Embedding"></a>Input Embedding</h4><p>在 BERT 中 Input Embedding 模块主要包含三个部分:Token Embedding、Positional Embedding 和 Segment Embedding。</p>
<blockquote>
<ul>
<li>这里需要注意的是 BERT 中的 Positional Embedding 对于每个位置的编码并不是采用公式计算得到，而是类似普 通的词嵌入一样为每一个位置初始化了一个向量，然后随着网络一起训练得到。BERT 开源的预训练模型最大只支持 512 个字符的长度，这是因为其在训练过程中(位置)词表的最大长度只有 512。</li>
<li>Segment Embedding 的作用是用来区分输入序列中的不同部分，其本质就是通过一个普通的词嵌入来区分每一个序列所处的位置。</li>
</ul>
</blockquote>
<p>最后，将这3个Embedding进行相加（并进行标准化）便得到了最终的Input Embedding部分的输出。<br><img src="/pictures/NLP/BERT/img1.png" alt="BERT的Embedding输入"><br>最上面的 Input 表示原始的输入序列，其中第一个字符“[CLS]” 是一个特殊的分类标志，如果下游任务是做文本分类的话，那么在 BERT 的输出 结果中可以只取“[CLS]”对应的向量进行分类即可**(不过实验表明，取所有位置向量的均值往往有着更好的效果)**；而其中的“[SEP]”字符则是用来作为将两句话分开的标志。</p>
<h4 id="BERT-Encoder"><a href="#BERT-Encoder" class="headerlink" title="BERT Encoder"></a>BERT Encoder</h4><p>在论文中作者分别用 L 来表示 BertLayer 的层数，即 BertEncoder 是由 L 个 BertLayer 所构成;用 H 来表示模型的维度;用 A 来表示多头注意力中多头的个数。<br>同时，在论文中作者分别就 BERT_BASE (L&#x3D;12, H&#x3D;768, A&#x3D;12) 和 BERT_LARGE (L&#x3D;24,H&#x3D;1024,A&#x3D;16)这两种尺寸的 BERT 模型进行了实验对比。</p>
<h4 id="MLM与NSP"><a href="#MLM与NSP" class="headerlink" title="MLM与NSP"></a>MLM与NSP</h4><p>对于 MLM 任务来说，其做法是随机掩盖掉输入序列中15% 的 Token(即用“[MASK]”替换掉原有的 Token)，然后在 BERT 的输出结果中 取对应掩盖位置上的向量进行真实值预测。<br>虽然 MLM 的这种做法能够得到一个很好的预训练模型，但 是仍旧存在不足之处。由于在 fine-tuning 时，由于输入序列中并不存在“[MASK]” 这样的 Token，因此这将导致 pre-training 和 fine-tuning 之间存在不匹配不一致的 问题(GAP)。<br>为了解决这一问题，作者在原始 MLM 的基础了做了部分改动，即先选定15% 的Token，然后将其中的80%替换为“[MASK]”、10%随机替换为其它Token、 剩下的10% 不变。最后取这15% 的 Token 对应的输出做分类来预测其真实值。</p>
<p>由于很多下游任务需要依赖于分析两句话之间的关系来进行建模，例如问题 回答等。为了使得模型能够具备有这样的能力，作者在论文中又提出了二分类的 下句预测任务。<br>具体地，对于每个样本来说都是由 A 和 B 两句话构成，其中50%的情况 B 确实为 A 的下一句话(标签为 IsNext)，另外的50%的情况是 B 为语料中其它 的随机句子(标签为 NotNext)，然后模型来预测 B 是否为 A 的下一句话。<br><img src="/pictures/NLP/BERT/img2.png" alt="BERT的训练任务"></p>
<blockquote>
<p>总的来说，如果单从网络结构上来看 BERT 并没有太大的创新，这也正如作 者所说“BERT 整体上就是由多层的 Transformer Encoder 堆叠而来”，并且所谓 的“bidirectional”其实指的也就是 Transformer 中的 self-attention 机制。同时， 在掌柜看来真正让 BERT 表现出色的应该是基于 MLM 和 NSP 这两种任务的预 训练过程，使得训练得到的模型具有强大的表征能力。</p>
</blockquote>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>FastText</title>
    <url>/2022/09/04/NLP/FastText/</url>
    <content><![CDATA[<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>fasttext是facebook开源的一个词向量与文本分类工具，在2016年开源，典型应用场景是“带监督的文本分类问题”。提供简单而高效的 <strong>文本分类</strong> 和 <strong>表征学习</strong> 的方法，性能比肩深度学习而且速度更快。</p>
<p>FastText结合了自然语言处理和机器学习中的思想：</p>
<ul>
<li>使用词袋以及N-gram袋表征语句；</li>
<li>使用子字信息；</li>
<li>通过隐藏表征在类别间共享信息</li>
</ul>
<h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>核心思想：将整篇文档的词以及N-gram向量 <strong>叠加平均</strong> 得到文档向量，然后使用文档向量做softmax分类。</p>
<h4 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h4><p>FastText架构和CBOW架构类似。不同之处在于，FastText预测标签，CBOW通过上下文预测中间词。<br><img src="/pictures/NLP/FastText/img1.jpg" alt="FastText模型架构"></p>
<blockquote>
<p><strong>注意：</strong> 此架构图并没有展示词向量的训练过程。FastText模型也只有三层：输入层，隐含层，输出层。输入是多个向量表示的单词，输出是特定的Target，隐含层是对多个词向量的叠加平均。</p>
</blockquote>
<p>与CBOW模型不同的是，</p>
<ul>
<li>CBOW的输入是目标单词的上下文，FastText的输入是多个单词以及其N-gram特征，这些特征用来表示单个文档；</li>
<li>CBOW的输入单词是被one-hot编码过后的，FastText的输入特征是被embedding后的；</li>
<li>CBOW的输出是目标词汇，FastText的输出是文档对应的类标。</li>
</ul>
<h4 id="层序softmax"><a href="#层序softmax" class="headerlink" title="层序softmax"></a>层序softmax</h4><p>对于有大量类别的数据集，FastText使用一个分层分类器，降低计算复杂度。<br>FastText利用了类别不均衡这个事实，通过使用Huffman算法建立用于表征类别的树形结构。因此，出现频次较高的类别更加接近根节点。</p>
<h4 id="N-gram特征"><a href="#N-gram特征" class="headerlink" title="N-gram特征"></a>N-gram特征</h4><p>Word2Vec把语料库中的每个单词作为一个原子。这忽略了单词内部的形态特征，比如apple和apples。传统的Word2Vec中，这种单词内部形态信息因为它们被转换为不同的id丢失了。</p>
<p>为了克服这个问题，FastText使用了字符级别的N-gram来表示一个单词。对于单词apple，假设N的取值为3，则它的trigram有：</p>
<blockquote>
<p>“&lt;ap”,  “app”,  “ppl”,  “ple”, “le&gt;”</p>
</blockquote>
<p>其中，&lt;表示前缀，&gt;表示后缀。于是，我们可以用这些trigram来表示“apple”这个单词，进一步，我们可以用这5个trigram的向量叠加来表示“apple”的词向量。</p>
<p>这带来两点好处：</p>
<ul>
<li>对于低频词生成的词向量效果会更好。因为它们的N-gram可以和其他词共享；</li>
<li>对于训练词库之外的单词，仍然可以构建它们的词向量。我们可以叠加它们的字符级N-gram向量。</li>
</ul>
<h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><p><strong>模型搭建步骤：</strong></p>
<ol>
<li>添加输入层：Embedding层输入是一批文档，每个文档有一个词汇索引序列构成；</li>
<li>添加隐含层：投影层对一个文档中所有单词的向量进行叠加平均；</li>
<li>添加输出层：Softmax层</li>
<li>指定损失函数，优化器类型，评价指标，编译模型。</li>
</ol>
<p><strong>训练数据feed模型步骤</strong></p>
<ol>
<li>将文档分好词，构建词汇表；</li>
<li>对类标进行one-hot化；</li>
<li>对一批文本，将每个文本转化为词索引序列，每个类标转化为one-hot向量</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><h4 id="如何获得词向量？"><a href="#如何获得词向量？" class="headerlink" title="如何获得词向量？"></a>如何获得词向量？</h4><p><strong>累加平均</strong> 一个词的N-gram集合包括自身整个词的隐向量表示。</p>
<h4 id="FastText和Word2Vec区别"><a href="#FastText和Word2Vec区别" class="headerlink" title="FastText和Word2Vec区别"></a>FastText和Word2Vec区别</h4><p>相似处：</p>
<blockquote>
<ol>
<li>图模型结构很想，都是采用embedding向量形式，得到word的隐向量表达；</li>
<li>采用很多相似的优化，比如使用层级sotmax优化训练和预测中的打分速度</li>
</ol>
</blockquote>
<p>不同之处：</p>
<blockquote>
<ol>
<li>输入层：Word2Vec的输出层，是context window内的term；而FastText对应的整个sentence的内容，包括term和N-gram的内容；</li>
<li>输出层：Word2Vec的输出层，对应的是每个term，计算某个term的概率最大；而FastText的输出层对应的分类的label</li>
</ol>
</blockquote>
<p>两者本质的不同，体现在H-softmax的使用：</p>
<blockquote>
<ul>
<li>Word2Vec的目标是得到词向量，该词向量最终是在输入层中得到。输出层对应的H-softmax也会生成一系列的向量，但最终都会被抛弃；</li>
<li>FastText充分利用H-softmax的分类功能，遍历分类树的所有叶节点，找到概率最大的label</li>
</ul>
</blockquote>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>FastText的学习速度比较快，效果不错。fastText适用于分类类别非常大而且数据集足够多的情况。当分类类别比较小或者数据集比较少的话，很容易过拟合。</p>
<p>可以完成无监督的词向量的学习，可以学习得到词向量；也可以用于有监督学习的分类任务（新闻文本分类，垃圾邮件分类，情感分析，电商中用户评论的褒贬分析）</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>RoBERTa</title>
    <url>/2022/10/05/NLP/RoBERTa/</url>
    <content><![CDATA[<p>参考博客：<a href="https://blog.csdn.net/Decennie/article/details/120010025">https://blog.csdn.net/Decennie/article/details/120010025</a></p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>RoBERTa 模型是BERT 的改进版(A Robustly Optimized BERT，即简单粗暴称为强力优化的BERT方法)。</p>
<p>在 <strong>模型规模、算力和数据</strong> 上，与BERT相比主要有以下几点改进：</p>
<blockquote>
<ol>
<li>更大的模型参数量（论文提供的训练时间来看，模型使用 1024 块 V100 GPU 训练了 1 天的时间）</li>
<li>更大bacth size。RoBERTa 在训练过程中使用了更大的bacth size。尝试过从 256 到 8000 不等的bacth size。</li>
<li>更多的训练数据（包括：CC-NEWS 等在内的 160GB 纯文本。而最初的BERT使用16GB BookCorpus数据集和英语维基百科进行训练）</li>
</ol>
</blockquote>
<p>另外，RoBERTa在 <strong>训练方法</strong> 上有以下改进：</p>
<blockquote>
<ol>
<li>去掉下一句预测(NSP)任务</li>
<li>动态掩码。BERT 依赖随机掩码和预测 token。原版的 BERT 实现在数据预处理期间执行一次掩码，得到一个静态掩码。 而 RoBERTa 使用了动态掩码：每次向模型输入一个序列时都会生成新的掩码模式。这样，在大量数据不断输入的过程中，模型会逐渐适应不同的掩码策略，学习不同的语言表征。</li>
<li>文本编码。Byte-Pair Encoding（BPE）是字符级和词级别表征的混合，支持处理自然语言语料库中的众多常见词汇。原版的 BERT 实现使用字符级别的 BPE 词汇，大小为 30K，是在利用启发式分词规则对输入进行预处理之后学得的。Facebook 研究者没有采用这种方式，而是考虑用更大的 byte 级别 BPE 词汇表来训练 BERT，这一词汇表包含 50K 的 subword 单元，且没有对输入作任何额外的预处理或分词。</li>
</ol>
</blockquote>
<h3 id="对于NSP任务的解析和改用"><a href="#对于NSP任务的解析和改用" class="headerlink" title="对于NSP任务的解析和改用"></a>对于NSP任务的解析和改用</h3><p>NSP这个任务在预训练类型的训练中，一直被人诟病，觉得作用不大。SpanBERT文章,也弃用了NSP，而是 <strong>直接把上下句当成一个长句的语料</strong>，进行训练。觉得超长句语料可以让预训练模型学习的更好（这个观点其实在Roberta里实验里论证过了，更长的句子更利于模型获取更多上下文的信息，利于模型参数训练）。</p>
<p>关于NSP为什么作用不大，可能有以下原因：</p>
<blockquote>
<ol>
<li>超长句相比于NSP上下句的分割模式，利于模型学习更长上下文信息，也利于模型参数训练（但这也必然带来训练的时间、开销变大）；</li>
<li>在 NSP 的负例构造，上下句来自于不同的文档组装，会给 MLM 任务带来很大噪音。</li>
</ol>
</blockquote>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>RoBERTa建立在BERT的语言掩蔽策略的基础上，修改BERT中的关键超参数，包括 <strong>删除BERT的下一个句子训练前目标</strong>，以及 <strong>使用更大的bacth size和学习率进行训练</strong>。RoBERTa也接受了比BERT多一个数量级的训练，时间更长。这使得RoBERTa表示能够比BERT更好地推广到下游任务。</p>
<h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>在 DGX-1 机器上使用混合精度浮点算法进行训练，每台机器都有 8×32GB Nvidia V100 GPU。</p>
<h3 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h3><p>RoBERTa 采用 160 G 训练文本，远超 BERT 的 16G 文本，其中包括：</p>
<blockquote>
<ol>
<li>BOOKCORPUS 和英文维基百科：原始 BERT 的训练集，大小 16GB。</li>
<li>CC-NEWS：包含2016年9月到2019年2月爬取的6300万篇英文新闻，大小 76 GB（经过过滤之后）。</li>
<li>OPENWEBTEXT：从 Reddit 上共享的 URL （至少3个点赞）中提取的网页内容，大小 38 GB 。</li>
<li>STORIES：CommonCrawl 数据集的一个子集，包含 Winograd 模式的故事风格，大小 31GB 。</li>
</ol>
</blockquote>
<h3 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h3><p>使用以下三个基准评估下游任务的预训练模型:</p>
<blockquote>
<ol>
<li>GLUE 通用语言理解评估（GLUE）基准是用于评估自然语言理解系统的 9 个数据集的集合。</li>
<li>SQuAD 斯坦福问题答疑数据集（SQuAD）提供了一段背景和一个问题。任务是通过从上下文中提取相关跨度来回答问题。</li>
<li>RACE 考试的重新理解（RACE）任务是一个大型阅读理解数据集，有超过 28000 个段落和近100000 个问题。该数据集来自中国的英语考试，专为中学生和高中生设计。</li>
</ol>
</blockquote>
<h2 id="训练过程分析"><a href="#训练过程分析" class="headerlink" title="训练过程分析"></a>训练过程分析</h2><p>本节探讨在保持模型架构不变的情况下，哪些量化指标对预训练BERT模型有影响。首先维持训练BERT模型架构不变，其配置与BERT-base相同（L &#x3D; 12， H &#x3D; 768， A &#x3D; 12，110M 参数）。</p>
<h3 id="静态-VS-动态mask"><a href="#静态-VS-动态mask" class="headerlink" title="静态 VS 动态mask"></a>静态 VS 动态mask</h3><h4 id="原始静态mask"><a href="#原始静态mask" class="headerlink" title="原始静态mask"></a>原始静态mask</h4><p>BERT中是预处理训练数据时，每个样本只会进行 <strong>一次随机mask</strong> （因此每个epoch都是重复），后续的每个训练步都采用相同的mask，这是原始静态mask，即单个静态mask，这是原始 BERT 的做法。</p>
<h4 id="修改版静态mask"><a href="#修改版静态mask" class="headerlink" title="修改版静态mask"></a>修改版静态mask</h4><p>在预处理的时候将数据集 <strong>拷贝 10 次</strong>，每次拷贝采用 <strong>不同的 mask</strong>（总共40 epochs，所以每一个mask对应的数据被训练4个epoch）。这等价于 <strong>原始的数据集采用10种静态 mask 来训练 40个 epoch</strong>。</p>
<h4 id="动态mask"><a href="#动态mask" class="headerlink" title="动态mask"></a>动态mask</h4><p>并没有在预处理的时候执行 mask，而是在 <strong>每次向模型提供输入时动态生成 mask</strong> ，所以是时刻变化的。</p>
<h4 id="实验结论"><a href="#实验结论" class="headerlink" title="实验结论"></a>实验结论</h4><p>不同模式的实验效果如下表所示。其中 reference 为BERT 用到的原始静态 mask，static 为修改版的静态mask。<br><img src="/pictures/NLP/RoBERTa/img1.png" alt="静态VS动态mask效果对比"></p>
<p>从Table1中可以看出，修改版的静态mask与BERT原始静态mask效果相当；动态mask又与静态mask效果差不多，或者说略好了静态mask。基于上述结果的判断，及其动态mask在效率上的优势，<strong>本文后续的实验统一采用动态mask</strong>。</p>
<h3 id="Model-Input-Format-and-NSP"><a href="#Model-Input-Format-and-NSP" class="headerlink" title="Model Input Format and NSP"></a>Model Input Format and NSP</h3><p>原始的BERT包含2个任务，预测被mask掉的单词和下一句预测。鉴于最近有研究(Lample and Conneau,2019; Yang et al., 2019; Joshi et al., 2019)开始质疑下一句预测(NSP)的必要性，本文设计了以下4种训练方式：</p>
<h4 id="SEGMENT-PAIR-NSP"><a href="#SEGMENT-PAIR-NSP" class="headerlink" title="SEGMENT-PAIR + NSP"></a>SEGMENT-PAIR + NSP</h4><p>输入包含两部分，每个部分是来自同一文档或者不同文档的 segment （segment 是连续的多个句子），这两个segment 的token总数少于 512 。预训练包含 MLM 任务和 NSP 任务。这是原始 BERT 的做法。</p>
<h4 id="SENTENCE-PAIR-NSP"><a href="#SENTENCE-PAIR-NSP" class="headerlink" title="SENTENCE-PAIR + NSP"></a>SENTENCE-PAIR + NSP</h4><p>输入也是包含两部分，每个部分是来自同一个文档或者不同文档的单个句子，这两个句子的token 总数少于 512 。由于这些输入明显少于512 个tokens，因此增加batch size的大小，以使 tokens 总数保持与SEGMENT-PAIR + NSP 相似。预训练包含 MLM 任务和 NSP 任务。</p>
<h4 id="FULL-SENTENCES"><a href="#FULL-SENTENCES" class="headerlink" title="FULL-SENTENCES"></a>FULL-SENTENCES</h4><p>输入只有一部分（而不是两部分），来自同一个文档或者不同文档的连续多个句子，token 总数不超过 512 。输入可能跨越文档边界，如果跨文档，则在上一个文档末尾添加文档边界token 。预训练不包含 NSP 任务。</p>
<h4 id="DOC-SENTENCES"><a href="#DOC-SENTENCES" class="headerlink" title="DOC-SENTENCES"></a>DOC-SENTENCES</h4><p>输入只有一部分（而不是两部分），输入的构造类似于FULL-SENTENCES，只是不需要跨越文档边界，其输入来自同一个文档的连续句子，token 总数不超过 512 。在文档末尾附近采样的输入可以短于 512个tokens， 因此在这些情况下动态增加batch size大小以达到与 FULL-SENTENCES 相同的tokens总数。预训练不包含 NSP 任务。</p>
<h4 id="实验结论-1"><a href="#实验结论-1" class="headerlink" title="实验结论"></a>实验结论</h4><p><img src="/pictures/NLP/RoBERTa/img2.png" alt="训练方式对比"></p>
<p>BERT采用 <strong>SEGMENT-PAIR</strong> 输入格式。如果在采用NSP loss情况下，SEGMENT-PAIR优于SENTENCE-PAIR。单个句子会损害下游任务，可能是因为句子过短，模型没有充分学习。</p>
<p>在不采用NSP loss情况下，用来自单个文档的文本块进行训练。该设置性能优于最初发布的BRRT-base结果。与原始BERT相比，去掉NSP损失能够使得下游任务的表现持平或者略微升高。</p>
<p>实验还发现将序列限制为来自单个文档(DOC-SENTENCES)的性能略好于序列来自多个文档(FULL-SENTENCES)。</p>
<h3 id="Training-with-large-batches"><a href="#Training-with-large-batches" class="headerlink" title="Training with large batches"></a>Training with large batches</h3><p>通过梯度累积，训练batch size&#x3D;2K序列的125K步，或batch size&#x3D;8K的31K步，这两者在计算成本上大约是是等价的。</p>
<p>large batches训练提高了masked language modeling 目标的困惑度，以及最终任务的准确性。large batches也更容易分布式数据并行训练， 在后续实验中，文本使用bacth size&#x3D;8K进行并行训练。</p>
<h3 id="Text-Encoding"><a href="#Text-Encoding" class="headerlink" title="Text Encoding"></a>Text Encoding</h3><p>字节对编码(BPE)(Sennrich et al.,2016)是字符级和单词级表示的混合，该编码方案可以处理自然语言语料库中常见的大量词汇。BPE不依赖于完整的单词，而是依赖于子词(sub-word)单元，这些子词单元是通过对训练语料库进行统计分析而提取的，其词表大小通常在 1万到 10万之间。当对海量多样语料建模时，unicode characters占据了该词表的大部分。</p>
<blockquote>
<p>基于 char-level ：原始 BERT 的方式，它通过对输入文本进行启发式的词干化之后处理得到。<br>基于 bytes-level：与 char-level 的区别在于bytes-level 使用 bytes 而不是 unicode 字符作为 sub-word 的基本单位，因此可以编码任何输入文本而不会引入 UNKOWN 标记。</p>
</blockquote>
<p>当采用 bytes-level 的 BPE 之后，词表大小从3万（原始 BERT 的 char-level ）增加到5万。这分别为 BERT-base和 BERT-large增加了1500万和2000万额外的参数。</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>K-BERT</title>
    <url>/2022/09/09/NLP/K-BERT/</url>
    <content><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>预训练的语言表示模型从大型语料库中捕获一般语言的表示，但是 <strong>缺乏领域特定的知识</strong>。</p>
<p>过多的知识加入会使得 <strong>句子偏离正确的含义</strong>，这就是知识噪声问题。</p>
<blockquote>
<p>如何将外部知识整合到模型中成了一个关键点，这一步通常存在两个难点：</p>
<ul>
<li>异构嵌入空间（Heterogeneous Embedding Space）： 即文本的单词embedding和知识库的实体embedding通常是通过不同方式获取的，使得向量空间不一致。</li>
<li>知识噪声（Knowledge Noise）： 即过多的知识融合可能会使原始句子偏离正确的本意。</li>
</ul>
</blockquote>
<p>为了克服上述问题，K-Bert引入了<strong>软定位</strong> 和 <strong>可见矩阵</strong> 来限制插入知识的影响。</p>
<p>K-BERT能够从预先训练好的BERT中加载模型参数，因此不需要单独的预训练，只需要一个KG数据，K-BERT就很容易将领域知识注入到模型中。</p>
<h3 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h3><p>K-BERT模型主要包括四部分：知识层（Knowledge layer）、嵌入层（Embedding layer）、可见层（Seeing layer）和 Mask-Transformer编码层（Mask-Transformer Encoder）<br><img src="/pictures/NLP/K-BERT/img1.jpg" alt="K-BERT模型架构"></p>
<p>如上图所示，K-BERT主要由4个层组成，分别是：</p>
<ul>
<li>knowledge layer：知识层，顾名思义是将知识图谱的事实融入到输入中，构建 <strong>sentence tree</strong> 作为新的输入。</li>
<li>embedding layer：将sentence tree 进行embedding，转换成向量表达。</li>
<li>seeing layer：该层的作用是为了避免知识噪声（Knowledge Noise）而引入的，主要是通过构建 visible matrix，<strong>限定每个字只能够看到跟自己相关的上下文以及知识</strong>，从而避免了知识噪声的引入。</li>
<li>mask-transformer：mask-transformer 是在对transformer的一个改进，对于其中的self-attention，根据 <strong>visible matrix 限制了每个字的attention的范围</strong>，避免了字对于其他无相关的信息的关注。</li>
</ul>
<p>对于一个输入的句子，</p>
<ol>
<li>knowledge layer 首先是从知识图谱KG中找到相关的三元组，</li>
<li>然后将这些三元组插入到输入的 input sentence 中，形成知识丰富（knowledge-rich）的句子树（sentence tree）。</li>
<li>句子树然后同时输入给 embedding layer 以及 seeing layer，从而获得一个 token 级别的 embedding 表示以及一个 visible matrix。</li>
<li>这个visible matrix 是用来控制每个token的可见域（visible scope），以防止输入的句子因为太多的知识嵌入而发生意思的改变。</li>
</ol>
<h4 id="Knowledge-Layer"><a href="#Knowledge-Layer" class="headerlink" title="Knowledge Layer"></a>Knowledge Layer</h4><p>知识层主要用于句子知识嵌入（knowledge injection）以及句子树（sentence tree）的转换。<br><img src="/pictures/NLP/K-BERT/img2.jpg" alt="句子树结构"></p>
<p>举例说明句子树的构建：<br><img src="/pictures/NLP/K-BERT/img3.jpg" alt="句子树构建的例子"></p>
<h4 id="Embedding-Layer"><a href="#Embedding-Layer" class="headerlink" title="Embedding Layer"></a>Embedding Layer</h4><p>和 BERT 类似，输入的句子需要经过embedding，作为模型的输入。具体 embedding 由三个部分组成，分别是 token embedding，soft position embedding 以及 segment embedding。<br><img src="/pictures/NLP/K-BERT/img4.jpg" alt="Embedding表示"></p>
<h5 id="token-embedding"><a href="#token-embedding" class="headerlink" title="token embedding"></a>token embedding</h5><p>token embbeding 是将句子中的每个 token 通过look up table 映射成为一个维度为 H 的向量表示。此外，每个句子的开头有一个 [CLS] 这个特殊token，主要是为了句子分类的作用，同时 [MAKS] 是作为mask任务使用的。</p>
<h5 id="soft-position-embedding"><a href="#soft-position-embedding" class="headerlink" title="soft-position embedding"></a>soft-position embedding</h5><p>我们可以发现，BERT 使用的时position embedding，并且使用的是绝对的position 表示。<br><img src="/pictures/NLP/K-BERT/img5.jpg" alt="Sentence Tree"></p>
<p>如果使用BERT的position embedding方式，即hard-position index。这就导致<strong>原本的句子顺序发生了变化，失去了句子主干的信息位置</strong>。<br>解决方案就是：使用soft-position index。</p>
<p>这就引发了另一个问题：知识噪音。一般字会给周围其他的字很大的attention score。<br>解决方案：引入seeing layer，控制self-attention的可见域。</p>
<h4 id="seeing-layer"><a href="#seeing-layer" class="headerlink" title="seeing layer"></a>seeing layer</h4><p>Seeing layer是BERT和K-BERT之间最大的不同。</p>
<blockquote>
<p>我们插入的知识，只作用于它自身的三元组中的元素，对于其他的token，不产生任何影响。</p>
</blockquote>
<p>根据上述规则，我们可以得到一个visible matrix：<br><img src="/pictures/NLP/K-BERT/img6.jpg" alt="Visible Matrix"></p>
<p>具体可见下图，红色表示可见区域，白色表示不可见区域。<br><img src="/pictures/NLP/K-BERT/img7.jpg" alt="Visible Matrix应用"></p>
<h4 id="mask-attention"><a href="#mask-attention" class="headerlink" title="mask-attention"></a>mask-attention</h4><p>我们可以认为 visible matrix 获得了它的 sentence tree 的结构信息，我们根据这个矩阵构造 mask-self-attention, 实现了在嵌入知识的情况下，不增加噪音的目的。具体公式如下：<br><img src="/pictures/NLP/K-BERT/img8.png" alt="mask-self-attention计算公式"></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>K-BERT 主要的创新点是将知识图谱的事实三元组融入到了预训练的语言模型中，并且不要我们自己进行预训练的操作，只需要在 fine-tuning 以及 inference 阶段进行知识嵌入即可，大大地方便了使用，并且在知识驱动的任务，例如QA，NER，推理任务中取得了很好的效果。</p>
<blockquote>
<p>文章主要解决了两个问题，包括了</p>
<ul>
<li>如何将异质向量空间（heterogeneous embedding space）的知识和预训练的语言空间进行结合，主要就是采用了knowledge layer 结合知识构建 sentence tree。</li>
<li>另外就是在引入了 knowledge 之后，如何避免 knowledge noise，这边就是采用 soft position embedding 以及 seeing layer 中产生的 visible matrix，通过改造 transformer 的self-attetion 为 mask self-attention，控制每个 token 的可见域，从而解决KN问题。</li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer</title>
    <url>/2022/09/04/NLP/Transformer/</url>
    <content><![CDATA[<p>参考资料：<a href="https://zhuanlan.zhihu.com/p/420820453">https://zhuanlan.zhihu.com/p/420820453</a></p>
<h3 id="多头注意力机制"><a href="#多头注意力机制" class="headerlink" title="多头注意力机制"></a>多头注意力机制</h3><h4 id="提出背景"><a href="#提出背景" class="headerlink" title="提出背景"></a>提出背景</h4><p>现在主流的序列模型都是基于复杂的循环神经网络或者卷积神经网络构造而来的Encoder-Decoder模型。传统的Encoder-Decoder架构在建模过程中，下一时刻的计算过程会很依赖于上一时刻的输出，这种固有属性导致很难以 <strong>并行</strong> 的方式进行计算。<br>基于以上原因，提出了一种全新的Transformer架构来解决这一问题。Transformer架构的优点在于它完全摒弃了传统的循环结构，取而代之的是通过 <strong>注意力机制</strong> 来计算模型输入与输出的隐含表示，即自注意力机制。<br><img src="/pictures/NLP/Transformer/img1.png" alt="Transformer的网络结构"></p>
<blockquote>
<p>自注意力机制就是通过某种运算直接计算得到句子在编码过程中每个位置上的注意力权重；然后再以权重和的形式来计算得到整个句子的隐含向量表示。</p>
</blockquote>
<h4 id="什么是self-attention"><a href="#什么是self-attention" class="headerlink" title="什么是self-attention?"></a>什么是self-attention?</h4><p>注意力机制可以描述为将query和一系列的key-value对映射到某个输出的过程，而这个输出向量就是根据query和key计算得到的权重作用于value上的权重和。<br><img src="/pictures/NLP/Transformer/img2.png" alt="自注意力机制"><br>自注意力机制的核心过程就是通过Q和K计算得到注意力权重；然后再作用于V得到整个权重和的输出。计算公式如下：<br><img src="/pictures/NLP/Transformer/img3.png" alt="自注意力机制计算公式"><br>之所以要对QK进行缩放，是因为对于较大的dk来说，完成QK计算后会得到很大的值。而这将导致在经过 <strong>softmax</strong> 操作后产生非常小的梯度，不利于网络的训练。</p>
<h4 id="Q-K-V怎么来的？"><a href="#Q-K-V怎么来的？" class="headerlink" title="Q,K,V怎么来的？"></a>Q,K,V怎么来的？</h4><p>假设输入序列“我是谁”，通过embedding映射得到3x4的矩阵进行句子表示。<br>Q、K和V其实就是输入X分别乘以3个不同的矩阵计算而来（但这仅仅局限于Encoder和Decoder在各自输入部分利用自注意力机制进行编码的过程，Encoder和Decoder交互部分的Q、K和V另有指代）。此处对于计算得到的Q、K、V，你可以理解为这是 <em><strong>对于同一个输入进行3次不同的线性变换来表示其不同的3种状态</strong></em>。<br><img src="/pictures/NLP/Transformer/img4.jpg" alt="QKV是怎么来的"><br>在计算得到Q、K、V之后，就可以进一步计算得到权重向量。<br><img src="/pictures/NLP/Transformer/img5.jpg" alt="注意力权重计算图"><br>从上图可以知道，通过权重矩阵模型就可以知道当前位置的向量，应该以何种方式（权重）将注意力集中到不同的位置上。 <em><strong>模型在对当前位置的信息进行编码时，会过度将注意力集中于自身位置</strong></em>，而这会导致其忽略其他位置信息。因此，作者使用的解决方案就是 <strong>多头注意力机制</strong>。<br>计算得到权重矩阵后，便可以将其作用于V，进而得到最终的编码输出。<br><img src="/pictures/NLP/Transformer/img6.jpg" alt="编码输出计算"><br>对于最终的输出的编码向量，每个位置的编码向量其实就是 <strong>所有向量的加权和</strong>。这也就体现了自注意力机制在编码过程中的权重分配过程。</p>
<blockquote>
<p>有了自注意力机制后，仅需要对于原始输入进行几次矩阵变换就能够得到最终包含不同注意力信息的编码向量。解决了传统序列逆袭那个在编码过程中需要无法并行的弊端。</p>
</blockquote>
<h4 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head-Attention"></a>Multi-Head-Attention</h4><p>自注意力机制的缺陷在于：<em><strong>模型对于当前位置的信息进行整合编码时候，会过多的将注意力集中于自身位置</strong></em>，影响模型的表征能力。<br>多头注意力机制可以解决上述问题（对于周围信息获取受限的问题），并且还能够给予注意力层的输出包含有不同子空间的编码表示信息，增强模型的表达能力。<br>多头注意力机制就是将原始的输入序列进行多组的自注意力处理；然后将每组自注意力机制的结果 <strong>拼接</strong>起来进行一次线性变换得到最终的输出结果。<br><img src="/pictures/NLP/Transformer/img7.png" alt="多头注意力机制网络结构"><br>其计算公式为：<br><img src="/pictures/NLP/Transformer/img8.png" alt="多头注意力机制计算公式"><br>论文中，作者使用 <strong>8</strong> 个并行的自注意力模块来构建一个自注意力层，并且限定每个模块的维度为 <strong>64</strong>。论文中使用的多头注意力机制其实就是将一个大的高维单头拆分成h个多头。<br><img src="/pictures/NLP/Transformer/img9.jpg" alt="多头注意力计算"><br>根据输入序列X和W1可以得到Q1,K1,V1，进一步根据自注意力计算公式得到输出Z1；同理，可以得到另一个自注意力模块得到输出Z2；最后，将Z1,Z2水平堆叠形成Z，乘以W便可以得到最终的多头注意力层的输出。</p>
<h3 id="位置编码与编码解码过程"><a href="#位置编码与编码解码过程" class="headerlink" title="位置编码与编码解码过程"></a>位置编码与编码解码过程</h3><h4 id="Embedding机制"><a href="#Embedding机制" class="headerlink" title="Embedding机制"></a>Embedding机制</h4><h5 id="Token-Embedding"><a href="#Token-Embedding" class="headerlink" title="Token Embedding"></a>Token Embedding</h5><p>在Transformer模型中，首先要将文本通过Embedding层映射到低维稠密的向量空间，得到向量化表示，即Token Embedding。</p>
<blockquote>
<p>如果是换做之前的网络模型，例如CNN或者RNN，那么对于文本向量化的步骤就到此结束了，因为这些网络结构本身已经具备了捕捉时序特征的能力，不管是CNN中的n-gram形式还是RNN中的时序形式。</p>
</blockquote>
<p>自注意力机制在实际运算过程中，不过是几个矩阵来回相乘进行线性变换，这就导致即使打乱词序，最终计算得到的结果本质上没有任何变化。 <strong>自注意力机制会丢失文本原有的序列信息！</strong></p>
<blockquote>
<p>举个例子：在经过词嵌入表示后，序列“武松 打 虎”和“虎 打 武松”经过相同的权重矩阵后，输出结果并没有任何区别，只是交换了对应的位置。</p>
</blockquote>
<p>为了解决自注意力机制丢失序列信息问题，引入了positional Embedding来刻画数据在时序上的特征。</p>
<h5 id="Positional-Embedding"><a href="#Positional-Embedding" class="headerlink" title="Positional Embedding"></a>Positional Embedding</h5><p>作者采用如下规则生成各个维度的位置信息：<br><img src="/pictures/NLP/Transformer/img10.png" alt="位置信息生成公式"></p>
<p>在交换位置前与交换位置后，与同一个权重矩阵进行线性变换后的结果截然不同。因此，这就证明通过Positional Embedding可以弥补自注意力机制不能捕捉序列时序信息的缺陷。</p>
<h4 id="Transformer网络结构"><a href="#Transformer网络结构" class="headerlink" title="Transformer网络结构"></a>Transformer网络结构</h4><p>一个单层Transformer网络结构图。<br><img src="/pictures/NLP/Transformer/img11.png" alt="Transformer网络结构"></p>
<h5 id="Encoder层"><a href="#Encoder层" class="headerlink" title="Encoder层"></a>Encoder层</h5><p>对于Encoder部分来说其内部主要由两部分网络所构成(6层堆叠)：<em><strong>多头注意力机制</strong></em> 和 <em><strong>两层前馈神经网络</strong></em>。<br>同时，对于这两部分网络来说，都加入了残差连接，并且在残差连接后还进行了层归一化操作。对于每个部分来说其输出均为LayNorm(x + Sub-Layer(x))，并且在都加入了Dropout操作。<br>进一步，为了便于在这些地方使用残差连接，这两部分网络输出向量的维度均为 <strong>512</strong>。<br>对于第2部分的两层全连接网络来说，其具体计算过程为<br><img src="/pictures/NLP/Transformer/img12.png" alt="FFN层计算"><br>其中输入的维度为 <strong>512</strong>，第1层全连接层的输出维度为 <strong>2048</strong>，第2层全连接层的输出为 <strong>512</strong>，且同时 <em><strong>仅对于第1层网络的输出</strong></em> 还运用了Relu激活函数。</p>
<h5 id="Decoder层"><a href="#Decoder层" class="headerlink" title="Decoder层"></a>Decoder层</h5><p>对于Decoder部分来说，其整体上与Encoder类似（6层堆叠），只是多了一个用于与Encoder输出进行交互的多头注意力机制。<br>不同于Encoder部分，在Decoder中一共包含有3个部分的网络结构。最上面的和最下面的部分（暂时忽略Mask）与Encoder相同，只是多了中间这个与Encoder输出（Memory）进行交互的部分，作者称之为“Encoder-Decoder attention”。<br>对于这部分的输入，<strong>Q来自于下面多头注意力机制的输出，K和V均是Encoder部分的输出（Memory）经过线性变换后得到</strong>。而作者之所以这样设计也是在模仿传统Encoder-Decoder网络模型的解码过程。<br>传统的基于Encoder-Decoder的Seq2Seq翻译模型的解码过程：<br><img src="/pictures/NLP/Transformer/img13.jpg" alt="传统的Seq2Seq翻译模型"><br>左半部分是编码器，右下部分为解码器，右上部分为注意力机制部分。 ~h[i]表示<strong>编码过程</strong>中，各个时刻的隐含状态，称之为每个时刻的Memory；h[t]表示解码当前时刻的隐含状态。此时，注意力机制的思想在于，<strong>希望模型能够在解码时，参考编码阶段每个时刻的记忆</strong>。</p>
<blockquote>
<ol>
<li>解码第一个时刻”s”时，h[t]会首先同每一个记忆状态~h[i]进行相似度计算，得到注意力权重；</li>
<li>然后，通过对隐含状态的加权求和，得到context vector内容</li>
</ol>
</blockquote>
<p>以上是传统的解码交互方案。在Transformer中，K和V均是编码部分的输出Memory经过线性变换后的结果（此时Memory中包含了原始输入序列每个位置的编码信息），而Q是解码部分多头注意力机制输出的隐含向量经过线性变换后的结果。</p>
<blockquote>
<ol>
<li>首先，通过Q和K交互计算得到注意力权重矩阵；</li>
<li>然后，通过注意力权重与V进行计算，得到权重解码向量。此向量考虑了memory中各个位置编码信息的输出向量。</li>
<li>最后，得到解码向量后，经过两层全连接层后，将其输入到分类层进行分类得到当前时刻的解码输出值。</li>
</ol>
</blockquote>
<h5 id="QKV的来源"><a href="#QKV的来源" class="headerlink" title="QKV的来源"></a>QKV的来源</h5><p>根据Transformer结构图可知，在整个Transformer中涉及到自注意力机制的一共有3个部分：</p>
<blockquote>
<ul>
<li>Encoder中的Multi-Head Attention；</li>
<li>Decoder中的Masked Multi-Head Attention；</li>
<li>Encoder和Decoder交互部分的Multi-Head Attention。</li>
</ul>
</blockquote>
<ol>
<li>对于Encoder中的Multi-Head Attention来说，其原始q、k、v均是Encoder的Token输入经过Embedding后的结果。q、k、v分别经过一次线性变换（各自乘以一个权重矩阵）后得到了Q、K、V，然后再进行自注意力运算得到Encoder部分的输出结果Memory。</li>
<li>对于Decoder中的Masked Multi-Head Attention来说，其原始q、k、v均是Decoder的Token输入经过Embedding后的结果。q、k、v分别经过一次线性变换后得到了Q、K、V，然后再进行自注意力运算得到Masked Multi-Head Attention部分的输出结果，即待解码向量。</li>
<li>对于Encoder和Decoder交互部分的Multi-Head Attention，其原始q、k、v分别是上面的带解码向量、Memory和Memory。q、k、v分别经过一次线性变换后得到了Q、K、V，然后再进行自注意力运算得到Decoder部分的输出结果。之所以这样设计也是在模仿传统Encoder-Decoder网络模型的解码过程。</li>
</ol>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>Word2Vec</title>
    <url>/2022/09/25/NLP/Word2Vec/</url>
    <content><![CDATA[<h3 id="什么是Word2Vec"><a href="#什么是Word2Vec" class="headerlink" title="什么是Word2Vec?"></a>什么是Word2Vec?</h3><p>Word2Vec模型实际上分了两个部分，第一部分建立模型，第二部分通过模型获取嵌入词向量。<br>Word2Vec的整个建模过程实际上与自编码器的思想很相似。</p>
<blockquote>
<p>先基于训练数据构建神经网络。当模型训练好以后，我们并不会使用这个训练好的模型处理新的任务，我们需要的是通过训练数据学习得到的参数，例如隐层的权重矩阵。</p>
</blockquote>
<p>Word2Vec的训练模型本质上是只具有一个隐含层的神经元网络，从大量文本语料中以无监督的方式学习语义知识。<br><img src="/pictures/NLP/Word2Vec/img1.jpg.png" alt="Word2Vec单层网络结构"></p>
<blockquote>
<ul>
<li>输入是One-Hot向量，Hidden Layer的激活函数是线性。Output Layer维度和Input Layer维度相同，用的是Softmax回归；</li>
<li>训练Word2Vec需要用到反向传播算法，本质是链式求导；</li>
<li>我们并不关心模型训练任务，我们真正需要的是这个模型通过学习得到的参数，即隐层的权重矩阵；</li>
<li>Word2Vec本质是一种降维操作。</li>
</ul>
</blockquote>
<p>Word2Vec其实就是通过学习文本来用词向量的方式表征词的语义信息，即通过一个嵌入空间是的语义相似的单词在该空间内距离很近。<br><strong>Embedding</strong> 其实就是一个映射，将单词从原先所属的空间映射到新的多维空间中。通过对词汇表中单词进行这种数值表示方式的学习，能够进行 <em><strong>向量化</strong></em> 的操作。  </p>
<h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>Word2Vec模型中，主要有两种结构：</p>
<blockquote>
<ul>
<li><strong>CBOW模型</strong> ：训练输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量；  </li>
<li><strong>Skip-gram模型</strong> ：输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量；</li>
</ul>
</blockquote>
<p><img src="/pictures/NLP/Word2Vec/img2.png" alt="Word2Vec网络结构"></p>
<h4 id="Skip-gram"><a href="#Skip-gram" class="headerlink" title="Skip-gram"></a>Skip-gram</h4><p>Skip-gram模型，通过中间词预测上下文。</p>
<ul>
<li>首先，选择句子中的一个词作为中心词；</li>
<li>定义skip_window参数，限制从中心词左右可以选词的范围；</li>
<li>神经网络基于这些训练数据将会输出一个概率分布，这个概率代表词典中每个词是上下文的可能性。</li>
</ul>
<p>训练样本的构成是通过选择输入词前后skip_window范围内的词语与输入词进行组合。下图中，蓝色代表input word，方框内代表位于窗口内的单词。<br><img src="/pictures/NLP/Word2Vec/img5.png" alt="训练样本构建"><br>模型将会从每对单词出现的次数中学习得到统计规律。</p>
<p>以下是Skip-gram模型结构：<br><img src="/pictures/NLP/Word2Vec/img3.png" alt="Skip-gram网络结构"><br>隐层没有使用任何激活函数，但是输出层使用了softmax。<br>我们基于成对的单词来对神经网络进行训练，训练样本是上述单词对，其中input word和output word都是onehot向量。最终模型输出是一个概率分布。</p>
<blockquote>
<p>可以看成y &#x3D; f(x)模型的并联，cost function是单个cost function的累加 <strong>（取log之后）</strong>。</p>
</blockquote>
<h4 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h4><p><img src="/pictures/NLP/Word2Vec/img4.jpg" alt="CBOW网络结构"><br>注意到，跟Skip-gram模型的并联不同，CBOW输入要对多个单词进行输入处理，一般是求和然后平均，输出的cost function不变。</p>
<blockquote>
<ol>
<li>输入层：上下文单词的one-hot向量表示；</li>
<li>所有one-hot向量分别乘以共享的输入权重矩阵W；</li>
<li>所得的向量 <strong>相加求平均</strong> 作为隐层向量；</li>
<li>乘以输出矩阵W’；</li>
<li>得到向量，经过softmax函数处理得到V-dim概率分布；</li>
<li>概率最大的index所指示的单词作为预测词与true label的one-hot做比较，误差越小越好（根据误差更新权重矩阵）。</li>
</ol>
</blockquote>
<h3 id="训练Tricks"><a href="#训练Tricks" class="headerlink" title="训练Tricks"></a>训练Tricks</h3><p>Word2Vec本质上是一个语言模型，它的输出节点数是V个，对应了V个词语，本质上是一个多分类问题。但实际当中，词表数量巨大，计算复杂度巨高，所以需要技巧来加速训练。</p>
<blockquote>
<ul>
<li>层级softmax：本质是把N分类问题变成log(N)次的二分类；</li>
<li>负采样：本质是预测总体类别的一个子集</li>
</ul>
</blockquote>
<h4 id="负采样"><a href="#负采样" class="headerlink" title="负采样"></a>负采样</h4><p>在训练神经网络时，每个训练样本都将会调整所有神经网络中参数。词汇表决定了Word2Vec模型将会有非常大的权重矩阵，并且所有权重参数会随着数十亿训练昂呢不断调整。<br>负采样每次让一个训练样本更新一小部分的权重参数，从而降低梯度下降过程中的计算成本。  </p>
<p>负样本的选择规则：一个单词被选作负采样的概率与它出现的频次有关，出现频次越高的单词越容易被选择作为负样本，经验公式如下：<br><img src="/pictures/NLP/Word2Vec/img6.png.webp" alt="负采样概率"><br>f(w)代表每个单词被赋予的一个权重，即出现的词频。</p>
<h4 id="层序Softmax"><a href="#层序Softmax" class="headerlink" title="层序Softmax"></a>层序Softmax</h4><p>Huffman原理：权重越大的节点，越靠近根节点。</p>
<blockquote>
<ol>
<li>对每个词按照权重进行排序，将每次词看成一个独立的单节点的树；</li>
<li>合并最小的两个子树，新的根节点权重为两者根节点权重之和；</li>
<li>将新的树插入排序进树集合中；</li>
<li>重复2，3步骤，直到合并所有树。</li>
</ol>
</blockquote>
<h3 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h3><blockquote>
<p>cbow是用周围词预测中心词，训练过程中其实是在从output的loss学习周围词的信息也就是embedding，但是在中间层是average的，一共预测V次；<br>skip-gram是用中心词预测周围词，对每一个中心词都有K个词作为output，对一个词的预测有K次，所以能够更有效的从context中学习信息，共预测K*V次，因此，skip-gram的训练时间更长。</p>
</blockquote>
<p>鉴于skip-gram学习的词向量更细致，当 <strong>数据量较少或者语料库中有大量低频词</strong> 时，使用skip-gram学习比较合适。</p>
<blockquote>
<p>CBOW中的目标函数是使条件概率P(w|context(w))最大化<br>Skip-gram中的目标函数是使条件概率P(context(w)|w)最大化</p>
</blockquote>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>DIN</title>
    <url>/2022/09/13/RecomSys/DIN/</url>
    <content><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>针对电子商务领域的CTR预估，重点在与充分利用&#x2F;挖掘用户历史行为数据中的信息。</p>
<blockquote>
<p>按照传统方式，模型在预估针对用户推荐的广告时，对于 <em><strong>所有用户的特征选取总是使用固定长度</strong></em>。这样带来的问题就是，<em><strong>推荐系统并不能准确的把握用户的兴趣所在</strong></em>。</p>
</blockquote>
<h4 id="Attention机制引入"><a href="#Attention机制引入" class="headerlink" title="Attention机制引入"></a>Attention机制引入</h4><p>并不是所有的用户历史行为数据，对每一次的点击有贡献，而 <em><strong>仅仅有一部分在起作用</strong></em>。这个性质有些像attention，对于当前状态的预估，需要告知模型，哪些点与当前的预估最相关；</p>
<p>在对用户历史行为数据进行处理时，每个用户的历史点击个数是不相等的，我们需要把它们编码成一个固定长的向量。以往的做法是，对每次历史点击做相同的embedding操作之后，将它们做一个 <em><strong>求和或者求最大值</strong></em> 的操作，类似经过了一个pooling层操作，简单粗暴，但是容易丢失很多信息。</p>
<h4 id="模型改进"><a href="#模型改进" class="headerlink" title="模型改进"></a>模型改进</h4><blockquote>
<ul>
<li>使用 <strong>用户兴趣分布</strong> 来表示用户多种多样的兴趣爱好；</li>
<li>使用 <strong>attention机制</strong> 来实现Local Activation；</li>
<li>针对模型训练，提出了 <strong>Dice激活函数，自适应正则</strong>，显著提升了模型性能与收敛速度。</li>
</ul>
</blockquote>
<h4 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h4><h5 id="Diversity-多样性"><a href="#Diversity-多样性" class="headerlink" title="Diversity 多样性"></a>Diversity 多样性</h5><p>用户在访问电商网站时会对多种商品感兴趣，也就是用户的兴趣非常广泛。<br>针对用户广泛的兴趣，DIN用 an interest distribution 去表示。</p>
<h5 id="Local-Activation-局部激活"><a href="#Local-Activation-局部激活" class="headerlink" title="Local Activation 局部激活"></a>Local Activation 局部激活</h5><p>用户是否会点击推荐给他的商品 ，仅仅取决与历史行为数据的一小部分，而不是全部。</p>
<p>DIN借鉴机器翻译中的Attention机制，设计了一种 <strong>attention-like network structure</strong>， 针对当前候选Ad，去局部的激活(Local Activate)相关的历史兴趣信息。<strong>和当前候选Ad相关性越高的历史行为，会获得更高的attention score，从而会主导这一次预测</strong>。</p>
<h4 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h4><p>DIN方法也可以应用于其他有丰富用户行为数据的场景，比如：</p>
<blockquote>
<ul>
<li>电子商务中的个性化推荐；</li>
<li>社交网络中的信息推流排序(feeds ranking)</li>
</ul>
</blockquote>
<h3 id="系统构建"><a href="#系统构建" class="headerlink" title="系统构建"></a>系统构建</h3><p>阿里推荐系统工作流程：</p>
<blockquote>
<ol>
<li>检查用户历史行为数据；</li>
<li>使用 matching module 产生 候选ads；</li>
<li>通过 ranking module 得到 候选ads 的点击概率，并根据概率排序得到推荐列表；</li>
<li>记录下用户对当前展示广告的反应（点击与否）</li>
</ol>
</blockquote>
<p>这是一个闭环系统，对于用户行为数据（User Behavior Data），系统自己生产并自己消费。</p>
<h3 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h3><p>数据有以下特点：</p>
<blockquote>
<ul>
<li>Diversity – 兴趣爱好非常广泛；</li>
<li>Local Activation – 历史行为中部分数据主导是否会点击候选广告；</li>
<li>高纬度；</li>
<li>非常稀疏；</li>
<li>特征往往都是 multi-hot 的稀疏ids。</li>
</ul>
</blockquote>
<p><img src="/pictures/RecomSys/DIN/img1.png" alt="特征数据"></p>
<h4 id="特征处理"><a href="#特征处理" class="headerlink" title="特征处理"></a>特征处理</h4><p>论文中作者把特征分为四大类，并 <strong>没有进行特征组合&#x2F;交叉特征</strong>。而是 <strong>通过 DNN 去学习特征间的交互信息</strong>。</p>
<blockquote>
<ul>
<li>User Profile Features</li>
<li>User Behavior Features</li>
<li>Ad Features</li>
<li>Context Features</li>
</ul>
</blockquote>
<p>为了得到一个 <strong>固定长度</strong> 的 Embedding Vector 表示，原来的做法是在 Embedding Layer 后面 <strong>增加一个 Pooling Layer</strong>。Pooling可以用 sum 或 average。最终得到一个固定长度的 Embedding Vector，是用户兴趣的一个抽象表示，常被称作 User Representation。缺点是会损失一些信息。<br><img src="/pictures/RecomSys/DIN/img2.png" alt="传统模型"></p>
<p>DIN使用 Attention机制 来解决这个问题。Attention机制 来源于 Neural Machine Translation(NMT)。DIN使用 Attention机制 去更好的建模 局部激活。在DIN场景中，针对不同的候选广告需要自适应地调整 User Representation。也就是说：在 Embedding Layer -&gt; Pooling Layer 得到用户兴趣表示的时候，赋予不同的历史行为不同的权重，实现局部激活。从最终反向训练的角度来看，就是根据当前的候选广告，来反向的激活用户历史的兴趣爱好，赋予不同历史行为不同的权重。<br><img src="/pictures/RecomSys/DIN/img3.png" alt="DIN模型结构"></p>
<h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><h4 id="评价指标-GAUC"><a href="#评价指标-GAUC" class="headerlink" title="评价指标 GAUC"></a>评价指标 GAUC</h4><p>不同于以往CTR模型采用AUC作为评价指标，论文采用的评价指标是自己设计的 GAUC 评价指标。</p>
<p><em><strong>AUC的含义是正样本得分比负样本得分高的概率</strong></em>。在CTR的实际应用场景中，CTR预测常被应用于对每个用户的候选广告进行排序，也即最终想得到的效果是 <em><strong>每个用户的AUC达到最高</strong></em>。同时，<strong>不同用户的AUC之间也确实存在差别，有的用户天生点击率就高，有的用户却不怎么喜欢点击广告</strong>。</p>
<p>以往的评价指标是对样本不区分用户地进行AUC计算。论文采用的GAUC计算了 <strong>用户级别的AUC</strong>，在单个用户AUC的基础上，按照 <strong>点击次数或展示次数进行加权平均</strong>，消除了用户偏差对模型的影响，更准确地描述了模型对于每个用户的表现效果。<br><img src="/pictures/RecomSys/DIN/img4.png" alt="GAUC计算公式"><br>w 可以是 <strong>clicks（点击次数） 或者 impressions（展示次数）</strong>，n 是用户数量。这中AUC也应该是在 <strong>个性化推荐</strong> 里面更适合的，用户每个个体都有自己的AUC。</p>
<h4 id="激活函数-Dice"><a href="#激活函数-Dice" class="headerlink" title="激活函数 Dice"></a>激活函数 Dice</h4><p>Dice其实是ReLU的改良版，ReLU可以看作是 x * Max(x, 0)，相当于输出 x  经过了一个在0点的阶跃整流器。由于ReLU在 x&lt;0 的时候，梯度为0，可能导致网络停止更新，PReLU对整流器的左半部分形式进行了修改，使得 x&lt;0 时输出不为0。<br><img src="/pictures/RecomSys/DIN/img5.png" alt="激活函数"></p>
<p>论文里认为，对于所有输入不应该都选择0点为整流点。于是提出了一种data dependent的方法，并称该激活函数为Dice函数。<br><img src="/pictures/RecomSys/DIN/img6.png" alt="Dice激活函数"><br>概率值 p[i] 决定输出是取 y[i] 或者是 a[i] * y[i]，p[i] 也起到了整流器的作用。<br>获取 p[i] 的两步操作：</p>
<blockquote>
<ol>
<li>对 x 进行均值归一化处理。使得整流点是在数据的均值处，实现data dependent的想法；</li>
<li>经过一个 sigmoid函数的计算，得到一个0到1的概率值。</li>
</ol>
</blockquote>
<h4 id="自适应正则"><a href="#自适应正则" class="headerlink" title="自适应正则"></a>自适应正则</h4><p>在CTR预估任务中，用户行为数据具有长尾分布的特点，也即数据非常的稀疏。</p>
<p>稀疏输入，为什么会overfitting呢？这个跟数据分布有关系，互联网时代的数据特点，<em><strong>超长尾头部重，头重（小比例的特征频繁出现）容易过拟合，长尾（大比例的特征低频出现）则容易带来噪声</strong></em>，不好学。当增加细粒度的特征时，也极其容易由于细粒度的样本过于密集而带来负面效果。</p>
<p>为了防止模型过拟合，论文设计了一个针对 <strong>feature id出现的频率</strong> 进行自适应的正则方法。</p>
<blockquote>
<ul>
<li>针对feature id出现的频率，来自适应的调整他们正则化的强度；</li>
<li>对于出现频率高的，给与较小的正则化强度；</li>
<li>对于出现频率低的，给予较大的正则化强度。</li>
</ul>
</blockquote>
<h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>传统深度模型<br><img src="/pictures/RecomSys/DIN/img7.png" alt="传统深度模型"></p>
<p>DIN模型在对用户的表示计算上引入了attention network (也即图中的 Activation Unit ) 。<br><img src="/pictures/RecomSys/DIN/img8.png" alt="DIN模型结构"></p>
<p>DIN把用户特征、用户历史行为特征进行embedding操作，视为对用户兴趣的表示，之后通过attention network，<em><strong>对每个兴趣表示赋予不同的权值</strong></em>。这个权值是由用户的兴趣和待估算的广告进行匹配计算得到的，如此模型结构符合了之前的两个观察——用户兴趣的多样性以及部分对应。attention network 的计算公式如下， V_u 代表用户表示向量， V_i 代表用户兴趣表示向量， V_a 代表广告表示向量。<br><img src="/pictures/RecomSys/DIN/img9.png" alt="attention机制"></p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>Recom</tag>
      </tags>
  </entry>
  <entry>
    <title>DeepFM</title>
    <url>/2022/09/12/RecomSys/DeepFM/</url>
    <content><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><h4 id="特征组合的挑战"><a href="#特征组合的挑战" class="headerlink" title="特征组合的挑战"></a>特征组合的挑战</h4><p>对于基于CTR预估的推荐系统，最重要的是学习到用户点击行为背后隐含的特征组合。在不同的推荐场景中，低阶组合特征或者高阶组合特征可能会对最终的CTR产生影响。</p>
<p>因子分解机通过对于每一维特征的隐变量的内积来提取特征组合。理论上FM可以对高阶特征组合进行建模，但是实际上因为计算复杂度的原因，一般只用到了二阶特征组合。对于高阶特征组合，使用多层神经网络DNN解决。</p>
<h4 id="DNN的局限性"><a href="#DNN的局限性" class="headerlink" title="DNN的局限性"></a>DNN的局限性</h4><p>对于离散特征的处理，使用one-hot编码。但是将one-hot编码直接输入到DNN中，会导致网络参数过多。<br><img src="/pictures/RecomSys/DeepFM/img1.png" alt="one-hot编码不可以直接输入DNN"></p>
<p>采用类似于FFM中的思想，将特征分为不同的field。<br><img src="/pictures/RecomSys/DeepFM/img2.png" alt="Embedding生成"></p>
<p>再加两层全连接层，便可以组合出高阶特征。<br><img src="/pictures/RecomSys/DeepFM/img3.png" alt="高阶特征组合"></p>
<p>但是低阶和高阶特征组合隐含地体现在隐藏层中，如果我们希望把低阶特征组合单独建模，然后融合高阶特征组合。<br><img src="/pictures/RecomSys/DeepFM/img4.png" alt="并行结构 DeepFM"></p>
<p><img src="/pictures/RecomSys/DeepFM/img5.png" alt="串行结构 FNN"></p>
<p>目前的CTR预估模型，实质上都是在“利用模型”进行特征工程上狠下功夫。传统的LR，简单易解释，但特征之间信息的挖掘需要大量的人工特征工程来完成。由于深度学习的出现，利用神经网络本身对于隐含特征关系的挖掘能力，成为了一个可行的方式。<em><strong>DNN本身主要是针对于高阶的隐含特征</strong></em>，而像FNN（利用FM做预训练实现embedding，再通过DNN进行训练，有时间会写写对该模型的认识）这样的模型则是考虑了高阶特征，而在最后sigmoid输出时 <em><strong>忽略了低阶特征本身</strong></em>。</p>
<p>鉴于上述理论，目前新出的很多基于深度学习的CTR模型都从wide、deep（即低阶、高阶）两方面同时进行考虑，进一步提高模型的泛化能力，比如DeepFM。</p>
<h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>DeepFM包含两个部分：神经网络部分与因子分解机部分，分别负责低阶特征的提取和高阶特征的提取。这两个部分 <em><strong>共享同样的输入</strong></em>。<br><img src="/pictures/RecomSys/DeepFM/img6.png" alt="DeepFM网络结构"></p>
<h4 id="FM部分"><a href="#FM部分" class="headerlink" title="FM部分"></a>FM部分</h4><p><img src="/pictures/RecomSys/DeepFM/img7.png" alt="FM模块结构"><br>传统度量特征 i 和 j 权重的方法，<strong>需要两者同时存在于同一条数据记录中</strong>。<br>FM部分是一个因子分解机。因为FM中引入 <strong>隐变量</strong> 的原因，对于几乎不出现或者很少出现的隐变量，FM也可以很好的学习。</p>
<p>FM通过两个特征的隐向量的内乘积进行表示。不需要同时存在于同一条记录中。<br><img src="/pictures/RecomSys/DeepFM/img8.png" alt="因子分解"></p>
<p>FM的输出为：<br><img src="/pictures/RecomSys/DeepFM/img9.png" alt="FM模型公式"></p>
<h4 id="Deep部分"><a href="#Deep部分" class="headerlink" title="Deep部分"></a>Deep部分</h4><p><img src="/pictures/RecomSys/DeepFM/img10.png" alt="DNN模块结构"><br>Deep部分是一个前馈神经网络。与图像或者语音这类输入不同，图像语音的输入一般是连续并且密集的，然而用于CTR的输入一般是及其稀疏的。因此，在第一层隐藏层之前，<strong>引入一个嵌入层来完成将输入向量压缩到低维稠密向量</strong>。</p>
<h4 id="Embedding层"><a href="#Embedding层" class="headerlink" title="Embedding层"></a>Embedding层</h4><p><img src="/pictures/RecomSys/DeepFM/img11.png" alt="Embedding层网络结构"><br>嵌入层(embedding layer)的结构如上图所示。当前网络结构有两个有趣的特性<br>尽管不同field的输入长度不同，但是embedding之后向量的长度均为K；<br>在FM里得到的隐变量 V_ik 现在作为了嵌入层网络的权重。</p>
<p>这里的第二点如何理解呢，假设我们的 k&#x3D;5，首先，对于输入的一条记录，<em><strong>同一个field 只有一个位置是1</strong></em>，那么在由输入得到dense vector的过程中，输入层只有一个神经元起作用，得到的dense vector其实就是 <em><strong>输入层到embedding层该神经元相连的五条线的权重</strong></em>，即v_i1，v_i2，v_i3，v_i4，v_i5。这五个值组合起来就是我们在FM中所提到的V_i。</p>
<p>在FM部分和DNN部分，这一块是 <em><strong>共享权重</strong></em> 的，对同一个特征来说，得到的V_i是相同的。</p>
<h4 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h4><p>DeepFM的预测结果为：<br><img src="/pictures/RecomSys/DeepFM/img12.png" alt="输出层计算"></p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>Recom</tag>
      </tags>
  </entry>
  <entry>
    <title>MMoE</title>
    <url>/2022/09/13/RecomSys/MMoE/</url>
    <content><![CDATA[<p>在工业界基于神经网络的多任务学习在推荐等场景业务应用广泛，比如在推进啊系统中对用户推荐物品时，不仅要推荐用户感兴趣的物品，还要尽可能地促进转化和购买，因此要对用户评分和购买两种目标同时建模。</p>
<h3 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h3><p>把多个任务放在一起学习，任务共享同一个模型空间，它们 <strong>共享同一个表示层</strong>。在训练过程中，多个任务会对这个共享模型进行参数更新。</p>
<h4 id="相关任务"><a href="#相关任务" class="headerlink" title="相关任务"></a>相关任务</h4><p>multi task同时学习多个相关任务，并且具有相当的优势。同时，我们在做多任务学习时，有时关注的点在某个 <strong>主要任务</strong> 上，其他的共同学习的任务可能更多的只是起到帮助作用，这些起到帮助作用的任务叫做 <strong>辅助任务</strong>。</p>
<p><strong>辅助任务与主任务越相关，那么起到的效果可能会越好</strong>。<br>如下图所示，假如有这样两个相似的任务：狗的分类模型和猫的分类模型。在单任务学习中，他们都拥有比较接近的底层特征，比如皮毛颜色啦、眼睛颜色啦、耳朵形状啦等等。<br><img src="/pictures/RecomSys/MMoE/img1.png" alt="学习任务相近的单任务学习"></p>
<p>由于 <strong>多任务学习本质上是共享表示层</strong>，任务之间互相影响。那么在多任务学习中，他们就可以很好地进行底层特征共享。<br><img src="/pictures/RecomSys/MMoE/img2.png" alt="相关性较高的多任务学习"></p>
<p>但是对于不相似的任务来说，如下图，汽车的识别和狗的识别，他们的 <strong>底层表示差异很大</strong>，共享表示层可能就没那么有效果了。进行参数共享时很有可能会互相冲突或噪声太多，对多任务学习而言非常不友好。<br><img src="/pictures/RecomSys/MMoE/img3.png" alt="相关性较低的多任务学习"></p>
<blockquote>
<p>由于multi task在不相关的任务上表现不佳，同时，在实际应用中，你很难判断任务在数据层面是否是相似的。<br>所以多任务学习如何在相关性不高的任务上获得好效果是一件很有挑战性也很有实际意义的事。</p>
</blockquote>
<h4 id="共享表示"><a href="#共享表示" class="headerlink" title="共享表示"></a>共享表示</h4><p>神经网络中，Multi Task Learning的共享表示有两种方式：<strong>hard参数共享和soft参数共享</strong>。</p>
<h5 id="Hard参数共享"><a href="#Hard参数共享" class="headerlink" title="Hard参数共享"></a>Hard参数共享</h5><p>在所有任务之间 <strong>共享隐藏层</strong>，同时保留几个特定任务的输出层。这种方式很大程度上 <strong>降低了过拟合的风险</strong>。因为同时学习的工作越多，模型找到一个含有所有任务的表征就越困难，而过拟合某特定原始任务的可能性就越小。<br><img src="/pictures/RecomSys/MMoE/img4.png" alt="Hard参数共享"></p>
<h5 id="Soft参数共享"><a href="#Soft参数共享" class="headerlink" title="Soft参数共享"></a>Soft参数共享</h5><p>每个任务有自己的参数和模型，最后 <strong>通过对不同任务的参数之间的差异加约束</strong>，表达相似性。比如可以使用L2进行正则, 迹范数（trace norm）等。<br><img src="/pictures/RecomSys/MMoE/img5.png" alt="Soft参数共享"></p>
<h4 id="多任务学习优势"><a href="#多任务学习优势" class="headerlink" title="多任务学习优势"></a>多任务学习优势</h4><ol>
<li>多个任务一起学习时，<strong>有相关部分也有不那么相关的地方</strong>，在学习一个任务时，与它不相关的部分就相当于是加入一些噪声，而 <strong>加入噪声可以提升模型的泛化能力</strong>。</li>
<li>单任务学习时容易陷入局部最优，而多任务学习中 <strong>不同任务的局部最优解处于不同的位置</strong>，通过相互作用，可以逃离局部最优。</li>
<li>增加任务会影响网络参数的更新，比如增加额外的任务增加了隐层的有效的学习率，具体取决于每个任务输出的错误反馈权重。可能较大的学习速率提升了学习效果</li>
<li>某些特征可能在主任务不好学习（比如以很复杂的方式与特征进行交互，或被其他因素抑制），但在辅助任务上这个特征好学习到。可以通过辅助任务来学习这些特征，方法比如hints（预测重要特征）</li>
<li>通过学习足够大的假设空间，在未来某些新任务中可以有较好的表现（解决冷启动），前提是这些任务都是 <strong>同源</strong> 的。</li>
<li>多个任务在浅层共享表示，引入归纳偏置作为正则化项。因此，它降低了过拟合的风险以及模型的 Rademacher 复杂度（即适合随机噪声的能力）</li>
</ol>
<h3 id="MMoE模型结构"><a href="#MMoE模型结构" class="headerlink" title="MMoE模型结构"></a>MMoE模型结构</h3><p>关于共享隐层方面，MMoE和一般多任务学习模型的区别：</p>
<blockquote>
<p><strong>一般多任务学习模型</strong>：接近输入层的隐层作为一个整体被共享；<br><strong>MMoE</strong>：将共享的底层表示层分为 <strong>多个expert</strong>，同时设置了gate，使得 <strong>不同的任务可以多样化的使用共享层</strong>。</p>
</blockquote>
<p><img src="/pictures/RecomSys/MMoE/img6.png" alt="网络结构变化"></p>
<blockquote>
<p>a）是最原始的多任务学习模型，也就是base；<br>b）是加入单门（one gate）的MoE layer的多任务学习模型；<br>c）本质上是将base的shared bottom换成了MoE layer，并对每个任务都加gate</p>
</blockquote>
<h4 id="Mixture-of-Expert-Model"><a href="#Mixture-of-Expert-Model" class="headerlink" title="Mixture-of-Expert Model"></a>Mixture-of-Expert Model</h4><p>隐层是三个expert子网组成，各自的输出 f[i]（第 i 个expert的输出）会传入gate，也就是 g(x) 维度与expert个数相同的 <strong>softmax</strong>，g(x)[i] 是它输出的第 i 个logits。<strong>gate对expert的输出进行加权求和，得到不同任务的输入</strong>。<br><img src="/pictures/RecomSys/MMoE/img10.png" alt="MoE模型计算公式"></p>
<h4 id="Shared-Bootom-Model"><a href="#Shared-Bootom-Model" class="headerlink" title="Shared-Bootom Model"></a>Shared-Bootom Model</h4><p>模型 (a) 最为常见，两个任务直接共享模型的 bottom 部分，只在最后处理时做区分，图 (a) 中使用了 Tower A 和 Tower B，然后分别接损失函数。<br><img src="/pictures/RecomSys/MMoE/img7.png" alt="Base模型"><br>x 表示 input，f 表示 shared-bottom network， h[k] 表示第 k 个tower network，针对第k个任务。</p>
<p>这种网络非常简单，可以理解为在DNN上接了 k 个不同的tower 网络，不同的tower网络针对不同任务，有着各自的损失函数，但是 <strong>这些损失函数是放在一起进行联合训练</strong>。</p>
<p>直觉告诉我们，如此进行多任务学习，在某些情况下效果可能并不好，例如当多个任务间是矛盾的，或者完全不相关的。</p>
<h4 id="One-gate-MoE-Model"><a href="#One-gate-MoE-Model" class="headerlink" title="One-gate MoE Model"></a>One-gate MoE Model</h4><p>模型 (b) 是常见的多任务学习模型。将 input 分别输入给三个 Expert，但 <strong>三个Expert并不共享参数</strong>。同时将 input 输出给 Gate，<strong>Gate输出每个Expert被选择的概率</strong>，然后将三个Expert的输出 <strong>加权求和</strong>，输出给 Tower。有点 attention 的感觉。<br><img src="/pictures/RecomSys/MMoE/img8.png" alt="OMoE模型公式"><br>上式中，n 表示有 n 个 expert networks，f<a href="x">i</a> 表示第 i 个expert network，在论文expert network就是DNN网络。g(x)[i] 是由 输入x 控制的，其中 W[g] ∈ R[n × d]，n 表示expert network数量，d表示输入x 的特征维度，在n维度上进行softmax，因此 g(x)[i] 可理解为 <strong>通过输入x 得到在n个 exper network 上的权重分布</strong>。同样，h[k] 表示第k个 tower network。</p>
<p>这个网络也很简单，可以理解为 对多个不同 expert network进行不同权重的集成，在集成的结果上接不同的tower network 而已。模型在训练过程中，会学习到不同expert network重要程度。</p>
<p>那么何为 One-gate 呢？ 从上面的分析可以看出，不同的tower network的输入是相同的，都是经过同一套权重组合（同一个gate network）得到expert networks的输出。这么这样做合理吗？</p>
<h4 id="Multi-gate-MoE-Model"><a href="#Multi-gate-MoE-Model" class="headerlink" title="Multi-gate MoE Model"></a>Multi-gate MoE Model</h4><p>模型 (c) 是作者新提出的方法，对于不同的任务，模型的权重选择是不同的，所以作者为每个任务都配备一个 Gate 模型。<strong>对于不同的任务，特定的 Gate k 的输出表示不同的 Expert 被选择的概率</strong>，将多个 Expert 加权求和，得到 f<a href="x">k</a> ，并输出给特定的 Tower 模型，用于最终的输出。<br><img src="/pictures/RecomSys/MMoE/img9.png" alt="MMoE模型公式"><br>与OMoE区别仅在 对于不同的tower network，有着不同的gate network，在OMoE上，只会初始化一个W[g] 参数矩阵，而在MMoE上，会初始化 k 个 W[gk]，得到 k 个gate network（multi-gates&#x2F;task-specific gates)，参数增加了一些。</p>
<p>相对于OMoE，MMoE的做法更加合理一些，不同的任务有着不同的gate network，对expert networks输出有着不同权重组合。</p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>Recom</tag>
      </tags>
  </entry>
  <entry>
    <title>FM</title>
    <url>/2022/09/12/RecomSys/FM/</url>
    <content><![CDATA[<h3 id="FM"><a href="#FM" class="headerlink" title="FM"></a>FM</h3><h4 id="提出背景"><a href="#提出背景" class="headerlink" title="提出背景"></a>提出背景</h4><p>传统线性模型忽略了特征之间的交叉联系；特征高维稀疏，并且容易维度爆炸。</p>
<p>FM就是Factor Machine，因子分解机。<br>FM通过对两两特征组合，引入交叉项特征，提高模型得分；其次是高维灾难，通过引入隐向量（对参数矩阵进行矩阵分解），完成对特征的参数估计。</p>
<h4 id="模型公式"><a href="#模型公式" class="headerlink" title="模型公式"></a>模型公式</h4><p><strong>一般的线性模型</strong><br><img src="/pictures/RecomSys/FM/img1.png" alt="一般线性模型"></p>
<p><strong>二阶多项式模型</strong><br><img src="/pictures/RecomSys/FM/img2.png" alt="二阶多项式模型"><br>上式中，n表示样本的特征数量，x[i]表示第i个特征。<br>与线性模型相比，FM模型多了后面特征组合的部分。</p>
<h4 id="FM求解"><a href="#FM求解" class="headerlink" title="FM求解"></a>FM求解</h4><p>从上面的式子可以看到，组合部分的特征相关参数有 n(n−1)&#x2F;2 个。但是对于稀疏数据来说，同时满足 x i , x[i], x[j] 都不为0的情况十分少，这就会导致 w[i][j] 无法通过训练得到。</p>
<p>为了求解得到w[i][j]，我们对于每一个特征分量 x[i] 引入 <strong>隐向量</strong> V[i] &#x3D; (v[i][1], …, v[i][k])，利用v[i]，v[j]对w[i][j]进行求解。<br><img src="/pictures/RecomSys/FM/img3.png" alt="权重矩阵W求解"></p>
<p>求解v[i]和v[j]的具体过程如下：<br><img src="/pictures/RecomSys/FM/img4.png" alt="核心计算公式"></p>
<h3 id="FFM"><a href="#FFM" class="headerlink" title="FFM"></a>FFM</h3><h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><p>同一个categorical特征经过One-Hot编码生成的数值特征都可以放到同一个field，包括用户性别、职业、品类偏好等。</p>
<p>在FFM中，每一维特征 x[i]，针对其它特征的每一种field f[j]，都会学习一个隐向量 v[i][f]。因此，<em><strong>隐向量不仅与特征相关，也与field相关</strong></em>。也就是说，“Day&#x3D;26&#x2F;11&#x2F;15”这个特征与“Country”特征和“Ad_type”特征进行关联的时候使用不同的隐向量，这与“Country”和“Ad_type”的内在差异相符，也是FFM中“field-aware”的由来。</p>
<p>假设样本的 n 个特征属于 f 个field，那么FFM的二次项有 nf个隐向量。而在FM模型中，每一维特征的隐向量只有一个，即二次项有n个隐向量。FM可以看作FFM的特例，是把所有特征都归属到一个field时的FFM模型。根据FFM的field敏感特性，可以导出其模型方程。<br><img src="/pictures/RecomSys/FM/img5.png" alt="FFM计算公式"><br>其中，fj 是第 j 个特征所属的field。如果隐向量的长度为 k，那么FFM的二次参数有 nfk 个，远多于FM模型的 nk 个。此外，由于隐向量与field相关，FFM二次项并不能够化简，其预测复杂度是 O(kn2)。</p>
<h4 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h4><blockquote>
<ol>
<li>FM和FFM模型的二次项的个数都是 n(n−1)&#x2F;2 个，区别在于FM模型中二次项<strong>存在重复使用的隐向量</strong>，而FFM模型没有，这正是由于FFM的域的概念的存在</li>
<li>FM模型的参数量为nk，FFM模型的参数量为nfk个</li>
<li>FM模型的时间复杂度可以优化为线性的，而FFM模型为nfk（最坏时，即当所有特征都是独自一个域时，为n^2k）</li>
</ol>
</blockquote>
<h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><p>在DSP或者推荐场景中，FFM主要用来评估站内的CTR和CVR，即一个用户对一个商品的潜在点击率和点击后的转化率。<br>CTR和CVR预估模型都是在线下训练，然后线上预测。两个模型采用的特征大同小异，主要分三类：</p>
<blockquote>
<ol>
<li>用户相关的特征: 年龄、性别、职业、兴趣、品类偏好、浏览&#x2F;购买品类等基本信息，以及用户近期点击量&#x2F;购买量&#x2F;消费额等统计信息</li>
<li>商品相关的特征: 商品所属品类、销量、价格、评分、历史CTR&#x2F;CVR等信息</li>
<li>用户-商品匹配特征: 浏览&#x2F;购买品类匹配、浏览&#x2F;购买商家匹配、兴趣偏好匹配等</li>
</ol>
</blockquote>
<p>为了使用FFM方法，所有的特征必须转换成“field_id:feat_id:value”格式，field_id代表特征所属field的编号，feat_id是特征编号，value是特征的值。数值型的特征比较容易处理，只需分配单独的field编号，如用户评论得分、商品的历史CTR&#x2F;CVR等。categorical特征需要经过One-Hot编码成数值型，编码产生的所有特征同属于一个field，而特征的值只能是0或1，如用户的性别、年龄段，商品的品类id等。除此之外，还有第三类特征，如用户浏览&#x2F;购买品类，有多个品类id且用一个数值衡量用户浏览或购买每个品类商品的数量。这类特征按照categorical特征处理，不同的只是特征的值不是0或1，而是代表用户浏览或购买数量的数值。按前述方法得到field_id之后，再对转换后特征顺序编号，得到feat_id，特征的值也可以按照之前的方法获得。 </p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>Recom</tag>
      </tags>
  </entry>
  <entry>
    <title>Wide&amp;Deep</title>
    <url>/2022/09/12/RecomSys/Wide-and-Deep/</url>
    <content><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>推荐系统的主要挑战之一，是同时解决 Memorization 和 Generalization。Wide&amp;Deep模型旨在使得训练得到的模型能够同时获得记忆和泛化能力。</p>
<blockquote>
<p><strong>Memorization</strong>: 根据历史行为数据，产生的推荐通常和用户已有行为的物品直接相关的物品；<br><strong>Generalization</strong>: 会学习新的特征组合，提高推荐物品的多样性。</p>
</blockquote>
<h4 id="记忆能力"><a href="#记忆能力" class="headerlink" title="记忆能力"></a>记忆能力</h4><p>面对拥有大规模离散sparse特征的CTR预估问题，将特征进行非线性转换，然后再使用线性模型是业界非常普遍的做法，最流行的即 <em><strong>LR+特征叉乘</strong></em>。Memorization 通过一系列人工的特征叉乘来构造这些非线性特征，捕捉 sparse 特征之间的高阶相关性，即 <em><strong>“记忆”历史数据中曾经共同出现过的特征对</strong></em>。</p>
<p>典型代表是LR模型，使用大量的原始sparse特征和叉乘特征作为输入，很多原始的dense特征通常会被分桶离散化为sparse特征。</p>
<p>这种做法的优点是：</p>
<blockquote>
<p>模型可解释性高，实现快速高效，特征重要度易于分析。</p>
</blockquote>
<p>缺点是：</p>
<blockquote>
<ol>
<li>需要更多的人工设计；</li>
<li>可能出现过拟合。可以理解为，如果将所有特征叉乘起来，那么几乎相当于纯粹记住每个训练样本，这个极端情况是最细粒度的叉乘，我们可以通过构建更粗粒度的特征叉乘来增强泛化性；</li>
<li>无法捕捉训练数据集中未曾出现过的特征对；</li>
</ol>
</blockquote>
<h4 id="泛化能力"><a href="#泛化能力" class="headerlink" title="泛化能力"></a>泛化能力</h4><p>Generalization 为 sparse 特征学习低维的 dense embedding 来捕捉特征相关性，学习到的embeddings 本身带有一定的语义信息。可以联想到 NLP 的词向量，不同词的词向量有相关性，因此文中也称 Generalization 是基于相关性之间的传递。这类模型的代表是 DNN 和 FM。</p>
<p>Generalization 的优点是更少的人工参与，对历史上没有出现的特征组合有更好的泛化性。但是在推荐系统中，当 user-item matrix 非常稀疏，NN很难为 users 和 items 学习到有效的 embedding。这种情况下，大部分 user-item 应该是没有关联的，但 dense embedding 的方法还是可以得到对所有 user-item pair 的非零预测，因此导致 over-generalize 并推荐不怎么相关的物品。此时 Memorization 就展示了优势，它可以记住这些特殊的特征组合。</p>
<h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>Wide&amp;Deep 模型结合了 LR 和 DNN，其框架图如下所示：<br><img src="/pictures/RecomSys/Wide-and-Deep/img1.png" alt="网络结构"></p>
<h4 id="Wide部分"><a href="#Wide部分" class="headerlink" title="Wide部分"></a>Wide部分</h4><p>该部分是广义线性模型</p>
<blockquote>
<p>y &#x3D; W * [x, f(x)] + b<br>其中，x 和 f(x) 分别表示 <em><strong>原始特征和交叉特征</strong></em>。</p>
</blockquote>
<h4 id="Deep部分"><a href="#Deep部分" class="headerlink" title="Deep部分"></a>Deep部分</h4><p>该部分是前馈神经网络，网络会对一些sparse特征学习一个低维的dense embedding（维度量级通常在O(10)到O(100)之间），然后和一些原始 dense 特征一起作为网络的输入。</p>
<p>每一层隐层计算为：<br><img src="/pictures/RecomSys/Wide-and-Deep/img2.png" alt="隐层计算公式"></p>
<h4 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h4><p>模型选取 logistic loss 作为损失函数，此时 Wide&amp;Deep 最后的预测输出为：<br><img src="/pictures/RecomSys/Wide-and-Deep/img3.png" alt="输出层计算"></p>
<h3 id="联合训练"><a href="#联合训练" class="headerlink" title="联合训练"></a>联合训练</h3><p>联合训练（Joint Training）和集成（Ensemble）是不同的。<br>集成是每个模型单独训练，再将模型的结果汇合。相比于联合训练，集成的每个独立模型都得学的足够好才有利于随后的汇合，因此每个 model size 相对更大。<br>而联合学习的wide部分只需要做一小部分的特征叉乘来弥补deep部分的不足，不需要一个 full-size 的wide模型。</p>
<p>在论文中，作者通过梯度的反向传播，使用 mini-batch stochastic optimization 训练参数，并对 wide 部分使用带 L1正则的 Follow-the-regularized-leader(FTRL)算法，对 deep 部分使用 AdaGrad 算法。</p>
<h3 id="场景应用"><a href="#场景应用" class="headerlink" title="场景应用"></a>场景应用</h3><h4 id="应用背景"><a href="#应用背景" class="headerlink" title="应用背景"></a>应用背景</h4><p>Google Play 商店的 app 推荐中，当一个 user 访问 Google Play，会生成一个包含 user 和 contextual 信息的 query，推荐系统的精排模型会对于候选池中召回的一系列 app（即 item，文中也称 impression）进行打分，按打分生成 app 的排列列表返回给用户。Deep&amp;Wide 对应这里的精排模型，输入 x 包括 &lt;user, contextual, impression&gt;的信息，y &#x3D; 1表示用户下载了 impression app，打分即为 p(y|x)。</p>
<p>实验的Deep &amp; Wide模型结构如下：<br><img src="/pictures/RecomSys/Wide-and-Deep/img4.png" alt="网络结构"></p>
<h4 id="实验细节"><a href="#实验细节" class="headerlink" title="实验细节"></a>实验细节</h4><blockquote>
<ul>
<li>训练样本约5000亿</li>
<li>Categorical 特征（sparse）会有一个过滤阈值，即至少在训练集中出现m次才会被加入</li>
<li>Continuous 特征（dense）通过CDF被归一化到 [0,1] 之间</li>
<li>Categorical 特征映射到32维embeddings，和原始Continuous特征共1200维作为NN输入</li>
<li>Wide部分只用了一组特征叉乘，即被推荐的app ☓ 用户下载的app</li>
<li>线上模型更新时，通过“热启动”重训练，即使用上次的embeddings和模型参数初始化</li>
</ul>
</blockquote>
<p>Wide部分设置很有意思，作者为什么这么做呢？<br>结合业务思考，在Google Play商店的app下载中，不断有新的app推出，并且有很多“非常冷门、小众”的app，而现在的智能手机user几乎全部会安装一系列必要的app。</p>
<p>联想前面对Memorization和Generalization的介绍，此时的Deep部分无法很好的为这些app学到有效的embeddding，而这时Wide可以发挥了它“记忆”的优势，作者在这里选择了 <em><strong>“记忆”user下载的app与被推荐的app之间的相关性</strong></em>，有点类似“装个这个app后还可能会装什么”。</p>
<p>对于Wide来说，它现在的任务是弥补Deep的缺陷，其他大部分的活就交给Deep了，所以这时的Wide相比单独Wide也显得非常“轻量级”，这也是Join相对于Ensemble的优势。</p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>Recom</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop</title>
    <url>/2022/09/05/tools/Hadoop/</url>
    <content><![CDATA[<h2 id="Hadoop概念"><a href="#Hadoop概念" class="headerlink" title="Hadoop概念"></a>Hadoop概念</h2><p>Hadoop 框架是用于计算机集群大数据处理的框架，所以它必须是一个可以部署在多台计算机上的软件。部署了 Hadoop 软件的主机之间通过<strong>套接字</strong> (网络) 进行通讯。<br>Hadoop 主要包含 <strong>HDFS</strong> 和 <strong>MapReduce</strong> 两大组件。</p>
<blockquote>
<ul>
<li>HDFS 负责分布储存数据;</li>
<li>MapReduce 负责对数据进行映射、规约处理，并汇总处理结果。</li>
</ul>
</blockquote>
<p>Hadoop 框架最根本的原理就是利用大量的计算机同时运算来加快大量数据的处理速度。</p>
<h3 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h3><p>Hadoop Distributed File System，Hadoop 分布式文件系统，简称 HDFS。<br>HDFS 用于在集群中储存文件，它所使用的核心思想是 Google 的 GFS 思想，可以存储很大的文件。</p>
<p>在服务器集群中，文件存储往往被要求高效而稳定，HDFS同时实现了这两个优点。</p>
<blockquote>
<ul>
<li>HDFS 高效的存储是通过 <strong>计算机集群独立处理请求</strong> 实现的。因为用户 (一半是后端程序) 在发出数据存储请求时，往往 <em><strong>响应服务器</strong></em> 正在处理其他请求，这是导致服务效率缓慢的主要原因。但如果响应服务器直接分配一个数据服务器给用户，然后 <em><strong>用户直接与数据服务器交互</strong></em>，效率会快很多。</li>
<li>数据存储的稳定性往往通过”多存几份”的方式实现，HDFS 也使用了这种方式。<strong>HDFS 的存储单位是块 (Block)</strong> ，一个文件可能会被分为多个块储存在物理存储器中。因此 HDFS 往往会按照设定者的要求把数据块复制 n 份并存储在不同的数据节点 (储存数据的服务器) 上，如果一个数据节点发生故障数据也不会丢失。</li>
</ul>
</blockquote>
<p><img src="/pictures/tools/Hadoop/img1.png" alt="HDFS架构图"></p>
<h4 id="Block数据块"><a href="#Block数据块" class="headerlink" title="Block数据块"></a>Block数据块</h4><ul>
<li>基本存储单位，一般大小为 <strong>64M</strong> （配置大的块主要是因为：<ul>
<li>减少搜寻时间，一般硬盘传输速率比寻道时间要快，大的块可以减少寻道时间；</li>
<li>减少管理块的数据开销，每个块都需要在NameNode上有对应的记录；</li>
<li>对数据块进行读写，减少建立网络的连接成本）</li>
</ul>
</li>
<li>一个大文件会被拆分成一个个的块，然后存储于不同的机器。如果一个文件少于Block大小，那么实际占用的空间为其文件的大小</li>
<li>基本的读写单位，类似于磁盘的页，每次都是读写一个块</li>
<li>每个块都会被复制到多台机器，默认复制 <strong>3</strong> 份</li>
</ul>
<blockquote>
<p>HDFS 2.x以后的block默认为 <strong>128M</strong></p>
</blockquote>
<h4 id="HDFS节点"><a href="#HDFS节点" class="headerlink" title="HDFS节点"></a>HDFS节点</h4><p>HDFS 运行在许多不同的计算机上，有的计算机专门用于存储数据，有的计算机专门用于指挥其它计算机储存数据。这里所提到的”计算机”我们可以称之为集群中的节点。</p>
<h5 id="命名节点-NameNode"><a href="#命名节点-NameNode" class="headerlink" title="命名节点 NameNode"></a>命名节点 NameNode</h5><p>命名节点 (NameNode) 是用于指挥其它节点存储的节点。任何一个”文件系统”(File System, FS) 都需要具备 <strong>根据文件路径映射到文件</strong> 的功能，命名节点就是用于储存这些映射信息并提供映射服务的计算机，在整个 HDFS 系统中扮演”管理员”的角色，因此 <em><strong>一个 HDFS 集群中只有一个命名节点</strong></em>。</p>
<h5 id="数据节点-DataNode"><a href="#数据节点-DataNode" class="headerlink" title="数据节点 (DataNode)"></a>数据节点 (DataNode)</h5><p>数据节点 (DataNode) 使用来储存数据块的节点。当一个文件被命名节点承认并分块之后将会被储存到被分配的数据节点中去。数据节点具有储存数据、读写数据的功能，其中 <strong>存储的数据块比较类似于硬盘中的”扇区”概念，是 HDFS 存储的基本单位</strong>。</p>
<blockquote>
<ol>
<li>保存具体的block数据</li>
<li>负责数据的读写操作和复制操作</li>
<li>DataNode启动时会向NameNode报告当前存储的数据块信息，后续也会定时报告修改信息</li>
<li>DataNode之间会进行通信，复制数据块，保证数据的冗余性</li>
</ol>
</blockquote>
<h5 id="副命名节点-Secondary-NameNode"><a href="#副命名节点-Secondary-NameNode" class="headerlink" title="副命名节点 (Secondary NameNode)"></a>副命名节点 (Secondary NameNode)</h5><p>副命名节点 (Secondary NameNode) 别名”次命名节点”，是命名节点的”秘书”。这个形容很贴切，因为它并不能代替命名节点的工作，无论命名节点是否有能力继续工作。它主要负责 <strong>分摊命名节点的压力、备份命名节点的状态并执行一些管理工作</strong>，如果命名节点要求它这样做的话。如果命名节点坏掉了，它也可以提供备份数据以恢复命名节点。副命名节点可以有多个。</p>
<h4 id="Hadoop写文件"><a href="#Hadoop写文件" class="headerlink" title="Hadoop写文件"></a>Hadoop写文件</h4><blockquote>
<ol>
<li>客户端将文件写入本地磁盘的 HDFS Client 文件中</li>
<li>当临时文件大小达到一个 block 大小时，HDFS client 通知 NameNode，申请写入文件</li>
<li>NameNode 在 HDFS 的文件系统中创建一个文件，并把该 block id 和要写入的 DataNode 的列表返回给客户端</li>
<li>客户端收到这些信息后，将临时文件写入 DataNodes<br>4.1. 客户端将文件内容写入第一个 DataNode（一般以 4kb 为单位进行传输）<br>4.2. 第一个 DataNode 接收后，将数据写入本地磁盘，同时也传输给第二个 DataNode<br>4.3. 依此类推到最后一个 DataNode，数据在 DataNode 之间是通过 pipeline 的方式进行复制的<br>4.4. 后面的 DataNode 接收完数据后，都会发送一个确认给前一个 DataNode，最终第一个 DataNode 返回确认给客户端<br>4.5. 当客户端接收到整个 block 的确认后，会向 NameNode 发送一个最终的确认信息<br>4.6. 如果写入某个 DataNode 失败，数据会继续写入其他的 DataNode。然后 NameNode 会找另外一个好的 DataNode 继续复制，以保证冗余性<br>4.7. 每个 block 都会有一个校验码，并存放到独立的文件中，以便读的时候来验证其完整性</li>
<li>文件写完后（客户端关闭），NameNode 提交文件（这时文件才可见，如果提交前，NameNode 垮掉，那文件也就丢失了。fsync：只保证数据的信息写到 NameNode 上，但并不保证数据已经被写到DataNode 中）</li>
</ol>
</blockquote>
<p><img src="/pictures/tools/Hadoop/img2.png" alt="HDFS写入数据流程"></p>
<h4 id="Hadoop读文件"><a href="#Hadoop读文件" class="headerlink" title="Hadoop读文件"></a>Hadoop读文件</h4><blockquote>
<ol>
<li>客户端向NameNode发送读取请求</li>
<li>NameNode返回文件的所有block和这些block所在的DataNodes（包括复制节点）</li>
<li>客户端直接从DataNode中读取数据，如果该DataNode读取失败（DataNode失效或校验码不对），则从复制节点中读取（如果读取的数据就在本机，则直接读取，否则通过网络读取）</li>
</ol>
</blockquote>
<p><img src="/pictures/tools/Hadoop/img3.png" alt="HDFS读取数据流程"></p>
<h3 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h3><p>MapReduce是一种可用于数据处理的编程模型。Hadoop可以运行各种版本的MapReduce程序。MapReduce程序本质上是并行运行的，它可以将大规模的数据分析任务分发给任何一个拥有足够多机器的数据中心。</p>
<p>MapReduce任务过程分为两个处理阶段：map阶段和reduce阶段。每个阶段都以键值对作为输入和输出，其类型由程序员选择。</p>
<blockquote>
<ul>
<li><strong>map阶段</strong> – 输入是原始数据。键是某一行起始位置相对于文件起始位置的偏移量。map函数是一个数据准备阶段。</li>
<li><strong>reduce阶段</strong> – 对map阶段的输出值进行处理。reduce函数进行数据进一步的筛选及其他操作。</li>
</ul>
</blockquote>
<p><img src="/pictures/tools/Hadoop/img4.png" alt="MapReduce计算逻辑"></p>
<blockquote>
<p>map: (K1, V1) → list(K2, V2)<br>combine: (K2, list(V2)) → list(K2, V2)<br>reduce: (K2, list(V2)) → list(K3, V3)</p>
</blockquote>
<p>MapReduce主要是先读取文件数据，然后进行Map处理，接着Reduce处理，最后把处理结果写到文件中<br><img src="/pictures/tools/Hadoop/img5.png" alt="MapReduce基本流程"></p>
<h3 id="Hadoop数据倾斜"><a href="#Hadoop数据倾斜" class="headerlink" title="Hadoop数据倾斜"></a>Hadoop数据倾斜</h3>]]></content>
      <categories>
        <category>工具</category>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>tools</tag>
        <tag>Big Data</tag>
      </tags>
  </entry>
</search>
