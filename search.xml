<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>TransX</title>
    <url>/2022/09/11/KG/TransX/</url>
    <content><![CDATA[<p>参考信息：<a href="https://zhuanlan.zhihu.com/p/354867179">https://zhuanlan.zhihu.com/p/354867179</a></p>
<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>知识图谱&#x2F;知识库通常以网络的形式组织知识，网络中每个节点代表实体，边代表实体间关系，因此大部分知识往往可以用三元组（实体1，关系，实体2）来表示。</p>
<p>知识表示学习(Knowledge Representation Learning)，又称知识图谱嵌入(Knowledge Graph Embedding)，是指将由组成知识的实体和关系在低维连续向量空间中表征的过程。</p>
<p>我们以h,r,t分别表示头实体、关系、尾实体，对于一个三元组&lt;h[i],r[i],t[i]&gt;，如果其符合事实，我们称其为置信度(plausibility)为1，如果其不成立，则其置信度为0。</p>
<p>知识表示学习的一般流程为：</p>
<blockquote>
<ol>
<li>随机初始化实体和关系向量；</li>
<li>定义打分函数(Scoring Function)来计算一个三元组的 <strong>置信度</strong>；</li>
<li>最大化置信度来训练实体、关系向量。</li>
</ol>
</blockquote>
<p>从工作流程上而言，可以将KRL分解为四部分：</p>
<blockquote>
<ol>
<li>表征空间，关系和实体表征在一个什么样的空间；</li>
<li>打分函数，如何计算给定三元组的置信度；</li>
<li>补充信息，采用了哪些补充信息（实体类别、实体描述、关系路径等）来参与表示学习；</li>
<li>训练方式，如何生成正负样本，使用何种loss函数等。</li>
</ol>
</blockquote>
<h3 id="表征空间"><a href="#表征空间" class="headerlink" title="表征空间"></a>表征空间</h3><p>表征空间需要满足三个条件：<strong>可微分，可计算概率，可定义打分函数</strong>。</p>
<h4 id="实内积空间模型"><a href="#实内积空间模型" class="headerlink" title="实内积空间模型"></a>实内积空间模型</h4><p>将实体和关系表征在实内积空间中。</p>
<h4 id="复空间模型"><a href="#复空间模型" class="headerlink" title="复空间模型"></a>复空间模型</h4><p>将实体和关系表征在复空间中。复空间主要是能表征平移信息之外的旋转信息。</p>
<h4 id="高斯分布模型"><a href="#高斯分布模型" class="headerlink" title="高斯分布模型"></a>高斯分布模型</h4><p>使用高斯分布去表征实体和关系中的不确定性信息。</p>
<h4 id="流行和群"><a href="#流行和群" class="headerlink" title="流行和群"></a>流行和群</h4><p>这一类模型将知识表征在流形空间(manifold space)，李群(Lie group)或二面体群(dihedral group)。典型代表是ManifoldE，TorusE和DihEdra。</p>
<h3 id="打分函数"><a href="#打分函数" class="headerlink" title="打分函数"></a>打分函数</h3><p>打分函数用于衡量一个三元组的置信度。</p>
<h4 id="基于距离的打分函数"><a href="#基于距离的打分函数" class="headerlink" title="基于距离的打分函数"></a>基于距离的打分函数</h4><p>通过计算实体间的距离来衡量三元组的置信度。其中，基于加性平移的关系模型应用最广。</p>
<blockquote>
<p>h + r &#x3D; t</p>
</blockquote>
<p><img src="/pictures/KGE/TransX/img1.jpg" alt="传统基于距离变换模型"></p>
<p>基于平移表征的关系模型，即将 <strong>关系表示为头实体向尾实体的平移向量</strong>。</p>
<ul>
<li>TransE：基于平移表征；</li>
<li>TransH: 将实体和关系映射到超平面；</li>
<li>TransR：将实体和关系映射到不同的空间；</li>
<li>TransD：构建动态映射矩阵完成实体空间的映射；</li>
<li>TransA：将欧式距离替换成马氏距离；</li>
<li>TransF：松弛了严格平移条件，使用内积作为度量函数</li>
</ul>
<p><img src="/pictures/KGE/TransX/img2.jpg" alt="距离变换模型总结"></p>
<h4 id="基于语义匹配度的打分函数"><a href="#基于语义匹配度的打分函数" class="headerlink" title="基于语义匹配度的打分函数"></a>基于语义匹配度的打分函数</h4><p>基于语义匹配度衡量三元组置信度，通常使用关系矩阵将头实体映射至尾实体。</p>
<blockquote>
<p>h * M &#x3D; t</p>
</blockquote>
<h5 id="线性-x2F-双线性模型"><a href="#线性-x2F-双线性模型" class="headerlink" title="线性 &#x2F; 双线性模型"></a>线性 &#x2F; 双线性模型</h5><p>RESCAL将语义相似度定义为实体关系对的匹配程度，使用双线性函数对其进行表征。但是由于双线性函数满足交换律，所以RESCAL不能表达非对称关系，即(h,r,t)成立而(t,r,h)不成立的情况。同时其计算复杂度较高。</p>
<p>DistMult将双线性映射加以简化为对角阵。但由于DistMult仍然满足交换律，也不能表达非对称关系。</p>
<p>HolE提出使用头尾实体的循环相关操作来表示实体对，定义循环相关运算符，使用循环相关操作表示语义匹配程度。HolE的循环相关操作不满足交换律，所以可以表达非对称关系。</p>
<p><img src="/pictures/KGE/TransX/img3.jpg" alt="语义匹配模型"></p>
<h5 id="张量分解模型"><a href="#张量分解模型" class="headerlink" title="张量分解模型"></a>张量分解模型</h5><p>TuckerER使用Tucker张量分解(Tucker Decomposition)方法对原始矩阵进行分解，并使用分解的核心矩阵来参与打分函数计算。</p>
<p>LowFER提出多模态张量分解双线性池化机制来更好地表达实体和关系之间的语义联系，并通过低秩估计相较于TuckerER降低了计算复杂度。</p>
<h5 id="神经网络模型"><a href="#神经网络模型" class="headerlink" title="神经网络模型"></a>神经网络模型</h5><h6 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h6>]]></content>
      <categories>
        <category>知识表示学习</category>
      </categories>
      <tags>
        <tag>KGE</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop</title>
    <url>/2022/09/05/tools/Hadoop/</url>
    <content><![CDATA[<h2 id="Hadoop概念"><a href="#Hadoop概念" class="headerlink" title="Hadoop概念"></a>Hadoop概念</h2><p>Hadoop 框架是用于计算机集群大数据处理的框架，所以它必须是一个可以部署在多台计算机上的软件。部署了 Hadoop 软件的主机之间通过<strong>套接字</strong> (网络) 进行通讯。<br>Hadoop 主要包含 <strong>HDFS</strong> 和 <strong>MapReduce</strong> 两大组件。</p>
<blockquote>
<ul>
<li>HDFS 负责分布储存数据;</li>
<li>MapReduce 负责对数据进行映射、规约处理，并汇总处理结果。</li>
</ul>
</blockquote>
<p>Hadoop 框架最根本的原理就是利用大量的计算机同时运算来加快大量数据的处理速度。</p>
<h3 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h3><p>Hadoop Distributed File System，Hadoop 分布式文件系统，简称 HDFS。<br>HDFS 用于在集群中储存文件，它所使用的核心思想是 Google 的 GFS 思想，可以存储很大的文件。</p>
<p>在服务器集群中，文件存储往往被要求高效而稳定，HDFS同时实现了这两个优点。</p>
<blockquote>
<ul>
<li>HDFS 高效的存储是通过 <strong>计算机集群独立处理请求</strong> 实现的。因为用户 (一半是后端程序) 在发出数据存储请求时，往往 <em><strong>响应服务器</strong></em> 正在处理其他请求，这是导致服务效率缓慢的主要原因。但如果响应服务器直接分配一个数据服务器给用户，然后 <em><strong>用户直接与数据服务器交互</strong></em>，效率会快很多。</li>
<li>数据存储的稳定性往往通过”多存几份”的方式实现，HDFS 也使用了这种方式。<strong>HDFS 的存储单位是块 (Block)</strong> ，一个文件可能会被分为多个块储存在物理存储器中。因此 HDFS 往往会按照设定者的要求把数据块复制 n 份并存储在不同的数据节点 (储存数据的服务器) 上，如果一个数据节点发生故障数据也不会丢失。</li>
</ul>
</blockquote>
<p><img src="/pictures/tools/Hadoop/img1.png" alt="HDFS架构图"></p>
<h4 id="Block数据块"><a href="#Block数据块" class="headerlink" title="Block数据块"></a>Block数据块</h4><ul>
<li>基本存储单位，一般大小为 <strong>64M</strong> （配置大的块主要是因为：<ul>
<li>减少搜寻时间，一般硬盘传输速率比寻道时间要快，大的块可以减少寻道时间；</li>
<li>减少管理块的数据开销，每个块都需要在NameNode上有对应的记录；</li>
<li>对数据块进行读写，减少建立网络的连接成本）</li>
</ul>
</li>
<li>一个大文件会被拆分成一个个的块，然后存储于不同的机器。如果一个文件少于Block大小，那么实际占用的空间为其文件的大小</li>
<li>基本的读写单位，类似于磁盘的页，每次都是读写一个块</li>
<li>每个块都会被复制到多台机器，默认复制 <strong>3</strong> 份</li>
</ul>
<blockquote>
<p>HDFS 2.x以后的block默认为 <strong>128M</strong></p>
</blockquote>
<h4 id="HDFS节点"><a href="#HDFS节点" class="headerlink" title="HDFS节点"></a>HDFS节点</h4><p>HDFS 运行在许多不同的计算机上，有的计算机专门用于存储数据，有的计算机专门用于指挥其它计算机储存数据。这里所提到的”计算机”我们可以称之为集群中的节点。</p>
<h5 id="命名节点-NameNode"><a href="#命名节点-NameNode" class="headerlink" title="命名节点 NameNode"></a>命名节点 NameNode</h5><p>命名节点 (NameNode) 是用于指挥其它节点存储的节点。任何一个”文件系统”(File System, FS) 都需要具备 <strong>根据文件路径映射到文件</strong> 的功能，命名节点就是用于储存这些映射信息并提供映射服务的计算机，在整个 HDFS 系统中扮演”管理员”的角色，因此 <em><strong>一个 HDFS 集群中只有一个命名节点</strong></em>。</p>
<h5 id="数据节点-DataNode"><a href="#数据节点-DataNode" class="headerlink" title="数据节点 (DataNode)"></a>数据节点 (DataNode)</h5><p>数据节点 (DataNode) 使用来储存数据块的节点。当一个文件被命名节点承认并分块之后将会被储存到被分配的数据节点中去。数据节点具有储存数据、读写数据的功能，其中 <strong>存储的数据块比较类似于硬盘中的”扇区”概念，是 HDFS 存储的基本单位</strong>。</p>
<blockquote>
<ol>
<li>保存具体的block数据</li>
<li>负责数据的读写操作和复制操作</li>
<li>DataNode启动时会向NameNode报告当前存储的数据块信息，后续也会定时报告修改信息</li>
<li>DataNode之间会进行通信，复制数据块，保证数据的冗余性</li>
</ol>
</blockquote>
<h5 id="副命名节点-Secondary-NameNode"><a href="#副命名节点-Secondary-NameNode" class="headerlink" title="副命名节点 (Secondary NameNode)"></a>副命名节点 (Secondary NameNode)</h5><p>副命名节点 (Secondary NameNode) 别名”次命名节点”，是命名节点的”秘书”。这个形容很贴切，因为它并不能代替命名节点的工作，无论命名节点是否有能力继续工作。它主要负责 <strong>分摊命名节点的压力、备份命名节点的状态并执行一些管理工作</strong>，如果命名节点要求它这样做的话。如果命名节点坏掉了，它也可以提供备份数据以恢复命名节点。副命名节点可以有多个。</p>
<h4 id="Hadoop写文件"><a href="#Hadoop写文件" class="headerlink" title="Hadoop写文件"></a>Hadoop写文件</h4><blockquote>
<ol>
<li>客户端将文件写入本地磁盘的 HDFS Client 文件中</li>
<li>当临时文件大小达到一个 block 大小时，HDFS client 通知 NameNode，申请写入文件</li>
<li>NameNode 在 HDFS 的文件系统中创建一个文件，并把该 block id 和要写入的 DataNode 的列表返回给客户端</li>
<li>客户端收到这些信息后，将临时文件写入 DataNodes<br>4.1. 客户端将文件内容写入第一个 DataNode（一般以 4kb 为单位进行传输）<br>4.2. 第一个 DataNode 接收后，将数据写入本地磁盘，同时也传输给第二个 DataNode<br>4.3. 依此类推到最后一个 DataNode，数据在 DataNode 之间是通过 pipeline 的方式进行复制的<br>4.4. 后面的 DataNode 接收完数据后，都会发送一个确认给前一个 DataNode，最终第一个 DataNode 返回确认给客户端<br>4.5. 当客户端接收到整个 block 的确认后，会向 NameNode 发送一个最终的确认信息<br>4.6. 如果写入某个 DataNode 失败，数据会继续写入其他的 DataNode。然后 NameNode 会找另外一个好的 DataNode 继续复制，以保证冗余性<br>4.7. 每个 block 都会有一个校验码，并存放到独立的文件中，以便读的时候来验证其完整性</li>
<li>文件写完后（客户端关闭），NameNode 提交文件（这时文件才可见，如果提交前，NameNode 垮掉，那文件也就丢失了。fsync：只保证数据的信息写到 NameNode 上，但并不保证数据已经被写到DataNode 中）</li>
</ol>
</blockquote>
<p><img src="/pictures/tools/Hadoop/img2.png" alt="HDFS写入数据流程"></p>
<h4 id="Hadoop读文件"><a href="#Hadoop读文件" class="headerlink" title="Hadoop读文件"></a>Hadoop读文件</h4><blockquote>
<ol>
<li>客户端向NameNode发送读取请求</li>
<li>NameNode返回文件的所有block和这些block所在的DataNodes（包括复制节点）</li>
<li>客户端直接从DataNode中读取数据，如果该DataNode读取失败（DataNode失效或校验码不对），则从复制节点中读取（如果读取的数据就在本机，则直接读取，否则通过网络读取）</li>
</ol>
</blockquote>
<p><img src="/pictures/tools/Hadoop/img3.png" alt="HDFS读取数据流程"></p>
<h3 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h3><p>MapReduce是一种可用于数据处理的编程模型。Hadoop可以运行各种版本的MapReduce程序。MapReduce程序本质上是并行运行的，它可以将大规模的数据分析任务分发给任何一个拥有足够多机器的数据中心。</p>
<p>MapReduce任务过程分为两个处理阶段：map阶段和reduce阶段。每个阶段都以键值对作为输入和输出，其类型由程序员选择。</p>
<blockquote>
<ul>
<li><strong>map阶段</strong> – 输入是原始数据。键是某一行起始位置相对于文件起始位置的偏移量。map函数是一个数据准备阶段。</li>
<li><strong>reduce阶段</strong> – 对map阶段的输出值进行处理。reduce函数进行数据进一步的筛选及其他操作。</li>
</ul>
</blockquote>
<p><img src="/pictures/tools/Hadoop/img4.png" alt="MapReduce计算逻辑"></p>
<blockquote>
<p>map: (K1, V1) → list(K2, V2)<br>combine: (K2, list(V2)) → list(K2, V2)<br>reduce: (K2, list(V2)) → list(K3, V3)</p>
</blockquote>
<p>MapReduce主要是先读取文件数据，然后进行Map处理，接着Reduce处理，最后把处理结果写到文件中<br><img src="/pictures/tools/Hadoop/img5.png" alt="MapReduce基本流程"></p>
<h3 id="Hadoop数据倾斜"><a href="#Hadoop数据倾斜" class="headerlink" title="Hadoop数据倾斜"></a>Hadoop数据倾斜</h3>]]></content>
      <categories>
        <category>工具</category>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>tools</tag>
        <tag>Big Data</tag>
      </tags>
  </entry>
  <entry>
    <title>BERT</title>
    <url>/2022/09/04/NLP/BERT/</url>
    <content><![CDATA[<p>BERT全称为Bidirectional Encoder Representation from Transformers，是一个预训练的语言表征模型。<br>它强调了不再像以往一样采用传统的单向语言模型或者把两个单向语言模型进行浅层拼接的方法进行预训练，而是采用新的 <strong>masked language model（MLM）</strong> ，以致能生成深度的双向语言表征。</p>
<p>该模型的主要优点：</p>
<ul>
<li>采用MLM对双向的Transformers进行预训练，以生成深层的双向语言表征；</li>
<li>预训练后，只需要添加一个额外的输出层进行fine-tune，就可以在各种各样的下游任务中取得优异的表现。在此过程中不需要对BERT结构进行修改。</li>
</ul>
<h3 id="BERT提出动机"><a href="#BERT提出动机" class="headerlink" title="BERT提出动机"></a>BERT提出动机</h3><p>预训练语言模型对于下游很多自然语言处理任务都有着显著改善。现有的训练模型的网络结构限制了模型本身的表达能力，最主要的限制就是没有采用<strong>双向编码</strong>的方法来对输入进行编码。这就导致模型只能看见当前时刻之前的信息，而不能同时捕捉当前时刻之后的信息。</p>
<p>在论文中，作者提出了采用BERT(Bidirectional Encoder Representations from Transformers)这一网络结构来实现模型的双向编码学习能力。同时，为了使得模型能够有效的学习到双向编码的能力，BERT在训练过程中使用了基于掩盖的语言模型(Masked Language Model, MLM)，即随机对输入序列中的某些位置进行遮蔽，然后通过模型来对其进行预测。</p>
<p>由于MLM 预测任务能够使得模型编码得到的结果同时包含上下文的语境信息，因此有利于训练得到更深的BERT网络模型。除此之外，在训练BERT的过程中作者还加入了下句预测任务(Next Sentence Prediction, NSP)， 即同时输入两句话到模型中，然后预测第 2 句话是不是第 1 句话的下一句话。</p>
<h3 id="BERT网络结构"><a href="#BERT网络结构" class="headerlink" title="BERT网络结构"></a>BERT网络结构</h3><p>BERT网络结构整体上就是由多层的Transformer Encoder堆叠所形成。其上半部分的结构与之前介绍的Transformer Encoder差不多，只不过在Input部分多了一个<strong>Segment Embedding</strong>。</p>
<h4 id="Input-Embedding"><a href="#Input-Embedding" class="headerlink" title="Input Embedding"></a>Input Embedding</h4><p>在 BERT 中 Input Embedding 模块主要包含三个部分:Token Embedding、Positional Embedding 和 Segment Embedding。</p>
<blockquote>
<ul>
<li>这里需要注意的是 BERT 中的 Positional Embedding 对于每个位置的编码并不是采用公式计算得到，而是类似普 通的词嵌入一样为每一个位置初始化了一个向量，然后随着网络一起训练得到。BERT 开源的预训练模型最大只支持 512 个字符的长度，这是因为其在训练过程中(位置)词表的最大长度只有 512。</li>
<li>Segment Embedding 的作用是用来区分输入序列中的不同部分，其本质就是通过一个普通的词嵌入来区分每一个序列所处的位置。</li>
</ul>
</blockquote>
<p>最后，将这3个Embedding进行相加（并进行标准化）便得到了最终的Input Embedding部分的输出。<br><img src="/pictures/NLP/BERT/img1.png" alt="BERT的Embedding输入"><br>最上面的 Input 表示原始的输入序列，其中第一个字符“[CLS]” 是一个特殊的分类标志，如果下游任务是做文本分类的话，那么在 BERT 的输出 结果中可以只取“[CLS]”对应的向量进行分类即可**(不过实验表明，取所有位置向量的均值往往有着更好的效果)**；而其中的“[SEP]”字符则是用来作为将两句话分开的标志。</p>
<h4 id="BERT-Encoder"><a href="#BERT-Encoder" class="headerlink" title="BERT Encoder"></a>BERT Encoder</h4><p>在论文中作者分别用 L 来表示 BertLayer 的层数，即 BertEncoder 是由 L 个 BertLayer 所构成;用 H 来表示模型的维度;用 A 来表示多头注意力中多头的个数。<br>同时，在论文中作者分别就 BERT_BASE (L&#x3D;12, H&#x3D;768, A&#x3D;12) 和 BERT_LARGE (L&#x3D;24,H&#x3D;1024,A&#x3D;16)这两种尺寸的 BERT 模型进行了实验对比。</p>
<h4 id="MLM与NSP"><a href="#MLM与NSP" class="headerlink" title="MLM与NSP"></a>MLM与NSP</h4><p>对于 MLM 任务来说，其做法是随机掩盖掉输入序列中15% 的 Token(即用“[MASK]”替换掉原有的 Token)，然后在 BERT 的输出结果中 取对应掩盖位置上的向量进行真实值预测。<br>虽然 MLM 的这种做法能够得到一个很好的预训练模型，但 是仍旧存在不足之处。由于在 fine-tuning 时，由于输入序列中并不存在“[MASK]” 这样的 Token，因此这将导致 pre-training 和 fine-tuning 之间存在不匹配不一致的 问题(GAP)。<br>为了解决这一问题，作者在原始 MLM 的基础了做了部分改动，即先选定15% 的Token，然后将其中的80%替换为“[MASK]”、10%随机替换为其它Token、 剩下的10% 不变。最后取这15% 的 Token 对应的输出做分类来预测其真实值。</p>
<p>由于很多下游任务需要依赖于分析两句话之间的关系来进行建模，例如问题 回答等。为了使得模型能够具备有这样的能力，作者在论文中又提出了二分类的 下句预测任务。<br>具体地，对于每个样本来说都是由 A 和 B 两句话构成，其中50%的情况 B 确实为 A 的下一句话(标签为 IsNext)，另外的50%的情况是 B 为语料中其它 的随机句子(标签为 NotNext)，然后模型来预测 B 是否为 A 的下一句话。<br><img src="/pictures/NLP/BERT/img2.png" alt="BERT的训练任务"></p>
<blockquote>
<p>总的来说，如果单从网络结构上来看 BERT 并没有太大的创新，这也正如作 者所说“BERT 整体上就是由多层的 Transformer Encoder 堆叠而来”，并且所谓 的“bidirectional”其实指的也就是 Transformer 中的 self-attention 机制。同时， 在掌柜看来真正让 BERT 表现出色的应该是基于 MLM 和 NSP 这两种任务的预 训练过程，使得训练得到的模型具有强大的表征能力。</p>
</blockquote>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>FastText</title>
    <url>/2022/09/04/NLP/FastText/</url>
    <content><![CDATA[<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>fasttext是facebook开源的一个词向量与文本分类工具，在2016年开源，典型应用场景是“带监督的文本分类问题”。提供简单而高效的 <strong>文本分类</strong> 和 <strong>表征学习</strong> 的方法，性能比肩深度学习而且速度更快。</p>
<p>FastText结合了自然语言处理和机器学习中的思想：</p>
<ul>
<li>使用词袋以及N-gram袋表征语句；</li>
<li>使用子字信息；</li>
<li>通过隐藏表征在类别间共享信息</li>
</ul>
<h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>核心思想：将整篇文档的词以及N-gram向量 <strong>叠加平均</strong> 得到文档向量，然后使用文档向量做softmax分类。</p>
<h4 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h4><p>FastText架构和CBOW架构类似。不同之处在于，FastText预测标签，CBOW通过上下文预测中间词。<br><img src="/pictures/NLP/FastText/img1.jpg" alt="FastText模型架构"></p>
<blockquote>
<p><strong>注意：</strong> 此架构图并没有展示词向量的训练过程。FastText模型也只有三层：输入层，隐含层，输出层。输入是多个向量表示的单词，输出是特定的Target，隐含层是对多个词向量的叠加平均。</p>
</blockquote>
<p>与CBOW模型不同的是，</p>
<ul>
<li>CBOW的输入是目标单词的上下文，FastText的输入是多个单词以及其N-gram特征，这些特征用来表示单个文档；</li>
<li>CBOW的输入单词是被one-hot编码过后的，FastText的输入特征是被embedding后的；</li>
<li>CBOW的输出是目标词汇，FastText的输出是文档对应的类标。</li>
</ul>
<h4 id="层序softmax"><a href="#层序softmax" class="headerlink" title="层序softmax"></a>层序softmax</h4><p>对于有大量类别的数据集，FastText使用一个分层分类器，降低计算复杂度。<br>FastText利用了类别不均衡这个事实，通过使用Huffman算法建立用于表征类别的树形结构。因此，出现频次较高的类别更加接近根节点。</p>
<h4 id="N-gram特征"><a href="#N-gram特征" class="headerlink" title="N-gram特征"></a>N-gram特征</h4><p>Word2Vec把语料库中的每个单词作为一个原子。这忽略了单词内部的形态特征，比如apple和apples。传统的Word2Vec中，这种单词内部形态信息因为它们被转换为不同的id丢失了。</p>
<p>为了克服这个问题，FastText使用了字符级别的N-gram来表示一个单词。对于单词apple，假设N的取值为3，则它的trigram有：</p>
<blockquote>
<p>“&lt;ap”,  “app”,  “ppl”,  “ple”, “le&gt;”</p>
</blockquote>
<p>其中，&lt;表示前缀，&gt;表示后缀。于是，我们可以用这些trigram来表示“apple”这个单词，进一步，我们可以用这5个trigram的向量叠加来表示“apple”的词向量。</p>
<p>这带来两点好处：</p>
<ul>
<li>对于低频词生成的词向量效果会更好。因为它们的N-gram可以和其他词共享；</li>
<li>对于训练词库之外的单词，仍然可以构建它们的词向量。我们可以叠加它们的字符级N-gram向量。</li>
</ul>
<h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><p><strong>模型搭建步骤：</strong></p>
<ol>
<li>添加输入层：Embedding层输入是一批文档，每个文档有一个词汇索引序列构成；</li>
<li>添加隐含层：投影层对一个文档中所有单词的向量进行叠加平均；</li>
<li>添加输出层：Softmax层</li>
<li>指定损失函数，优化器类型，评价指标，编译模型。</li>
</ol>
<p><strong>训练数据feed模型步骤</strong></p>
<ol>
<li>将文档分好词，构建词汇表；</li>
<li>对类标进行one-hot化；</li>
<li>对一批文本，将每个文本转化为词索引序列，每个类标转化为one-hot向量</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><h4 id="如何获得词向量？"><a href="#如何获得词向量？" class="headerlink" title="如何获得词向量？"></a>如何获得词向量？</h4><p><strong>累加平均</strong> 一个词的N-gram集合包括自身整个词的隐向量表示。</p>
<h4 id="FastText和Word2Vec区别"><a href="#FastText和Word2Vec区别" class="headerlink" title="FastText和Word2Vec区别"></a>FastText和Word2Vec区别</h4><p>相似处：</p>
<blockquote>
<ol>
<li>图模型结构很想，都是采用embedding向量形式，得到word的隐向量表达；</li>
<li>采用很多相似的优化，比如使用层级sotmax优化训练和预测中的打分速度</li>
</ol>
</blockquote>
<p>不同之处：</p>
<blockquote>
<ol>
<li>输入层：Word2Vec的输出层，是context window内的term；而FastText对应的整个sentence的内容，包括term和N-gram的内容；</li>
<li>输出层：Word2Vec的输出层，对应的是每个term，计算某个term的概率最大；而FastText的输出层对应的分类的label</li>
</ol>
</blockquote>
<p>两者本质的不同，体现在H-softmax的使用：</p>
<blockquote>
<ul>
<li>Word2Vec的目标是得到词向量，该词向量最终是在输入层中得到。输出层对应的H-softmax也会生成一系列的向量，但最终都会被抛弃；</li>
<li>FastText充分利用H-softmax的分类功能，遍历分类树的所有叶节点，找到概率最大的label</li>
</ul>
</blockquote>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>FastText的学习速度比较快，效果不错。fastText适用于分类类别非常大而且数据集足够多的情况。当分类类别比较小或者数据集比较少的话，很容易过拟合。</p>
<p>可以完成无监督的词向量的学习，可以学习得到词向量；也可以用于有监督学习的分类任务（新闻文本分类，垃圾邮件分类，情感分析，电商中用户评论的褒贬分析）</p>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer</title>
    <url>/2022/09/04/NLP/Transformer/</url>
    <content><![CDATA[<p>参考资料：<a href="https://zhuanlan.zhihu.com/p/420820453">https://zhuanlan.zhihu.com/p/420820453</a></p>
<h3 id="多头注意力机制"><a href="#多头注意力机制" class="headerlink" title="多头注意力机制"></a>多头注意力机制</h3><h4 id="提出背景"><a href="#提出背景" class="headerlink" title="提出背景"></a>提出背景</h4><p>现在主流的序列模型都是基于复杂的循环神经网络或者卷积神经网络构造而来的Encoder-Decoder模型。传统的Encoder-Decoder架构在建模过程中，下一时刻的计算过程会很依赖于上一时刻的输出，这种固有属性导致很难以 <strong>并行</strong> 的方式进行计算。<br>基于以上原因，提出了一种全新的Transformer架构来解决这一问题。Transformer架构的优点在于它完全摒弃了传统的循环结构，取而代之的是通过 <strong>注意力机制</strong> 来计算模型输入与输出的隐含表示，即自注意力机制。<br><img src="/pictures/NLP/Transformer/img1.png" alt="Transformer的网络结构"></p>
<blockquote>
<p>自注意力机制就是通过某种运算直接计算得到句子在编码过程中每个位置上的注意力权重；然后再以权重和的形式来计算得到整个句子的隐含向量表示。</p>
</blockquote>
<h4 id="什么是self-attention"><a href="#什么是self-attention" class="headerlink" title="什么是self-attention?"></a>什么是self-attention?</h4><p>注意力机制可以描述为将query和一系列的key-value对映射到某个输出的过程，而这个输出向量就是根据query和key计算得到的权重作用于value上的权重和。<br><img src="/pictures/NLP/Transformer/img2.png" alt="自注意力机制"><br>自注意力机制的核心过程就是通过Q和K计算得到注意力权重；然后再作用于V得到整个权重和的输出。计算公式如下：<br><img src="/pictures/NLP/Transformer/img3.png" alt="自注意力机制计算公式"><br>之所以要对QK进行缩放，是因为对于较大的dk来说，完成QK计算后会得到很大的值。而这将导致在经过 <strong>softmax</strong> 操作后产生非常小的梯度，不利于网络的训练。</p>
<h4 id="Q-K-V怎么来的？"><a href="#Q-K-V怎么来的？" class="headerlink" title="Q,K,V怎么来的？"></a>Q,K,V怎么来的？</h4><p>假设输入序列“我是谁”，通过embedding映射得到3x4的矩阵进行句子表示。<br>Q、K和V其实就是输入X分别乘以3个不同的矩阵计算而来（但这仅仅局限于Encoder和Decoder在各自输入部分利用自注意力机制进行编码的过程，Encoder和Decoder交互部分的Q、K和V另有指代）。此处对于计算得到的Q、K、V，你可以理解为这是 <em><strong>对于同一个输入进行3次不同的线性变换来表示其不同的3种状态</strong></em>。<br><img src="/pictures/NLP/Transformer/img4.jpg" alt="QKV是怎么来的"><br>在计算得到Q、K、V之后，就可以进一步计算得到权重向量。<br><img src="/pictures/NLP/Transformer/img5.jpg" alt="注意力权重计算图"><br>从上图可以知道，通过权重矩阵模型就可以知道当前位置的向量，应该以何种方式（权重）将注意力集中到不同的位置上。 <em><strong>模型在对当前位置的信息进行编码时，会过度将注意力集中于自身位置</strong></em>，而这会导致其忽略其他位置信息。因此，作者使用的解决方案就是 <strong>多头注意力机制</strong>。<br>计算得到权重矩阵后，便可以将其作用于V，进而得到最终的编码输出。<br><img src="/pictures/NLP/Transformer/img6.jpg" alt="编码输出计算"><br>对于最终的输出的编码向量，每个位置的编码向量其实就是 <strong>所有向量的加权和</strong>。这也就体现了自注意力机制在编码过程中的权重分配过程。</p>
<blockquote>
<p>有了自注意力机制后，仅需要对于原始输入进行几次矩阵变换就能够得到最终包含不同注意力信息的编码向量。解决了传统序列逆袭那个在编码过程中需要无法并行的弊端。</p>
</blockquote>
<h4 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head-Attention"></a>Multi-Head-Attention</h4><p>自注意力机制的缺陷在于：<em><strong>模型对于当前位置的信息进行整合编码时候，会过多的将注意力集中于自身位置</strong></em>，影响模型的表征能力。<br>多头注意力机制可以解决上述问题（对于周围信息获取受限的问题），并且还能够给予注意力层的输出包含有不同子空间的编码表示信息，增强模型的表达能力。<br>多头注意力机制就是将原始的输入序列进行多组的自注意力处理；然后将每组自注意力机制的结果 <strong>拼接</strong>起来进行一次线性变换得到最终的输出结果。<br><img src="/pictures/NLP/Transformer/img7.png" alt="多头注意力机制网络结构"><br>其计算公式为：<br><img src="/pictures/NLP/Transformer/img8.png" alt="多头注意力机制计算公式"><br>论文中，作者使用 <strong>8</strong> 个并行的自注意力模块来构建一个自注意力层，并且限定每个模块的维度为 <strong>64</strong>。论文中使用的多头注意力机制其实就是将一个大的高维单头拆分成h个多头。<br><img src="/pictures/NLP/Transformer/img9.jpg" alt="多头注意力计算"><br>根据输入序列X和W1可以得到Q1,K1,V1，进一步根据自注意力计算公式得到输出Z1；同理，可以得到另一个自注意力模块得到输出Z2；最后，将Z1,Z2水平堆叠形成Z，乘以W便可以得到最终的多头注意力层的输出。</p>
<h3 id="位置编码与编码解码过程"><a href="#位置编码与编码解码过程" class="headerlink" title="位置编码与编码解码过程"></a>位置编码与编码解码过程</h3><h4 id="Embedding机制"><a href="#Embedding机制" class="headerlink" title="Embedding机制"></a>Embedding机制</h4><h5 id="Token-Embedding"><a href="#Token-Embedding" class="headerlink" title="Token Embedding"></a>Token Embedding</h5><p>在Transformer模型中，首先要将文本通过Embedding层映射到低维稠密的向量空间，得到向量化表示，即Token Embedding。</p>
<blockquote>
<p>如果是换做之前的网络模型，例如CNN或者RNN，那么对于文本向量化的步骤就到此结束了，因为这些网络结构本身已经具备了捕捉时序特征的能力，不管是CNN中的n-gram形式还是RNN中的时序形式。</p>
</blockquote>
<p>自注意力机制在实际运算过程中，不过是几个矩阵来回相乘进行线性变换，这就导致即使打乱词序，最终计算得到的结果本质上没有任何变化。 <strong>自注意力机制会丢失文本原有的序列信息！</strong></p>
<blockquote>
<p>举个例子：在经过词嵌入表示后，序列“武松 打 虎”和“虎 打 武松”经过相同的权重矩阵后，输出结果并没有任何区别，只是交换了对应的位置。</p>
</blockquote>
<p>为了解决自注意力机制丢失序列信息问题，引入了positional Embedding来刻画数据在时序上的特征。</p>
<h5 id="Positional-Embedding"><a href="#Positional-Embedding" class="headerlink" title="Positional Embedding"></a>Positional Embedding</h5><p>作者采用如下规则生成各个维度的位置信息：<br><img src="/pictures/NLP/Transformer/img10.png" alt="位置信息生成公式"></p>
<p>在交换位置前与交换位置后，与同一个权重矩阵进行线性变换后的结果截然不同。因此，这就证明通过Positional Embedding可以弥补自注意力机制不能捕捉序列时序信息的缺陷。</p>
<h4 id="Transformer网络结构"><a href="#Transformer网络结构" class="headerlink" title="Transformer网络结构"></a>Transformer网络结构</h4><p>一个单层Transformer网络结构图。<br><img src="/pictures/NLP/Transformer/img11.png" alt="Transformer网络结构"></p>
<h5 id="Encoder层"><a href="#Encoder层" class="headerlink" title="Encoder层"></a>Encoder层</h5><p>对于Encoder部分来说其内部主要由两部分网络所构成(6层堆叠)：<em><strong>多头注意力机制</strong></em> 和 <em><strong>两层前馈神经网络</strong></em>。<br>同时，对于这两部分网络来说，都加入了残差连接，并且在残差连接后还进行了层归一化操作。对于每个部分来说其输出均为LayNorm(x + Sub-Layer(x))，并且在都加入了Dropout操作。<br>进一步，为了便于在这些地方使用残差连接，这两部分网络输出向量的维度均为 <strong>512</strong>。<br>对于第2部分的两层全连接网络来说，其具体计算过程为<br><img src="/pictures/NLP/Transformer/img12.png" alt="FFN层计算"><br>其中输入的维度为 <strong>512</strong>，第1层全连接层的输出维度为 <strong>2048</strong>，第2层全连接层的输出为 <strong>512</strong>，且同时 <em><strong>仅对于第1层网络的输出</strong></em> 还运用了Relu激活函数。</p>
<h5 id="Decoder层"><a href="#Decoder层" class="headerlink" title="Decoder层"></a>Decoder层</h5><p>对于Decoder部分来说，其整体上与Encoder类似（6层堆叠），只是多了一个用于与Encoder输出进行交互的多头注意力机制。<br>不同于Encoder部分，在Decoder中一共包含有3个部分的网络结构。最上面的和最下面的部分（暂时忽略Mask）与Encoder相同，只是多了中间这个与Encoder输出（Memory）进行交互的部分，作者称之为“Encoder-Decoder attention”。<br>对于这部分的输入，<strong>Q来自于下面多头注意力机制的输出，K和V均是Encoder部分的输出（Memory）经过线性变换后得到</strong>。而作者之所以这样设计也是在模仿传统Encoder-Decoder网络模型的解码过程。<br>传统的基于Encoder-Decoder的Seq2Seq翻译模型的解码过程：<br><img src="/pictures/NLP/Transformer/img13.jpg" alt="传统的Seq2Seq翻译模型"><br>左半部分是编码器，右下部分为解码器，右上部分为注意力机制部分。 ~h[i]表示<strong>编码过程</strong>中，各个时刻的隐含状态，称之为每个时刻的Memory；h[t]表示解码当前时刻的隐含状态。此时，注意力机制的思想在于，<strong>希望模型能够在解码时，参考编码阶段每个时刻的记忆</strong>。</p>
<blockquote>
<ol>
<li>解码第一个时刻”s”时，h[t]会首先同每一个记忆状态~h[i]进行相似度计算，得到注意力权重；</li>
<li>然后，通过对隐含状态的加权求和，得到context vector内容</li>
</ol>
</blockquote>
<p>以上是传统的解码交互方案。在Transformer中，K和V均是编码部分的输出Memory经过线性变换后的结果（此时Memory中包含了原始输入序列每个位置的编码信息），而Q是解码部分多头注意力机制输出的隐含向量经过线性变换后的结果。</p>
<blockquote>
<ol>
<li>首先，通过Q和K交互计算得到注意力权重矩阵；</li>
<li>然后，通过注意力权重与V进行计算，得到权重解码向量。此向量考虑了memory中各个位置编码信息的输出向量。</li>
<li>最后，得到解码向量后，经过两层全连接层后，将其输入到分类层进行分类得到当前时刻的解码输出值。</li>
</ol>
</blockquote>
<h5 id="QKV的来源"><a href="#QKV的来源" class="headerlink" title="QKV的来源"></a>QKV的来源</h5><p>根据Transformer结构图可知，在整个Transformer中涉及到自注意力机制的一共有3个部分：</p>
<blockquote>
<ul>
<li>Encoder中的Multi-Head Attention；</li>
<li>Decoder中的Masked Multi-Head Attention；</li>
<li>Encoder和Decoder交互部分的Multi-Head Attention。</li>
</ul>
</blockquote>
<ol>
<li>对于Encoder中的Multi-Head Attention来说，其原始q、k、v均是Encoder的Token输入经过Embedding后的结果。q、k、v分别经过一次线性变换（各自乘以一个权重矩阵）后得到了Q、K、V，然后再进行自注意力运算得到Encoder部分的输出结果Memory。</li>
<li>对于Decoder中的Masked Multi-Head Attention来说，其原始q、k、v均是Decoder的Token输入经过Embedding后的结果。q、k、v分别经过一次线性变换后得到了Q、K、V，然后再进行自注意力运算得到Masked Multi-Head Attention部分的输出结果，即待解码向量。</li>
<li>对于Encoder和Decoder交互部分的Multi-Head Attention，其原始q、k、v分别是上面的带解码向量、Memory和Memory。q、k、v分别经过一次线性变换后得到了Q、K、V，然后再进行自注意力运算得到Decoder部分的输出结果。之所以这样设计也是在模仿传统Encoder-Decoder网络模型的解码过程。</li>
</ol>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>Word2Vec</title>
    <url>/2022/09/04/NLP/Word2Vec/</url>
    <content><![CDATA[<h3 id="什么是Word2Vec"><a href="#什么是Word2Vec" class="headerlink" title="什么是Word2Vec?"></a>什么是Word2Vec?</h3><p>Word2Vec模型实际上分了两个部分，第一部分建立模型，第二部分通过模型获取嵌入词向量。<br>Word2Vec的整个建模过程实际上与自编码器的思想很相似。</p>
<blockquote>
<p>先基于训练数据构建神经网络。当模型训练好以后，我们并不会使用这个训练好的模型处理新的任务，我们需要的是通过训练数据学习得到的参数，例如隐层的权重矩阵。</p>
</blockquote>
<p>Word2Vec的训练模型本质上是只具有一个隐含层的神经元网络，从大量文本语料中以无监督的方式学习语义知识。<br><img src="/picturesi/NLP/Word2Vec/img1.jpg.png" alt="Word2Vec单层网络结构"></p>
<blockquote>
<ul>
<li>输入是One-Hot向量，Hidden Layer的激活函数是线性。Output Layer维度和Input Layer维度相同，用的是Softmax回归；</li>
<li>训练Word2Vec需要用到反向传播算法，本质是链式求导；</li>
<li>我们并不关心模型训练任务，我们真正需要的是这个模型通过学习得到的参数，即隐层的权重矩阵；</li>
<li>Word2Vec本质是一种降维操作。</li>
</ul>
</blockquote>
<p>Word2Vec其实就是通过学习文本来用词向量的方式表征词的语义信息，即通过一个嵌入空间是的语义相似的单词在该空间内距离很近。<br><strong>Embedding</strong> 其实就是一个映射，将单词从原先所属的空间映射到新的多维空间中。通过对词汇表中单词进行这种数值表示方式的学习，能够进行 <em><strong>向量化</strong></em> 的操作。  </p>
<h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>Word2Vec模型中，主要有两种结构：</p>
<blockquote>
<ul>
<li><strong>CBOW模型</strong> ：训练输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量；  </li>
<li><strong>Skip-gram模型</strong> ：输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量；</li>
</ul>
</blockquote>
<p><img src="/pictures/NLP/Word2Vec/img2.png" alt="Word2Vec网络结构"></p>
<h4 id="Skip-gram"><a href="#Skip-gram" class="headerlink" title="Skip-gram"></a>Skip-gram</h4><p>Skip-gram模型，通过中间词预测上下文。</p>
<ul>
<li>首先，选择句子中的一个词作为中心词；</li>
<li>定义skip_window参数，限制从中心词左右可以选词的范围；</li>
<li>神经网络基于这些训练数据将会输出一个概率分布，这个概率代表词典中每个词是上下文的可能性。</li>
</ul>
<p>训练样本的构成是通过选择输入词前后skip_window范围内的词语与输入词进行组合。下图中，蓝色代表input word，方框内代表位于窗口内的单词。<br><img src="/pictures/NLP/Word2Vec/img5.png" alt="训练样本构建"><br>模型将会从每对单词出现的次数中学习得到统计规律。</p>
<p>以下是Skip-gram模型结构：<br><img src="/pictures/NLP/Word2Vec/img3.png" alt="Skip-gram网络结构"><br>隐层没有使用任何激活函数，但是输出层使用了softmax。<br>我们基于成对的单词来对神经网络进行训练，训练样本是上述单词对，其中input word和output word都是onehot向量。最终模型输出是一个概率分布。</p>
<blockquote>
<p>可以看成y &#x3D; f(x)模型的并联，cost function是单个cost function的累加 <strong>（取log之后）</strong>。</p>
</blockquote>
<h4 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h4><p><img src="/pictures/NLP/Word2Vec/img4.jpg" alt="CBOW网络结构"><br>注意到，跟Skip-gram模型的并联不同，CBOW输入要对多个单词进行输入处理，一般是求和然后平均，输出的cost function不变。</p>
<blockquote>
<ol>
<li>输入层：上下文单词的one-hot向量表示；</li>
<li>所有one-hot向量分别乘以共享的输入权重矩阵W；</li>
<li>所得的向量 <strong>相加求平均</strong> 作为隐层向量；</li>
<li>乘以输出矩阵W’；</li>
<li>得到向量，经过softmax函数处理得到V-dim概率分布；</li>
<li>概率最大的index所指示的单词作为预测词与true label的one-hot做比较，误差越小越好（根据误差更新权重矩阵）。</li>
</ol>
</blockquote>
<h3 id="训练Tricks"><a href="#训练Tricks" class="headerlink" title="训练Tricks"></a>训练Tricks</h3><p>Word2Vec本质上是一个语言模型，它的输出节点数是V个，对应了V个词语，本质上是一个多分类问题。但实际当中，词表数量巨大，计算复杂度巨高，所以需要技巧来加速训练。</p>
<blockquote>
<ul>
<li>层级softmax：本质是把N分类问题变成log(N)次的二分类；</li>
<li>负采样：本质是预测总体类别的一个子集</li>
</ul>
</blockquote>
<h4 id="负采样"><a href="#负采样" class="headerlink" title="负采样"></a>负采样</h4><p>在训练神经网络时，每个训练样本都将会调整所有神经网络中参数。词汇表决定了Word2Vec模型将会有非常大的权重矩阵，并且所有权重参数会随着数十亿训练昂呢不断调整。<br>负采样每次让一个训练样本更新一小部分的权重参数，从而降低梯度下降过程中的计算成本。  </p>
<p>负样本的选择规则：一个单词被选作负采样的概率与它出现的频次有关，出现频次越高的单词越容易被选择作为负样本，经验公式如下：<br><img src="/pictures/NLP/Word2Vec/img6.png.webp" alt="负采样概率"><br>f(w)代表每个单词被赋予的一个权重，即出现的词频。</p>
<h4 id="层序Softmax"><a href="#层序Softmax" class="headerlink" title="层序Softmax"></a>层序Softmax</h4><p>Huffman原理：权重越大的节点，越靠近根节点。</p>
<blockquote>
<ol>
<li>对每个词按照权重进行排序，将每次词看成一个独立的单节点的树；</li>
<li>合并最小的两个子树，新的根节点权重为两者根节点权重之和；</li>
<li>将新的树插入排序进树集合中；</li>
<li>重复2，3步骤，直到合并所有树。</li>
</ol>
</blockquote>
<h3 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h3><blockquote>
<p>cbow是用周围词预测中心词，训练过程中其实是在从output的loss学习周围词的信息也就是embedding，但是在中间层是average的，一共预测V次；<br>skip-gram是用中心词预测周围词，对每一个中心词都有K个词作为output，对一个词的预测有K次，所以能够更有效的从context中学习信息，共预测K*V次，因此，skip-gram的训练时间更长。</p>
</blockquote>
<p>鉴于skip-gram学习的词向量更细致，当 <strong>数据量较少或者语料库中有大量低频词</strong> 时，使用skip-gram学习比较合适。</p>
<blockquote>
<p>CBOW中的目标函数是使条件概率P(w|context(w))最大化<br>Skip-gram中的目标函数是使条件概率P(context(w)|w)最大化</p>
</blockquote>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>K-BERT</title>
    <url>/2022/09/09/NLP/K-BERT/</url>
    <content><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>预训练的语言表示模型从大型语料库中捕获一般语言的表示，但是 <strong>缺乏领域特定的知识</strong>。</p>
<p>过多的知识加入会使得 <strong>句子偏离正确的含义</strong>，这就是知识噪声问题。</p>
<blockquote>
<p>如何将外部知识整合到模型中成了一个关键点，这一步通常存在两个难点：</p>
<ul>
<li>异构嵌入空间（Heterogeneous Embedding Space）： 即文本的单词embedding和知识库的实体embedding通常是通过不同方式获取的，使得向量空间不一致。</li>
<li>知识噪声（Knowledge Noise）： 即过多的知识融合可能会使原始句子偏离正确的本意。</li>
</ul>
</blockquote>
<p>为了克服上述问题，K-Bert引入了<strong>软定位</strong> 和 <strong>可见矩阵</strong> 来限制插入知识的影响。</p>
<p>K-BERT能够从预先训练好的BERT中加载模型参数，因此不需要单独的预训练，只需要一个KG数据，K-BERT就很容易将领域知识注入到模型中。</p>
<h3 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h3><p>K-BERT模型主要包括四部分：知识层（Knowledge layer）、嵌入层（Embedding layer）、可见层（Seeing layer）和 Mask-Transformer编码层（Mask-Transformer Encoder）<br><img src="/pictures/NLP/K-BERT/img1.jpg" alt="K-BERT模型架构"></p>
<p>如上图所示，K-BERT主要由4个层组成，分别是：</p>
<ul>
<li>knowledge layer：知识层，顾名思义是将知识图谱的事实融入到输入中，构建 <strong>sentence tree</strong> 作为新的输入。</li>
<li>embedding layer：将sentence tree 进行embedding，转换成向量表达。</li>
<li>seeing layer：该层的作用是为了避免知识噪声（Knowledge Noise）而引入的，主要是通过构建 visible matrix，<strong>限定每个字只能够看到跟自己相关的上下文以及知识</strong>，从而避免了知识噪声的引入。</li>
<li>mask-transformer：mask-transformer 是在对transformer的一个改进，对于其中的self-attention，根据 <strong>visible matrix 限制了每个字的attention的范围</strong>，避免了字对于其他无相关的信息的关注。</li>
</ul>
<p>对于一个输入的句子，</p>
<ol>
<li>knowledge layer 首先是从知识图谱KG中找到相关的三元组，</li>
<li>然后将这些三元组插入到输入的 input sentence 中，形成知识丰富（knowledge-rich）的句子树（sentence tree）。</li>
<li>句子树然后同时输入给 embedding layer 以及 seeing layer，从而获得一个 token 级别的 embedding 表示以及一个 visible matrix。</li>
<li>这个visible matrix 是用来控制每个token的可见域（visible scope），以防止输入的句子因为太多的知识嵌入而发生意思的改变。</li>
</ol>
<h4 id="Knowledge-Layer"><a href="#Knowledge-Layer" class="headerlink" title="Knowledge Layer"></a>Knowledge Layer</h4><p>知识层主要用于句子知识嵌入（knowledge injection）以及句子树（sentence tree）的转换。<br><img src="/pictures/NLP/K-BERT/img2.jpg" alt="句子树结构"></p>
<p>举例说明句子树的构建：<br><img src="/pictures/NLP/K-BERT/img3.jpg" alt="句子树构建的例子"></p>
<h4 id="Embedding-Layer"><a href="#Embedding-Layer" class="headerlink" title="Embedding Layer"></a>Embedding Layer</h4><p>和 BERT 类似，输入的句子需要经过embedding，作为模型的输入。具体 embedding 由三个部分组成，分别是 token embedding，soft position embedding 以及 segment embedding。<br><img src="/pictures/NLP/K-BERT/img4.jpg" alt="Embedding表示"></p>
<h5 id="token-embedding"><a href="#token-embedding" class="headerlink" title="token embedding"></a>token embedding</h5><p>token embbeding 是将句子中的每个 token 通过look up table 映射成为一个维度为 H 的向量表示。此外，每个句子的开头有一个 [CLS] 这个特殊token，主要是为了句子分类的作用，同时 [MAKS] 是作为mask任务使用的。</p>
<h5 id="soft-position-embedding"><a href="#soft-position-embedding" class="headerlink" title="soft-position embedding"></a>soft-position embedding</h5><p>我们可以发现，BERT 使用的时position embedding，并且使用的是绝对的position 表示。<br><img src="/pictures/NLP/K-BERT/img5.jpg" alt="Sentence Tree"></p>
<p>如果使用BERT的position embedding方式，即hard-position index。这就导致<strong>原本的句子顺序发生了变化，失去了句子主干的信息位置</strong>。<br>解决方案就是：使用soft-position index。</p>
<p>这就引发了另一个问题：知识噪音。一般字会给周围其他的字很大的attention score。<br>解决方案：引入seeing layer，控制self-attention的可见域。</p>
<h4 id="seeing-layer"><a href="#seeing-layer" class="headerlink" title="seeing layer"></a>seeing layer</h4><p>Seeing layer是BERT和K-BERT之间最大的不同。</p>
<blockquote>
<p>我们插入的知识，只作用于它自身的三元组中的元素，对于其他的token，不产生任何影响。</p>
</blockquote>
<p>根据上述规则，我们可以得到一个visible matrix：<br><img src="/pictures/NLP/K-BERT/img6.jpg" alt="Visible Matrix"></p>
<p>具体可见下图，红色表示可见区域，白色表示不可见区域。<br><img src="/pictures/NLP/K-BERT/img7.jpg" alt="Visible Matrix应用"></p>
<h4 id="mask-attention"><a href="#mask-attention" class="headerlink" title="mask-attention"></a>mask-attention</h4><p>我们可以认为 visible matrix 获得了它的 sentence tree 的结构信息，我们根据这个矩阵构造 mask-self-attention, 实现了在嵌入知识的情况下，不增加噪音的目的。具体公式如下：<br><img src="/pictures/NLP/K-BERT/img8.png" alt="mask-self-attention计算公式"></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>K-BERT 主要的创新点是将知识图谱的事实三元组融入到了预训练的语言模型中，并且不要我们自己进行预训练的操作，只需要在 fine-tuning 以及 inference 阶段进行知识嵌入即可，大大地方便了使用，并且在知识驱动的任务，例如QA，NER，推理任务中取得了很好的效果。</p>
<blockquote>
<p>文章主要解决了两个问题，包括了</p>
<ul>
<li>如何将异质向量空间（heterogeneous embedding space）的知识和预训练的语言空间进行结合，主要就是采用了knowledge layer 结合知识构建 sentence tree。</li>
<li>另外就是在引入了 knowledge 之后，如何避免 knowledge noise，这边就是采用 soft position embedding 以及 seeing layer 中产生的 visible matrix，通过改造 transformer 的self-attetion 为 mask self-attention，控制每个 token 的可见域，从而解决KN问题。</li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
</search>
